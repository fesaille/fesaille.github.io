{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Un titre \u00e0 deux balles \u00b6","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#un-titre-a-deux-balles","text":"","title":"Un titre \u00e0 deux balles"},{"location":"aze/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is often described as a \"batteries included\" language due to its comprehensive standard library. High-level \u00b6 In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language Wikipedia Interpreted \u00b6 There are three general modes of execution for modern high-level languages: Interpreted : the syntax is read and then executed directly, with no compilation stage. A program called an interpreter reads each program statement, following the program flow, then decides what to do, and does it. Compiled : the code written in a language is compiled, its syntax is transformed into an executable form before running. There are two types of compilation: Machine code generation Intermediate representations : the code written in a language is compiled to an intermediate representation, that representation can be optimized or saved for later execution without the need to re-read the source file. When the intermediate representation is saved, it may be in a form such as bytecode. The intermediate representation must then be interpreted or further compiled to execute it. Source-to-source translated or transcompiled First, consider only the upper direct path. The code is parsed, i.e. split up into a list of pieces called tokens. These tokens are based on a set of rules for things that should be treated differently. For instance, the keyword if is a different token than a numeric value like 42 . The list of tokens is transformed to build an Abstract Syntax Tree, AST , collection of nodes which are linked together based on the Python language grammar . From an abstract syntax tree, the interpreter can produce a lower level form of instructions called bytecode . These instructions are things like BINARY_ADD and are meant to be very generic so that a computer can run them. With the bytecode instructions available, the interpreter can finally run your code. The bytecode is used to call functions in your operating system which will ultimately interact with a CPU and memory to run the program. Bytecode example \u00b6 print ( \"Hello, World!\" ) Hello, World! from dis import dis dis ( 'print(\"Hello, World!\")' ) 1 0 LOAD_NAME 0 (print) 2 LOAD_CONST 0 ('Hello, World!') 4 CALL_FUNCTION 1 6 RETURN_VALUE CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can \"push\" an item onto the \"top\" of the structure, or \"pop\" an item off the \"top\"). General purpose \u00b6 https://awesome-python.com/ : References: - Your Guide to the CPython Source Code - Inside The Python Virtual Machine - An introduction to Python bytecode - Deciphering Python: How to use Abstract Syntax Trees (AST) to understand code cpython/ \u2502 \u251c\u2500\u2500 Doc \u2190 Source for the documentation \u251c\u2500\u2500 Grammar \u2190 The computer-readable language definition \u251c\u2500\u2500 Include \u2190 The C header files \u251c\u2500\u2500 Lib \u2190 Standard library modules written in Python \u251c\u2500\u2500 Mac \u2190 macOS support files \u251c\u2500\u2500 Misc \u2190 Miscellaneous files \u251c\u2500\u2500 Modules \u2190 Standard Library Modules written in C \u251c\u2500\u2500 Objects \u2190 Core types and the object model \u251c\u2500\u2500 Parser \u2190 The Python parser source code \u251c\u2500\u2500 PC \u2190 Windows build support files \u251c\u2500\u2500 PCbuild \u2190 Windows build support files for older Windows versions \u251c\u2500\u2500 Programs \u2190 Source code for the python executable and other binaries \u251c\u2500\u2500 Python \u2190 The CPython interpreter source code \u2514\u2500\u2500 Tools \u2190 Standalone tools useful for building or extending Python Garbage collected \u00b6 https://github.com/python/cpython/blob/ce6a070414ed1e1374d1e6212bfbff61b6d5d755/Include/object.h#L104 from sys import getrefcount a = \"un\" getrefcount ( a ) 2 b = a b is a True getrefcount ( a ) 3 del ( b ) getrefcount ( a ) 2","title":"Aze"},{"location":"aze/#high-level","text":"In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language Wikipedia","title":"High-level"},{"location":"aze/#interpreted","text":"There are three general modes of execution for modern high-level languages: Interpreted : the syntax is read and then executed directly, with no compilation stage. A program called an interpreter reads each program statement, following the program flow, then decides what to do, and does it. Compiled : the code written in a language is compiled, its syntax is transformed into an executable form before running. There are two types of compilation: Machine code generation Intermediate representations : the code written in a language is compiled to an intermediate representation, that representation can be optimized or saved for later execution without the need to re-read the source file. When the intermediate representation is saved, it may be in a form such as bytecode. The intermediate representation must then be interpreted or further compiled to execute it. Source-to-source translated or transcompiled First, consider only the upper direct path. The code is parsed, i.e. split up into a list of pieces called tokens. These tokens are based on a set of rules for things that should be treated differently. For instance, the keyword if is a different token than a numeric value like 42 . The list of tokens is transformed to build an Abstract Syntax Tree, AST , collection of nodes which are linked together based on the Python language grammar . From an abstract syntax tree, the interpreter can produce a lower level form of instructions called bytecode . These instructions are things like BINARY_ADD and are meant to be very generic so that a computer can run them. With the bytecode instructions available, the interpreter can finally run your code. The bytecode is used to call functions in your operating system which will ultimately interact with a CPU and memory to run the program.","title":"Interpreted"},{"location":"aze/#bytecode-example","text":"print ( \"Hello, World!\" ) Hello, World! from dis import dis dis ( 'print(\"Hello, World!\")' ) 1 0 LOAD_NAME 0 (print) 2 LOAD_CONST 0 ('Hello, World!') 4 CALL_FUNCTION 1 6 RETURN_VALUE CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can \"push\" an item onto the \"top\" of the structure, or \"pop\" an item off the \"top\").","title":"Bytecode example"},{"location":"aze/#general-purpose","text":"https://awesome-python.com/ : References: - Your Guide to the CPython Source Code - Inside The Python Virtual Machine - An introduction to Python bytecode - Deciphering Python: How to use Abstract Syntax Trees (AST) to understand code cpython/ \u2502 \u251c\u2500\u2500 Doc \u2190 Source for the documentation \u251c\u2500\u2500 Grammar \u2190 The computer-readable language definition \u251c\u2500\u2500 Include \u2190 The C header files \u251c\u2500\u2500 Lib \u2190 Standard library modules written in Python \u251c\u2500\u2500 Mac \u2190 macOS support files \u251c\u2500\u2500 Misc \u2190 Miscellaneous files \u251c\u2500\u2500 Modules \u2190 Standard Library Modules written in C \u251c\u2500\u2500 Objects \u2190 Core types and the object model \u251c\u2500\u2500 Parser \u2190 The Python parser source code \u251c\u2500\u2500 PC \u2190 Windows build support files \u251c\u2500\u2500 PCbuild \u2190 Windows build support files for older Windows versions \u251c\u2500\u2500 Programs \u2190 Source code for the python executable and other binaries \u251c\u2500\u2500 Python \u2190 The CPython interpreter source code \u2514\u2500\u2500 Tools \u2190 Standalone tools useful for building or extending Python","title":"General purpose"},{"location":"aze/#garbage-collected","text":"https://github.com/python/cpython/blob/ce6a070414ed1e1374d1e6212bfbff61b6d5d755/Include/object.h#L104 from sys import getrefcount a = \"un\" getrefcount ( a ) 2 b = a b is a True getrefcount ( a ) 3 del ( b ) getrefcount ( a ) 2","title":"Garbage collected"},{"location":"DB/intro/","text":"Database \u00b6 EdgeDB Framework \u00b6 Ibis \u00b6 Data catalog \u00b6 Intake \u00b6 Persisting data","title":"intro"},{"location":"DB/intro/#database","text":"EdgeDB","title":"Database"},{"location":"DB/intro/#framework","text":"","title":"Framework"},{"location":"DB/intro/#ibis","text":"","title":"Ibis"},{"location":"DB/intro/#data-catalog","text":"","title":"Data catalog"},{"location":"DB/intro/#intake","text":"Persisting data","title":"Intake"},{"location":"DB/S3/_intro/","text":"Object Store (S3) \u00b6 MinIO \u00b6 https://docs.min.io/docs/minio-multi-user-quickstart-guide.html S3 API: https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html","title":"Intro"},{"location":"DB/S3/_intro/#object-store-s3","text":"","title":"Object Store (S3)"},{"location":"DB/S3/_intro/#minio","text":"https://docs.min.io/docs/minio-multi-user-quickstart-guide.html S3 API: https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html","title":"MinIO"},{"location":"DB/SQL/postgres/","text":"PostgreSQL \u00b6 Connect From Your Local Machine to a PostgreSQL Database in Docker https://medium.com/better-programming/connect-from-local-machine-to-postgresql-docker-container-f785f00461a7 Linux downloads (Ubuntu) https://www.postgresql.org/download/linux/ubuntu/ http://zetcode.com/db/postgresqlc/ Docker \u00b6","title":"PostgreSQL"},{"location":"DB/SQL/postgres/#postgresql","text":"Connect From Your Local Machine to a PostgreSQL Database in Docker https://medium.com/better-programming/connect-from-local-machine-to-postgresql-docker-container-f785f00461a7 Linux downloads (Ubuntu) https://www.postgresql.org/download/linux/ubuntu/ http://zetcode.com/db/postgresqlc/","title":"PostgreSQL"},{"location":"DB/SQL/postgres/#docker","text":"","title":"Docker"},{"location":"DB/mongoDB/Aggregation/","text":"Aggregation framework \u00b6 Selection stage \u00b6 $match - filtering documents \u00b6 Code { $ match : { < query > } } a $match may contain a $text query operator, but it must be the first in the pipeline $match should come early in the pipeline $where can not be used with $match $match uses the same syntax as find() $project - shaping document \u00b6 { $project: { } } specify fields to be retained _id must be explicitly removed let add new fields or reassign values can be used as many times as required in an aggregation pipeline $addFields : add transformation fields in the document \u00b6 $geoNear : filtering documents \u00b6 Cursor-like stages \u00b6 $limit : limit \u00b6 $skip : limit \u00b6 $count : limit \u00b6 $sort : limit \u00b6 $sample : limit \u00b6 Group stage \u00b6 $group : limit \u00b6","title":"Aggregation framework"},{"location":"DB/mongoDB/Aggregation/#aggregation-framework","text":"","title":"Aggregation framework"},{"location":"DB/mongoDB/Aggregation/#selection-stage","text":"","title":"Selection stage"},{"location":"DB/mongoDB/Aggregation/#match-filtering-documents","text":"Code { $ match : { < query > } } a $match may contain a $text query operator, but it must be the first in the pipeline $match should come early in the pipeline $where can not be used with $match $match uses the same syntax as find()","title":"$match - filtering documents"},{"location":"DB/mongoDB/Aggregation/#project-shaping-document","text":"{ $project: { } } specify fields to be retained _id must be explicitly removed let add new fields or reassign values can be used as many times as required in an aggregation pipeline","title":"$project - shaping document"},{"location":"DB/mongoDB/Aggregation/#addfields-add-transformation-fields-in-the-document","text":"","title":"$addFields: add transformation fields in the document"},{"location":"DB/mongoDB/Aggregation/#geonear-filtering-documents","text":"","title":"$geoNear: filtering    documents"},{"location":"DB/mongoDB/Aggregation/#cursor-like-stages","text":"","title":"Cursor-like stages"},{"location":"DB/mongoDB/Aggregation/#limit-limit","text":"","title":"$limit: limit"},{"location":"DB/mongoDB/Aggregation/#skip-limit","text":"","title":"$skip: limit"},{"location":"DB/mongoDB/Aggregation/#count-limit","text":"","title":"$count: limit"},{"location":"DB/mongoDB/Aggregation/#sort-limit","text":"","title":"$sort: limit"},{"location":"DB/mongoDB/Aggregation/#sample-limit","text":"","title":"$sample: limit"},{"location":"DB/mongoDB/Aggregation/#group-stage","text":"","title":"Group stage"},{"location":"DB/mongoDB/Aggregation/#group-limit","text":"","title":"$group: limit"},{"location":"DB/mongoDB/_intro/","text":"MongoDB \u00b6 Notation: field path: $fieldName system variable: $$UPPERCASE user variable: $$foo Docker image \u00b6 Jupyter kernel imongo \u00b6 imongo ipython wrapper kernel Jupyter messenging doc http://zguide.zeromq.org/page:all","title":"Intro"},{"location":"DB/mongoDB/_intro/#mongodb","text":"Notation: field path: $fieldName system variable: $$UPPERCASE user variable: $$foo","title":"MongoDB"},{"location":"DB/mongoDB/_intro/#docker-image","text":"","title":"Docker image"},{"location":"DB/mongoDB/_intro/#jupyter-kernel-imongo","text":"imongo ipython wrapper kernel Jupyter messenging doc http://zguide.zeromq.org/page:all","title":"Jupyter kernel imongo"},{"location":"Python/IterGen/","text":"Iterators, generators and coroutines \u00b6 Python standard library: Itertools David Beazley's PyCon'14 presentation","title":"Iterators, generators and coroutines"},{"location":"Python/IterGen/#iterators-generators-and-coroutines","text":"Python standard library: Itertools David Beazley's PyCon'14 presentation","title":"Iterators, generators and coroutines"},{"location":"Python/envs/","text":"Virtual environement \u00b6 venv \u00b6 Pipenv \u00b6 editable-dependencies pipenv install --dev -e . Poetry \u00b6","title":"Envs"},{"location":"Python/envs/#virtual-environement","text":"","title":"Virtual environement"},{"location":"Python/envs/#venv","text":"","title":"venv"},{"location":"Python/envs/#pipenv","text":"editable-dependencies pipenv install --dev -e .","title":"Pipenv"},{"location":"Python/envs/#poetry","text":"","title":"Poetry"},{"location":"Python/test/","text":"Tests \u00b6 tox","title":"Tests"},{"location":"Python/test/#tests","text":"tox","title":"Tests"},{"location":"Python/typing/","text":"Typing \u00b6 Sous-titre \u00b6 PEPS: Literature Overview for Type Hints Theory of Type Hints Type Hints TypedDict Protocols: Structural subtyping ClassVar ContextManager Counter DefaultDict Deque final Final Literal NewType NoReturn overload (note that older versions of typing only let you use overload in stubs) Protocol (except on Python 3.5.0) runtime (except on Python 3.5.0) Text Type TypedDict TYPE_CHECKING Package providing: pydantic","title":"Typing"},{"location":"Python/typing/#typing","text":"","title":"Typing"},{"location":"Python/typing/#sous-titre","text":"PEPS: Literature Overview for Type Hints Theory of Type Hints Type Hints TypedDict Protocols: Structural subtyping ClassVar ContextManager Counter DefaultDict Deque final Final Literal NewType NoReturn overload (note that older versions of typing only let you use overload in stubs) Protocol (except on Python 3.5.0) runtime (except on Python 3.5.0) Text Type TypedDict TYPE_CHECKING Package providing: pydantic","title":"Sous-titre"},{"location":"Viz/matplotlib/","text":"Matplotlib \u00b6 verything in matplotlib is organized in a hierarchy: at the top of the hierarchy is the matplotlib \"state-machine environment\" which is provided by the matplotlib.pyplot module. At this level, simple functions are used to add plot elements (lines, images, text, etc.) to the current axes in the current figure. https://dev.to/skotaro/artist-in-matplotlib---something-i-wanted-to-know-before-spending-tremendous-hours-on-googling-how-tos--31oo Anatomy of a figure \u00b6 https://matplotlib.org/tutorials/intermediate/artists.html Axis \u00b6 Accessor method Description get_scale The scale of the axis, e.g., 'log' or 'linear' get_view_interval The interval instance of the axis view limits get_data_interval The interval instance of the axis data limits get_gridlines A list of grid lines for the Axis get_label The axis label - a Text instance get_ticklabels A list of Text instances - keyword minor-True get_ticklines A list of Line2D instances - keyword minor-True get_ticklocs A list of Tick locations - keyword minor-True get_major_locator The matplotlib.ticker.Locator instance for major ticks get_major_formatter The matplotlib.ticker.Formatter instance for major ticks get_minor_locator The matplotlib.ticker.Locator instance for minor ticks get_minor_formatter The matplotlib.ticker.Formatter instance for minor ticks get_major_ticks A list of Tick instances for major ticks get_minor_ticks A list of Tick instances for minor ticks grid Turn the grid on or off for the major or minor ticks Spines \u00b6","title":"Matplolib"},{"location":"Viz/matplotlib/#matplotlib","text":"verything in matplotlib is organized in a hierarchy: at the top of the hierarchy is the matplotlib \"state-machine environment\" which is provided by the matplotlib.pyplot module. At this level, simple functions are used to add plot elements (lines, images, text, etc.) to the current axes in the current figure. https://dev.to/skotaro/artist-in-matplotlib---something-i-wanted-to-know-before-spending-tremendous-hours-on-googling-how-tos--31oo","title":"Matplotlib"},{"location":"Viz/matplotlib/#anatomy-of-a-figure","text":"https://matplotlib.org/tutorials/intermediate/artists.html","title":"Anatomy of a figure"},{"location":"Viz/matplotlib/#axis","text":"Accessor method Description get_scale The scale of the axis, e.g., 'log' or 'linear' get_view_interval The interval instance of the axis view limits get_data_interval The interval instance of the axis data limits get_gridlines A list of grid lines for the Axis get_label The axis label - a Text instance get_ticklabels A list of Text instances - keyword minor-True get_ticklines A list of Line2D instances - keyword minor-True get_ticklocs A list of Tick locations - keyword minor-True get_major_locator The matplotlib.ticker.Locator instance for major ticks get_major_formatter The matplotlib.ticker.Formatter instance for major ticks get_minor_locator The matplotlib.ticker.Locator instance for minor ticks get_minor_formatter The matplotlib.ticker.Formatter instance for minor ticks get_major_ticks A list of Tick instances for major ticks get_minor_ticks A list of Tick instances for minor ticks grid Turn the grid on or off for the major or minor ticks","title":"Axis"},{"location":"Viz/matplotlib/#spines","text":"","title":"Spines"},{"location":"WebDev/Intro/","text":"Web Developpment \u00b6 WHATWG spec First Term This is the definition of the first term. Second Term This is one definition of the second term. This is another definition of the second term. HTML semantic elements \u00b6 Elements, attributes, and attribute values in HTML are defined (by this specification) to have certain meanings (semantics). For example, the ol element represents an ordered list, and the lang attribute represents the language of the content. These definitions allow HTML processors, such as Web browsers or search engines, to present and use documents and applications in a wide variety of contexts that the author might not have considered. Web components \u00b6 JavaScript file defines a class called PopUpInfo , which extends HTMLElement . Inside the constructor, all the functionality the element will have when an instance of it is instantiated. Example Create a badge similar to the one at the top of the page: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class DocBadge extends HTMLElement { constructor () { // Always call super first in constructor super (); // Create a documentation badge var shadow = this . attachShadow ({ mode : 'open' }); var wrapper = document . createElement ( 'span' ); wrapper . setAttribute ( 'class' , 'myBadge' ); var href = this . hasAttribute ( 'href' ) ? this . getAttribute ( 'href' ) : 'img/default.png' ; var alt = this . hasAttribute ( 'alt' ) ? this . getAttribute ( 'alt' ) : 'Documentation' ; var label = this . hasAttribute ( 'label' ) ? this . getAttribute ( 'label' ) : 'docs' ; var message = this . hasAttribute ( 'message' ) ? this . getAttribute ( 'message' ) : 'stable' ; var color = this . hasAttribute ( 'color' ) ? this . getAttribute ( 'color' ) : 'brightgreen' ; var logo = this . hasAttribute ( 'logo' ) ? this . getAttribute ( 'logo' ) : 'Read-the-docs' ; var src = `https://img.shields.io/badge/ ${ label } - ${ message } - ${ color } ?style=flat&logo= ${ logo } ` ; var link = document . createElement ( 'a' ); link . setAttribute ( 'target' , '_blank' ); link . setAttribute ( 'href' , href ); var img = document . createElement ( 'img' ); img . setAttribute ( 'src' , src ); img . setAttribute ( 'alt' , alt ); shadow . appendChild ( wrapper ); wrapper . appendChild ( link ); link . appendChild ( img ); } } // register it customElements . define ( 'badge-doc' , DocBadge ); Usage: < badge-doc href = \"https://developer.mozilla.org/en-US/docs/Web/Web_Components\" message = \"MDN\" color = \"lightgrey\" ></ badge-doc > Resources \u00b6 Shield badges Simple icons","title":"Intro"},{"location":"WebDev/Intro/#web-developpment","text":"WHATWG spec First Term This is the definition of the first term. Second Term This is one definition of the second term. This is another definition of the second term.","title":"Web Developpment"},{"location":"WebDev/Intro/#html-semantic-elements","text":"Elements, attributes, and attribute values in HTML are defined (by this specification) to have certain meanings (semantics). For example, the ol element represents an ordered list, and the lang attribute represents the language of the content. These definitions allow HTML processors, such as Web browsers or search engines, to present and use documents and applications in a wide variety of contexts that the author might not have considered.","title":"HTML semantic elements"},{"location":"WebDev/Intro/#web-components","text":"JavaScript file defines a class called PopUpInfo , which extends HTMLElement . Inside the constructor, all the functionality the element will have when an instance of it is instantiated. Example Create a badge similar to the one at the top of the page: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class DocBadge extends HTMLElement { constructor () { // Always call super first in constructor super (); // Create a documentation badge var shadow = this . attachShadow ({ mode : 'open' }); var wrapper = document . createElement ( 'span' ); wrapper . setAttribute ( 'class' , 'myBadge' ); var href = this . hasAttribute ( 'href' ) ? this . getAttribute ( 'href' ) : 'img/default.png' ; var alt = this . hasAttribute ( 'alt' ) ? this . getAttribute ( 'alt' ) : 'Documentation' ; var label = this . hasAttribute ( 'label' ) ? this . getAttribute ( 'label' ) : 'docs' ; var message = this . hasAttribute ( 'message' ) ? this . getAttribute ( 'message' ) : 'stable' ; var color = this . hasAttribute ( 'color' ) ? this . getAttribute ( 'color' ) : 'brightgreen' ; var logo = this . hasAttribute ( 'logo' ) ? this . getAttribute ( 'logo' ) : 'Read-the-docs' ; var src = `https://img.shields.io/badge/ ${ label } - ${ message } - ${ color } ?style=flat&logo= ${ logo } ` ; var link = document . createElement ( 'a' ); link . setAttribute ( 'target' , '_blank' ); link . setAttribute ( 'href' , href ); var img = document . createElement ( 'img' ); img . setAttribute ( 'src' , src ); img . setAttribute ( 'alt' , alt ); shadow . appendChild ( wrapper ); wrapper . appendChild ( link ); link . appendChild ( img ); } } // register it customElements . define ( 'badge-doc' , DocBadge ); Usage: < badge-doc href = \"https://developer.mozilla.org/en-US/docs/Web/Web_Components\" message = \"MDN\" color = \"lightgrey\" ></ badge-doc >","title":"Web components"},{"location":"WebDev/Intro/#resources","text":"Shield badges Simple icons","title":"Resources"},{"location":"WebDev/REACT/","text":"REACT \u00b6 Synthetic events Theme \u00b6 material UI Forms \u00b6 See formik https://jaredpalmer.com/formik/docs/overview Hooks \u00b6 Introducing hooks Books \u00b6 Learning react by Kirupa Chinnathambi azeaze qsd Eloquent JavaScript by Marijn Haverbeke Read favorite Eloquent JavaScript by Marijn Haverbeke Read favorite","title":"REACT"},{"location":"WebDev/REACT/#react","text":"Synthetic events","title":"REACT"},{"location":"WebDev/REACT/#theme","text":"material UI","title":"Theme"},{"location":"WebDev/REACT/#forms","text":"See formik https://jaredpalmer.com/formik/docs/overview","title":"Forms"},{"location":"WebDev/REACT/#hooks","text":"Introducing hooks","title":"Hooks"},{"location":"WebDev/REACT/#books","text":"","title":"Books"},{"location":"WebDev/css/","text":"CSS \u00b6 Element size \u00b6 Width [ <length> | <percentage> ] && [ border-box | content-box ]? | available | min-content | max-content | fit-content | auto Flex box \u00b6 Initialisation \u00b6 display: flex; or display: inline-flex; Properties that apply to the parent \u00b6 Ordering and Orientation \u00b6 Property Value Comment flex-direction row | row-reverse | column | column-reverse Ex. : div { flex-direction: row; } flex-wrap nowrap | wrap | wrap-reverse Ex. : div { flex-direction: column; flex-wrap: wrap; } flex-flow see individual properties Shorthand for flex-direction and flex-wrap Ex. : div { felx-flow: row-reverse wrap-reverse; } 1 2 3 4 1 2 3 4 Ordering justify-content align-items align-content Property Value Comment justify-content flex-start | flex-end | center | space-between | space-around align-items flex-start | flex-end | center | baseline | stretch align-content flex-start | flex-end | center | space-between | space-around | stretch Properties that apply to the child elements \u00b6 Ordering order : flex flow direction Property Value order <integer> Flexibility Property Value flex-grow <number> flex-shrink <number> flex-basis content | <width>","title":"CSS"},{"location":"WebDev/css/#css","text":"","title":"CSS"},{"location":"WebDev/css/#element-size","text":"Width [ <length> | <percentage> ] && [ border-box | content-box ]? | available | min-content | max-content | fit-content | auto","title":"Element size"},{"location":"WebDev/css/#flex-box","text":"","title":"Flex box"},{"location":"WebDev/css/#initialisation","text":"display: flex; or display: inline-flex;","title":"Initialisation"},{"location":"WebDev/css/#properties-that-apply-to-the-parent","text":"","title":"Properties that apply to the parent"},{"location":"WebDev/css/#ordering-and-orientation","text":"Property Value Comment flex-direction row | row-reverse | column | column-reverse Ex. : div { flex-direction: row; } flex-wrap nowrap | wrap | wrap-reverse Ex. : div { flex-direction: column; flex-wrap: wrap; } flex-flow see individual properties Shorthand for flex-direction and flex-wrap Ex. : div { felx-flow: row-reverse wrap-reverse; } 1 2 3 4 1 2 3 4 Ordering justify-content align-items align-content Property Value Comment justify-content flex-start | flex-end | center | space-between | space-around align-items flex-start | flex-end | center | baseline | stretch align-content flex-start | flex-end | center | space-between | space-around | stretch","title":"Ordering and Orientation"},{"location":"WebDev/css/#properties-that-apply-to-the-child-elements","text":"Ordering order : flex flow direction Property Value order <integer> Flexibility Property Value flex-grow <number> flex-shrink <number> flex-basis content | <width>","title":"Properties that apply to the child elements"},{"location":"WebDev/node/","text":"Node.js \u00b6 List packages and version from package-lock.json using jq cat package-lock.json | jq '[.dependencies | to_entries[] | {\"key\": .key, \"value\": .value.version}] | from_entries'","title":"node"},{"location":"WebDev/node/#nodejs","text":"List packages and version from package-lock.json using jq cat package-lock.json | jq '[.dependencies | to_entries[] | {\"key\": .key, \"value\": .value.version}] | from_entries'","title":"Node.js"},{"location":"_nb/04_influxDB/01%20-%20Intro%20InfluxDB/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); InfluxDB is a time series database designed to handle timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. Key features \u00b6 CLI/HTTP write and query API. Expressive SQL-like query language Schemas don't have to be defined up front and schema preferences may change over time. Tags allow series to be indexed for fast and efficient queries. Retention policies efficiently auto-expire stale data. Plugins support for other data ingestion protocols such as Graphite, collectd, and OpenTSDB. Continuous queries automatically compute aggregate data to make frequent queries more efficient. InfluxDB isn\u2019t fully CRUD The open source edition of InfluxDB runs on a single node, high availability is only available in the InfluxDB Enterprise Edition. Data structure \u00b6 Time series key concepts : - time - a timestamp - is similar to a SQL primary key, - tags , zero to many key-values, contain any metadata about the value, tags are indexed - at least one key-value field set ( `field key identifies the measured element while field value are the measured value itself, e.g. \u201cvalue=0.64\u201d, or \u201ctemperature=21.2\u201d). **fields are not indexed** It\u2019s important to note that fields are not indexed. Queries that use field values as filters must scan all values that match the other conditions in the query. As a result, those queries are not performant relative to queries on tags. In general, fields should not contain commonly-queried metadata. Further concepts: - a measurement acts as a container for tags , fields , and the time column. Assimilable to a SQL table, where the primary index is always time . tags and fields are effectively columns in the table. - a series is the collection of data that share the same retention policy, measurement, and tag set. - a point represents a single data record that has four components: a measurement, tag set, field set, and a timestamp. A point is uniquely identified by its series and timestamp (similar to a row in a SQL database table) Element Optional/Required Description Type (See data types for more information.) Measurement Required The measurement name. InfluxDB accepts one measurement per point. String Tag set Optional All tag key-value pairs for the point. Tag keys and tag values are both strings. Field set Required. Points must have at least one field. All field key-value pairs for the point. Field keys are strings. Field values can be floats, integers, strings, or Booleans. Timestamp Optional. InfluxDB uses the server\u2019s local nanosecond timestamp in UTC if the timestamp is not included with the point. The timestamp for the data point. InfluxDB accepts one timestamp per point. Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API . Data type \u00b6 Datatype Element(s) Description Float Field values IEEE-754 64-bit floating-point numbers. This is the default numerical type. Examples: 1 , 1.0 , 1.e+78 , 1.E+78 . Integer Field values Signed 64-bit integers (-9223372036854775808 to 9223372036854775807). Specify an integer with a trailing i on the number. Example: 1i . String Measurements, tag keys, tag values, field keys, field values Length limit 64KB. Boolean Field values Stores TRUE or FALSE values. TRUE write syntax: [t, T, true, True, TRUE] . FALSE write syntax: [f, F, false, False, FALSE] Timestamp Timestamps Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API . The minimum valid timestamp is -9223372036854775806 or 1677-09-21T00:12:43.145224194Z . The maximum valid timestamp is 9223372036854775806 or 2262-04-11T23:47:16.854775806Z . Python module documentation Example \u00b6 *census*: time butterflies honeybees location scientist 2015-08-18T00:00:00Z 12 23 1 langstroth 2015-08-18T00:00:00Z 1 30 1 perpetua 2015-08-18T00:06:00Z 11 28 1 langstroth 015-08-18T00:06:00Z 3 28 1 perpetua 2015-08-18T05:54:00Z 2 11 2 langstroth 2015-08-18T06:00:00Z 1 10 2 langstroth 2015-08-18T06:06:00Z 8 23 2 perpetua 2015-08-18T06:12:00Z 7 22 2 perpetua **8 field sets** butterflies = 12 honeybees = 23 butterflies = 1 honeybees = 30 butterflies = 11 honeybees = 28 butterflies = 3 honeybees = 28 butterflies = 2 honeybees = 11 butterflies = 1 honeybees = 10 butterflies = 8 honeybees = 23 butterflies = 7 honeybees = 22 **4 tag sets** (different combinations of all the tag key-value pairs) location = 1, scientist = langstroth location = 2, scientist = langstroth location = 1, scientist = perpetua location = 2, scientist = perpetua Arbitrary series number Retention policy Measurement Tag set series 1 autogen census location = 1 , scientist = langstroth series 2 autogen census location = 2 , scientist = langstroth series 3 autogen census location = 1 , scientist = perpetua series 4 autogen census location = 2 , scientist = perpetua from datetime import ( datetime , timedelta ) from random import ( choice , randint , random , uniform , lognormvariate ) from influxdb import ( InfluxDBClient , DataFrameClient ) INFLUXDB_USER = 'telegraf' INFLUXDB_USER_PASSWORD = 'secretpassword' host = 'db.influxdb.app.com' port = 8086 \"\"\"Instantiate a connection to the InfluxDB.\"\"\" user = 'admin' password = 'supersecretpassword' dbname = 'example' dbuser = 'telegraf' dbuser_password = 'secretpassword' client = InfluxDBClient ( host , port , user , password , dbname ) DB init \u00b6 For creating the DB client.create_database(dbname) Define a specific retention policy, drop after 30d, with replica factor of 3 and applied by default to new elements client . create_retention_policy ( 'custom_policy' , '30d' , 3 , default = True ) # For dropping the policy: client.drop_retention_policy('custom_policy', dbname) client . switch_user ( dbuser , dbuser_password ) def feed_db ( n ): json_body = [ { \"measurement\" : \"starfleet_01\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"speed\" : lognormvariate ( 10 , 3 ), \"consumption\" : uniform ( 0 , 300 ), \"pressure_a\" : 3 + random (), \"status_b\" : choice (( True , False )) } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) def feed_db_array ( n ): json_body = [ { \"measurement\" : \"starfleet_02\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"curve_b[0]\" : 10 + random (), \"curve_b[1]\" : choice (( 15 + random (), 4 + random ())), \"curve_b[2]\" : 20 + random (), } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) % timeit feed_db ( 100_000 ) 13.2 s \u00b1 202 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) feed_db_array(1_000_000) Series can be deleted client.delete_series(database=\"example\", measurement=\"starfleet_01\") client . query ( query = 'select count(*) from starfleet_01' ) ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count_consumption': 1866653, 'count_pressure_a': 1866653, 'count_speed': 1866653, 'count_status_b': 1866653}]}) from IPython.core.magic import ( register_line_cell_magic ) @register_line_cell_magic def influxql ( line , cell = None ): \"Magic that works both as %lc magic and as %% lcmagic\" if cell is None : sqlstr = line else : sqlstr = \";\" . join (( op . strip ( \";\" ) for op in cell . strip ( \" \\n \" ) . split ( \" \\n \" ))) + \";\" return client . query ( query = sqlstr ) % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:05Z', 'speed': 774750.9698706511}, {'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}]}) result = % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; result ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}, {'time': '2019-08-19T20:52:25Z', 'speed': 9.829304084029904}]}) % influxql SELECT COUNT ( DISTINCT ( pressure_a )) FROM starfleet_01 ; ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count': 1000000}]}) % influxql SELECT COUNT ( pressure_a ) FROM starfleet_01 GROUP BY time ( 28 d ), region LIMIT 1 ; ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118099}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118282}], '('starfleet_01', {'region': 'Earth'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117988}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117959}]}) % influxql SELECT MEAN ( pressure_a ) FROM starfleet_01 GROUP BY region ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.499534039730786}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.4999780335201867}], '('starfleet_01', {'region': 'Earth'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5005520567253616}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5013387093463155}]}) You can explain query % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE region = 'Andoria' LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 750'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 2160'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 2160'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 5992395'}]}) % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE pressure_a > 3.5 LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 3000'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 8640'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 8640'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 23943525'}]}) res = % influxql SELECT * FROM \"starfleet_01\" WHERE pressure_a > 3.5 LIMIT 50 points = res . get_points ( tags = { \"region\" : \"Andoria\" }) from pandas import DataFrame df = DataFrame ( points ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region spacecraft speed status_b time 0 Archer 80.868742 3.764769 Andoria NX-1707 4.540769e+06 False 2019-07-28T01:35:50Z 1 Archer 64.351610 3.784118 Andoria NX-1707 8.549125e+05 False 2019-07-28T01:36:00Z 2 Archer 60.765559 3.907481 Andoria NX-1702 1.679463e+05 False 2019-07-28T01:36:04Z 3 Archer 252.870551 3.800938 Andoria NX-1706 1.444727e+04 True 2019-07-28T01:36:06Z 4 Kruge 192.871755 3.930994 Andoria NX-1702 4.376490e+03 False 2019-07-28T01:36:08Z df . groupby ( 'spacecraft' ) . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region speed status_b time spacecraft NX-1701 Kirk 226.626952 3.518973 Andoria 6.987498e+05 False 2019-07-28T01:38:44Z NX-1702 Kruge 217.135955 3.952388 Andoria 6.735246e+05 True 2019-07-28T01:37:38Z NX-1704 Kruge 278.172741 3.976138 Andoria 2.770339e+01 True 2019-07-28T01:37:54Z NX-1705 Kirk 197.423732 3.964069 Andoria 4.300635e+04 True 2019-07-28T01:37:42Z NX-1706 Kruge 252.870551 3.800938 Andoria 1.444727e+04 True 2019-07-28T01:37:02Z NX-1707 Kruge 289.899273 3.913948 Andoria 4.540769e+06 True 2019-07-28T01:38:46Z NX-1709 Kruge 272.085362 3.734222 Andoria 1.653500e+05 True 2019-07-28T01:37:30Z NX-1710 Kruge 297.309907 3.875039 Andoria 5.887814e+04 True 2019-07-28T01:38:20Z % influxql SHOW QUERIES ; ResultSet({'('results', None)': [{'qid': 2821, 'query': 'SHOW QUERIES', 'database': 'example', 'duration': '171\u00b5s', 'status': 'running'}]}) % influxql SHOW TAG KEYS ; ResultSet({'('starfleet_01', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}], '('starfleet_02', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}]})","title":"01   Intro InfluxDB"},{"location":"_nb/04_influxDB/01%20-%20Intro%20InfluxDB/#key-features","text":"CLI/HTTP write and query API. Expressive SQL-like query language Schemas don't have to be defined up front and schema preferences may change over time. Tags allow series to be indexed for fast and efficient queries. Retention policies efficiently auto-expire stale data. Plugins support for other data ingestion protocols such as Graphite, collectd, and OpenTSDB. Continuous queries automatically compute aggregate data to make frequent queries more efficient. InfluxDB isn\u2019t fully CRUD The open source edition of InfluxDB runs on a single node, high availability is only available in the InfluxDB Enterprise Edition.","title":"Key features"},{"location":"_nb/04_influxDB/01%20-%20Intro%20InfluxDB/#data-structure","text":"Time series key concepts : - time - a timestamp - is similar to a SQL primary key, - tags , zero to many key-values, contain any metadata about the value, tags are indexed - at least one key-value field set ( `field key identifies the measured element while field value are the measured value itself, e.g. \u201cvalue=0.64\u201d, or \u201ctemperature=21.2\u201d). **fields are not indexed** It\u2019s important to note that fields are not indexed. Queries that use field values as filters must scan all values that match the other conditions in the query. As a result, those queries are not performant relative to queries on tags. In general, fields should not contain commonly-queried metadata. Further concepts: - a measurement acts as a container for tags , fields , and the time column. Assimilable to a SQL table, where the primary index is always time . tags and fields are effectively columns in the table. - a series is the collection of data that share the same retention policy, measurement, and tag set. - a point represents a single data record that has four components: a measurement, tag set, field set, and a timestamp. A point is uniquely identified by its series and timestamp (similar to a row in a SQL database table) Element Optional/Required Description Type (See data types for more information.) Measurement Required The measurement name. InfluxDB accepts one measurement per point. String Tag set Optional All tag key-value pairs for the point. Tag keys and tag values are both strings. Field set Required. Points must have at least one field. All field key-value pairs for the point. Field keys are strings. Field values can be floats, integers, strings, or Booleans. Timestamp Optional. InfluxDB uses the server\u2019s local nanosecond timestamp in UTC if the timestamp is not included with the point. The timestamp for the data point. InfluxDB accepts one timestamp per point. Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API .","title":"Data structure"},{"location":"_nb/04_influxDB/01%20-%20Intro%20InfluxDB/#data-type","text":"Datatype Element(s) Description Float Field values IEEE-754 64-bit floating-point numbers. This is the default numerical type. Examples: 1 , 1.0 , 1.e+78 , 1.E+78 . Integer Field values Signed 64-bit integers (-9223372036854775808 to 9223372036854775807). Specify an integer with a trailing i on the number. Example: 1i . String Measurements, tag keys, tag values, field keys, field values Length limit 64KB. Boolean Field values Stores TRUE or FALSE values. TRUE write syntax: [t, T, true, True, TRUE] . FALSE write syntax: [f, F, false, False, FALSE] Timestamp Timestamps Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API . The minimum valid timestamp is -9223372036854775806 or 1677-09-21T00:12:43.145224194Z . The maximum valid timestamp is 9223372036854775806 or 2262-04-11T23:47:16.854775806Z . Python module documentation","title":"Data type"},{"location":"_nb/04_influxDB/01%20-%20Intro%20InfluxDB/#example","text":"*census*: time butterflies honeybees location scientist 2015-08-18T00:00:00Z 12 23 1 langstroth 2015-08-18T00:00:00Z 1 30 1 perpetua 2015-08-18T00:06:00Z 11 28 1 langstroth 015-08-18T00:06:00Z 3 28 1 perpetua 2015-08-18T05:54:00Z 2 11 2 langstroth 2015-08-18T06:00:00Z 1 10 2 langstroth 2015-08-18T06:06:00Z 8 23 2 perpetua 2015-08-18T06:12:00Z 7 22 2 perpetua **8 field sets** butterflies = 12 honeybees = 23 butterflies = 1 honeybees = 30 butterflies = 11 honeybees = 28 butterflies = 3 honeybees = 28 butterflies = 2 honeybees = 11 butterflies = 1 honeybees = 10 butterflies = 8 honeybees = 23 butterflies = 7 honeybees = 22 **4 tag sets** (different combinations of all the tag key-value pairs) location = 1, scientist = langstroth location = 2, scientist = langstroth location = 1, scientist = perpetua location = 2, scientist = perpetua Arbitrary series number Retention policy Measurement Tag set series 1 autogen census location = 1 , scientist = langstroth series 2 autogen census location = 2 , scientist = langstroth series 3 autogen census location = 1 , scientist = perpetua series 4 autogen census location = 2 , scientist = perpetua from datetime import ( datetime , timedelta ) from random import ( choice , randint , random , uniform , lognormvariate ) from influxdb import ( InfluxDBClient , DataFrameClient ) INFLUXDB_USER = 'telegraf' INFLUXDB_USER_PASSWORD = 'secretpassword' host = 'db.influxdb.app.com' port = 8086 \"\"\"Instantiate a connection to the InfluxDB.\"\"\" user = 'admin' password = 'supersecretpassword' dbname = 'example' dbuser = 'telegraf' dbuser_password = 'secretpassword' client = InfluxDBClient ( host , port , user , password , dbname )","title":"Example"},{"location":"_nb/04_influxDB/01%20-%20Intro%20InfluxDB/#db-init","text":"For creating the DB client.create_database(dbname) Define a specific retention policy, drop after 30d, with replica factor of 3 and applied by default to new elements client . create_retention_policy ( 'custom_policy' , '30d' , 3 , default = True ) # For dropping the policy: client.drop_retention_policy('custom_policy', dbname) client . switch_user ( dbuser , dbuser_password ) def feed_db ( n ): json_body = [ { \"measurement\" : \"starfleet_01\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"speed\" : lognormvariate ( 10 , 3 ), \"consumption\" : uniform ( 0 , 300 ), \"pressure_a\" : 3 + random (), \"status_b\" : choice (( True , False )) } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) def feed_db_array ( n ): json_body = [ { \"measurement\" : \"starfleet_02\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"curve_b[0]\" : 10 + random (), \"curve_b[1]\" : choice (( 15 + random (), 4 + random ())), \"curve_b[2]\" : 20 + random (), } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) % timeit feed_db ( 100_000 ) 13.2 s \u00b1 202 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) feed_db_array(1_000_000) Series can be deleted client.delete_series(database=\"example\", measurement=\"starfleet_01\") client . query ( query = 'select count(*) from starfleet_01' ) ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count_consumption': 1866653, 'count_pressure_a': 1866653, 'count_speed': 1866653, 'count_status_b': 1866653}]}) from IPython.core.magic import ( register_line_cell_magic ) @register_line_cell_magic def influxql ( line , cell = None ): \"Magic that works both as %lc magic and as %% lcmagic\" if cell is None : sqlstr = line else : sqlstr = \";\" . join (( op . strip ( \";\" ) for op in cell . strip ( \" \\n \" ) . split ( \" \\n \" ))) + \";\" return client . query ( query = sqlstr ) % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:05Z', 'speed': 774750.9698706511}, {'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}]}) result = % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; result ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}, {'time': '2019-08-19T20:52:25Z', 'speed': 9.829304084029904}]}) % influxql SELECT COUNT ( DISTINCT ( pressure_a )) FROM starfleet_01 ; ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count': 1000000}]}) % influxql SELECT COUNT ( pressure_a ) FROM starfleet_01 GROUP BY time ( 28 d ), region LIMIT 1 ; ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118099}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118282}], '('starfleet_01', {'region': 'Earth'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117988}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117959}]}) % influxql SELECT MEAN ( pressure_a ) FROM starfleet_01 GROUP BY region ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.499534039730786}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.4999780335201867}], '('starfleet_01', {'region': 'Earth'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5005520567253616}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5013387093463155}]}) You can explain query % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE region = 'Andoria' LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 750'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 2160'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 2160'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 5992395'}]}) % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE pressure_a > 3.5 LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 3000'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 8640'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 8640'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 23943525'}]}) res = % influxql SELECT * FROM \"starfleet_01\" WHERE pressure_a > 3.5 LIMIT 50 points = res . get_points ( tags = { \"region\" : \"Andoria\" }) from pandas import DataFrame df = DataFrame ( points ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region spacecraft speed status_b time 0 Archer 80.868742 3.764769 Andoria NX-1707 4.540769e+06 False 2019-07-28T01:35:50Z 1 Archer 64.351610 3.784118 Andoria NX-1707 8.549125e+05 False 2019-07-28T01:36:00Z 2 Archer 60.765559 3.907481 Andoria NX-1702 1.679463e+05 False 2019-07-28T01:36:04Z 3 Archer 252.870551 3.800938 Andoria NX-1706 1.444727e+04 True 2019-07-28T01:36:06Z 4 Kruge 192.871755 3.930994 Andoria NX-1702 4.376490e+03 False 2019-07-28T01:36:08Z df . groupby ( 'spacecraft' ) . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region speed status_b time spacecraft NX-1701 Kirk 226.626952 3.518973 Andoria 6.987498e+05 False 2019-07-28T01:38:44Z NX-1702 Kruge 217.135955 3.952388 Andoria 6.735246e+05 True 2019-07-28T01:37:38Z NX-1704 Kruge 278.172741 3.976138 Andoria 2.770339e+01 True 2019-07-28T01:37:54Z NX-1705 Kirk 197.423732 3.964069 Andoria 4.300635e+04 True 2019-07-28T01:37:42Z NX-1706 Kruge 252.870551 3.800938 Andoria 1.444727e+04 True 2019-07-28T01:37:02Z NX-1707 Kruge 289.899273 3.913948 Andoria 4.540769e+06 True 2019-07-28T01:38:46Z NX-1709 Kruge 272.085362 3.734222 Andoria 1.653500e+05 True 2019-07-28T01:37:30Z NX-1710 Kruge 297.309907 3.875039 Andoria 5.887814e+04 True 2019-07-28T01:38:20Z % influxql SHOW QUERIES ; ResultSet({'('results', None)': [{'qid': 2821, 'query': 'SHOW QUERIES', 'database': 'example', 'duration': '171\u00b5s', 'status': 'running'}]}) % influxql SHOW TAG KEYS ; ResultSet({'('starfleet_01', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}], '('starfleet_02', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}]})","title":"DB init"},{"location":"_nb/%3DC/C%20kernel%20Test/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); #include <stdio.h> int main () { int a ; char c [] = \"ABC\" ; char * b ; a = 34 ; b = & c [ 2 ]; printf ( \"Hello world %s \\n \" , b ); } Hello world C #include <stdio.h> int main () { int a = 2 ; printf ( \"The integer is %d \\n \" , a ); return 0 ; } The integer is 2 Titre \u00b6 #include <stdio.h> int main ( void ) { char name [] = \"Harry Potter\" ; printf ( \"%c\" , * name ); // Output: H printf ( \"%c\" , * ( name + 1 )); // Output: a printf ( \"%c\" , * ( name + 7 )); // Output: o char * namePtr ; namePtr = name ; printf ( \"%c\" , * namePtr ); // Output: H printf ( \"%c\" , * ( namePtr + 1 )); // Output: a printf ( \"%c\" , * ( namePtr + 7 )); // Output: o } HaoHao #include <stdio.h> #include <stdlib.h> int main () { int i , num = 10 ; float * data ; // Allocates the memory for 'num' elements. data = ( float * ) calloc ( num , sizeof ( float )); if ( data == NULL ) { printf ( \"Error!!! memory not allocated.\" ); exit ( 0 ); } printf ( \" \\n \" ); // Stores the number entered by the user. for ( i = 0 ; i < num ; ++ i ) { printf ( \"Enter Number %d: \" , i + 1 ); scanf ( \"%f\" , data + i ); } } Enter Number 1: Enter Number 2: Enter Number 3: Enter Number 4: Enter Number 5: Enter Number 6: Enter Number 7: Enter Number 8: Enter Number 9: Enter Number 10: #include <stdio.h> #include <unistd.h> /* sysconf(3) */ int main ( void ) { printf ( \"The page size for this system is %ld bytes. \\n \" , sysconf ( _SC_PAGESIZE )); /* _SC_PAGE_SIZE is OK too. */ return 0 ; } The page size for this system is 4096 bytes.","title":"C kernel Test"},{"location":"_nb/%3DC/C%20kernel%20Test/#titre","text":"#include <stdio.h> int main ( void ) { char name [] = \"Harry Potter\" ; printf ( \"%c\" , * name ); // Output: H printf ( \"%c\" , * ( name + 1 )); // Output: a printf ( \"%c\" , * ( name + 7 )); // Output: o char * namePtr ; namePtr = name ; printf ( \"%c\" , * namePtr ); // Output: H printf ( \"%c\" , * ( namePtr + 1 )); // Output: a printf ( \"%c\" , * ( namePtr + 7 )); // Output: o } HaoHao #include <stdio.h> #include <stdlib.h> int main () { int i , num = 10 ; float * data ; // Allocates the memory for 'num' elements. data = ( float * ) calloc ( num , sizeof ( float )); if ( data == NULL ) { printf ( \"Error!!! memory not allocated.\" ); exit ( 0 ); } printf ( \" \\n \" ); // Stores the number entered by the user. for ( i = 0 ; i < num ; ++ i ) { printf ( \"Enter Number %d: \" , i + 1 ); scanf ( \"%f\" , data + i ); } } Enter Number 1: Enter Number 2: Enter Number 3: Enter Number 4: Enter Number 5: Enter Number 6: Enter Number 7: Enter Number 8: Enter Number 9: Enter Number 10: #include <stdio.h> #include <unistd.h> /* sysconf(3) */ int main ( void ) { printf ( \"The page size for this system is %ld bytes. \\n \" , sysconf ( _SC_PAGESIZE )); /* _SC_PAGE_SIZE is OK too. */ return 0 ; } The page size for this system is 4096 bytes.","title":"Titre"},{"location":"_nb/%3DC/Jeux%20en%20C%20-%201/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); //%cflags:-ltlphi_hdr #include <tlphi_hdr.h> int main () { int sem_wait ( sem_t * sem ); return 0 } /tmp/tmpmr2p5q5y.c:2:10: fatal error: tlphi_hdr.h: No such file or directory #include <tlphi_hdr.h> ^~~~~~~~~~~~~ compilation terminated. [C kernel] GCC exited with code 1, the executable will not be executed //%cflags:-lm #include <stdio.h> #include <math.h> #define PI 3.14159265 double calc ( int * a ) { double val = PI * 3 * * a ; * a = 3 ; printf ( \"val= %lf \\n \" , val ); return val ; } int main () { int a = 2 ; double val = calc ( & a ) * sin ( a ); printf ( \"a = %lf, sin(a) = %lf\" , ( double ) a , val ); return 0 ; } val= 18.849556 a = 3.000000, sin(a) = 2.660049 #include <stdio.h> #include <stdlib.h> int main () { int n = 10 ; int a [ n ]; int * ptr = a ; for ( int i = 0 ; ptr < a + n ; ptr ++ , i ++ ) { * ptr = i * 3 ; printf ( \"index=%i, a[i]=%i \\n \" , i , * ( a + i )); } } index=0, a[i]=0 index=1, a[i]=3 index=2, a[i]=6 index=3, a[i]=9 index=4, a[i]=12 index=5, a[i]=15 index=6, a[i]=18 index=7, a[i]=21 index=8, a[i]=24 index=9, a[i]=27 #include <stdio.h> #include <stdlib.h> int main () { int n = 10 ; int a [ n ]; for ( int i = 0 ; i < n ; i ++ ) { a [ i ] = i * 2 ; printf ( \"index=%i, a[i]=%i \\n \" , i , a [ i ]); } } index=0, a[i]=0 index=1, a[i]=2 index=2, a[i]=4 index=3, a[i]=6 index=4, a[i]=8 index=5, a[i]=10 index=6, a[i]=12 index=7, a[i]=14 index=8, a[i]=16 index=9, a[i]=18 #include <stdio.h> #include <stdlib.h> int main () { int n = 10 ; float * fptr = malloc ( n * sizeof ( float )); register float * last = fptr + n ; * if ( fptr == NULL ) printf ( \"Hep\" ); for (; fptr < last ; fptr ++ ) printf ( \"index=%f \\n \" , * fptr ); } index=0.000000 index=0.000000 index=0.000000 index=0.000000 index=0.000000 index=0.000000 index=0.000000 index=0.000000 index=0.000000 index=0.000000","title":"Jeux en C   1"},{"location":"_nb/%3DGo/G0%2001/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import \"fmt\" const a int = 4 ; type rectangle struct { length float64 breadth float64 color string } r := rectangle { 10.5 , 25.10 , \"red\" } int ( r . color ) repl.go:1:5: cannot convert string to int: r.color r . length + 0.1 10.6 r . length + float64 ( a ) 14.5 rect1 := new ( rectangle ) rect1 . color type geometry struct { area int perimeter int } type rectangle struct { length int breadth int color string props geometry } rect2 := new ( rectangle ) rect2 . props . area 0 type Contact struct { phone , address string } type Employee struct { name string salary int contact Contact } func ( c * Contact ) changePhone ( newPhone string ) { c . phone = newPhone } e := Employee { name : \"Ross Geller\" , salary : 1200 , contact : Contact { phone : \"011 8080 8080\" , address : \"New Delhi, India\" , }, } f := & e e {Ross Geller 1200 {011 8080 8080 New Delhi, India}} e . contact . changePhone ( \"011 1010 1222\" ) e {Ross Geller 1200 {011 1010 1222 New Delhi, India}} f . salary = 1300 e {Ross Geller 1300 {011 1010 1222 New Delhi, India}} Methods \u00b6 Medium repl.go:1:7: expected ';', found 'IDENT' https","title":"G0 01"},{"location":"_nb/%3DGo/G0%2001/#methods","text":"Medium repl.go:1:7: expected ';', found 'IDENT' https","title":"Methods"},{"location":"_nb/%3DGo/Untitled/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import ( \"encoding/json\" \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"time\" ) type people struct { Number int `json:\"number\"` } func main ( url string ) { spaceClient := http . Client { Timeout : time . Second * 2 , // Maximum of 2 secs } req , err := http . NewRequest ( http . MethodGet , url , nil ) if err != nil { log . Fatal ( err ) } req . Header . Set ( \"User-Agent\" , \"spacecount-tutorial\" ) res , getErr := spaceClient . Do ( req ) if getErr != nil { log . Fatal ( getErr ) } body , readErr := ioutil . ReadAll ( res . Body ) if readErr != nil { log . Fatal ( readErr ) } people1 := people {} jsonErr := json . Unmarshal ( body , & people1 ) if jsonErr != nil { log . Fatal ( jsonErr ) } fmt . Println ( people1 . Number ) } main ( \"http://api.open-notify.org/astros.json\" ) 6","title":"Untitled"},{"location":"_nb/%3DJS/Apache%20arrow/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Imports \u00b6 const fetch = require ( \"node-fetch\" ); const arrow = require ( 'apache-arrow' ); dataUrl = \"https://raw.githubusercontent.com/RandomFractals/ChicagoCrimes/master/data/chicago-crimes-2017.arrow\" 'https://raw.githubusercontent.com/RandomFractals/ChicagoCrimes/master/data/chicago-crimes-2017.arrow' async function loadData ( dataUrl ){ const response = await fetch ( dataUrl ); return await response . arrayBuffer (); } fetch ( dataUrl ) . then ( res => { console . log ( res . ok ); console . log ( res . status ); console . log ( res . statusText ); console . log ( res . headers . raw ()); console . log ( res . headers . get ( 'content-type' )); }); true 200 OK [Object: null prototype] { 'content-security-policy': [ \"default-src 'none'; style-src 'unsafe-inline'; sandbox\" ], 'strict-transport-security': [ 'max-age=31536000' ], 'x-content-type-options': [ 'nosniff' ], 'x-frame-options': [ 'deny' ], 'x-xss-protection': [ '1; mode=block' ], etag: [ 'W/\"54801d6e8e8bce558eb964790a95105e16c4a597fef89e71c2edab21e18189a8\"' ], 'content-type': [ 'application/octet-stream' ], 'cache-control': [ 'max-age=300' ], 'x-geo-block-list': [ '' ], 'x-github-request-id': [ '9C90:1231:66FED:82564:5DC740BD' ], 'content-length': [ '3975450' ], 'accept-ranges': [ 'bytes' ], date: [ 'Sat, 09 Nov 2019 22:42:06 GMT' ], via: [ '1.1 varnish' ], connection: [ 'close' ], 'x-served-by': [ 'cache-hhn4067-HHN' ], 'x-cache': [ 'MISS' ], 'x-cache-hits': [ '0' ], 'x-timer': [ 'S1573339327.588676,VS0,VE208' ], vary: [ 'Authorization,Accept-Encoding' ], 'access-control-allow-origin': [ '*' ], 'x-fastly-request-id': [ '5480e6580b4a34022333d56daef2d409d3fa8c4e' ], expires: [ 'Sat, 09 Nov 2019 22:47:06 GMT' ], 'source-age': [ '0' ] } application/octet-stream const getBuffer = async url => { try { const response = await fetch(url); return await response.arrayBuffer(); } catch (error) { console.log(error); } }; buf = loadData ( dataUrl ) in script 1 in script 2 ArrayBuffer { [Uint8Contents]: <41 52 52 4f 57 31 00 00 00 eb 02 00 00 10 00 00 00 0c 00 0e 00 06 00 05 00 08 00 00 00 0c 00 00 00 00 01 02 00 10 00 00 00 00 00 0a 00 0c 00 00 00 04 00 08 00 0a 00 00 00 90 01 00 00 04 00 00 00 01 00 00 00 0c 00 00 00 08 00 0c 00 04 00 08 00 08 00 00 00 08 00 00 00 10 00 00 00 06 00 00 00 70 61 6e ... 3975350 more bytes>, byteLength: 3975450 } buf ArrayBuffer { [Uint8Contents]: <41 52 52 4f 57 31 00 00 00 eb 02 00 00 10 00 00 00 0c 00 0e 00 06 00 05 00 08 00 00 00 0c 00 00 00 00 01 02 00 10 00 00 00 00 00 0a 00 0c 00 00 00 04 00 08 00 0a 00 00 00 90 01 00 00 04 00 00 00 01 00 00 00 0c 00 00 00 08 00 0c 00 04 00 08 00 08 00 00 00 08 00 00 00 10 00 00 00 06 00 00 00 70 61 6e ... 3975350 more bytes>, byteLength: 3975450 } aze = { aze : 1 , qsd : \"aze\" } { aze: 1, qsd: 'aze' } t0 = new arrow . Float64 () Float64 [Float] { precision: 2 } f0 = new arrow . Field ( 'signal 0' , t0 , nullable = true , metadata = { label : 'A simple label' , unit : \"A label containing utf-8 character: \u00b5\" }) Field { name: 'signal 0', type: Float64 [Float] { precision: 2 }, nullable: true, metadata: { label: 'A simple label', unit: 'A label containing utf-8 character: \u00b5' } } function sayHello () { console . log ( \"Hello, World!\" ); } var t = setTimeout ( sayHello , 1000 ); LENGTH 2000 Hello, World! $$ . async (); console . log ( \"Hello, World!\" ); setTimeout ( $$ . done , 1000 ); \u0002hzzhzkh:20\u0003 let LENGTH = 2000 $$ . clear ({}) fields = [ { name : 'precipitation' , type : { name : 'floatingpoint' , precision : 'SINGLE' }, nullable : false , children : [] }, { name : 'date' , type : { name : 'date' , unit : 'MILLISECOND' }, nullable : false , children : [] } ] [ { name: 'precipitation', type: { name: 'floatingpoint', precision: 'SINGLE' }, nullable: false, children: [] }, { name: 'date', type: { name: 'date', unit: 'MILLISECOND' }, nullable: false, children: [] } ] Manipulating Flat Arrays, Arrow-Style \u00b6 https://observablehq.com/@lmeyerov/manipulating-flat-arrays-arrow-style rainAmounts = Array . from ({ length : LENGTH }, () => Number (( Math . random () * 20 ). toFixed ( 1 ))) rainDates = Array . from ({ length : LENGTH }, ( _ , i ) => Date . now () - 1000 * 60 * 60 * 24 * i ) rainfall = arrow . Table . from ({ schema : { fields : fields }, batches : [{ count : LENGTH , columns : [ { name : \"precipitation\" , count : LENGTH , VALIDITY : [], DATA : rainAmounts }, { name : \"date\" , count : LENGTH , VALIDITY : [], DATA : rainDates } ] }] }) Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 2, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 2000 ], _length: 2000, _numChildren: 2, _schema: Schema { fields: [ [Field], [Field] ], metadata: Map {}, dictionaries: Map {} } } rainfall . count () 2000 rainfall . get ( 10 ). toString () '{ \"precipitation\": 17.899999618530273, \"date\": Wed Oct 30 2019 11:17:50 GMT+0000 (Coordinated Universal Time) }' tab = arrow . Table . from ( new Uint8Array ([ 1 , 2 , 4 , 3 ])) Table { _nullCount: -1, _type: Struct { children: [] }, _chunks: [ _InternalEmptyPlaceholderRecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 0, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 0 ], _length: 0, _numChildren: 0, _schema: Schema { fields: [], metadata: Map {}, dictionaries: Map {} } } tab Schema { fields: [], metadata: Map {}, dictionaries: Map {} } data = fetch ( dataUrl ) . then ( res => { if ( res . status >= 400 ) { throw new Error ( \"Bad response from server\" ); } return res . arrayBuffer (); }) . then ( buffer => { return arrow . Table . from ( new Uint8Array ( buffer )); }) . catch ( err => { console . error ( err ); }); Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 3, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 165567 ], _length: 165567, _numChildren: 3, _schema: Schema { fields: [ [Field], [Field], [Field] ], metadata: Map { 'pandas' => '{\"index_columns\": [\"Date\"], \"columns\": [{\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Latitude\"}, {\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Longitude\"}, {\"numpy_type\": \"datetime64[ns]\", \"pandas_type\": \"datetime\", \"metadata\": null, \"name\": \"Date\"}], \"pandas_version\": \"0.20.3\"}' }, dictionaries: Map {} } } data Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 3, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 165567 ], _length: 165567, _numChildren: 3, _schema: Schema { fields: [ [Field], [Field], [Field] ], metadata: Map { 'pandas' => '{\"index_columns\": [\"Date\"], \"columns\": [{\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Latitude\"}, {\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Longitude\"}, {\"numpy_type\": \"datetime64[ns]\", \"pandas_type\": \"datetime\", \"metadata\": null, \"name\": \"Date\"}], \"pandas_version\": \"0.20.3\"}' }, dictionaries: Map {} } } crimes = loadData ( dataUrl ). then ( buffer => arrow . Table . from ( new Uint8Array ( buffer ))) in script 1 in script 2 Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 3, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 165567 ], _length: 165567, _numChildren: 3, _schema: Schema { fields: [ [Field], [Field], [Field] ], metadata: Map { 'pandas' => '{\"index_columns\": [\"Date\"], \"columns\": [{\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Latitude\"}, {\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Longitude\"}, {\"numpy_type\": \"datetime64[ns]\", \"pandas_type\": \"datetime\", \"metadata\": null, \"name\": \"Date\"}], \"pandas_version\": \"0.20.3\"}' }, dictionaries: Map {} } } rowCount = crimes . count () evalmachine.<anonymous>:1 rowCount = crimes.count() ^ TypeError: crimes.count is not a function at evalmachine.<anonymous>:1:19 at Script.runInThisContext (vm.js:124:20) at Object.runInThisContext (vm.js:314:38) at run ([eval]:1054:15) at onRunRequest ([eval]:888:18) at onMessage ([eval]:848:13) at process.emit (events.js:193:13) at emit (internal/child_process.js:848:12) at processTicksAndRejections (internal/process/task_queues.js:81:17) ( new Array ( 1 , 2 )). map (( elt , i ) => elt ) [ 1, 2 ]","title":"Apache arrow"},{"location":"_nb/%3DJS/Apache%20arrow/#imports","text":"const fetch = require ( \"node-fetch\" ); const arrow = require ( 'apache-arrow' ); dataUrl = \"https://raw.githubusercontent.com/RandomFractals/ChicagoCrimes/master/data/chicago-crimes-2017.arrow\" 'https://raw.githubusercontent.com/RandomFractals/ChicagoCrimes/master/data/chicago-crimes-2017.arrow' async function loadData ( dataUrl ){ const response = await fetch ( dataUrl ); return await response . arrayBuffer (); } fetch ( dataUrl ) . then ( res => { console . log ( res . ok ); console . log ( res . status ); console . log ( res . statusText ); console . log ( res . headers . raw ()); console . log ( res . headers . get ( 'content-type' )); }); true 200 OK [Object: null prototype] { 'content-security-policy': [ \"default-src 'none'; style-src 'unsafe-inline'; sandbox\" ], 'strict-transport-security': [ 'max-age=31536000' ], 'x-content-type-options': [ 'nosniff' ], 'x-frame-options': [ 'deny' ], 'x-xss-protection': [ '1; mode=block' ], etag: [ 'W/\"54801d6e8e8bce558eb964790a95105e16c4a597fef89e71c2edab21e18189a8\"' ], 'content-type': [ 'application/octet-stream' ], 'cache-control': [ 'max-age=300' ], 'x-geo-block-list': [ '' ], 'x-github-request-id': [ '9C90:1231:66FED:82564:5DC740BD' ], 'content-length': [ '3975450' ], 'accept-ranges': [ 'bytes' ], date: [ 'Sat, 09 Nov 2019 22:42:06 GMT' ], via: [ '1.1 varnish' ], connection: [ 'close' ], 'x-served-by': [ 'cache-hhn4067-HHN' ], 'x-cache': [ 'MISS' ], 'x-cache-hits': [ '0' ], 'x-timer': [ 'S1573339327.588676,VS0,VE208' ], vary: [ 'Authorization,Accept-Encoding' ], 'access-control-allow-origin': [ '*' ], 'x-fastly-request-id': [ '5480e6580b4a34022333d56daef2d409d3fa8c4e' ], expires: [ 'Sat, 09 Nov 2019 22:47:06 GMT' ], 'source-age': [ '0' ] } application/octet-stream const getBuffer = async url => { try { const response = await fetch(url); return await response.arrayBuffer(); } catch (error) { console.log(error); } }; buf = loadData ( dataUrl ) in script 1 in script 2 ArrayBuffer { [Uint8Contents]: <41 52 52 4f 57 31 00 00 00 eb 02 00 00 10 00 00 00 0c 00 0e 00 06 00 05 00 08 00 00 00 0c 00 00 00 00 01 02 00 10 00 00 00 00 00 0a 00 0c 00 00 00 04 00 08 00 0a 00 00 00 90 01 00 00 04 00 00 00 01 00 00 00 0c 00 00 00 08 00 0c 00 04 00 08 00 08 00 00 00 08 00 00 00 10 00 00 00 06 00 00 00 70 61 6e ... 3975350 more bytes>, byteLength: 3975450 } buf ArrayBuffer { [Uint8Contents]: <41 52 52 4f 57 31 00 00 00 eb 02 00 00 10 00 00 00 0c 00 0e 00 06 00 05 00 08 00 00 00 0c 00 00 00 00 01 02 00 10 00 00 00 00 00 0a 00 0c 00 00 00 04 00 08 00 0a 00 00 00 90 01 00 00 04 00 00 00 01 00 00 00 0c 00 00 00 08 00 0c 00 04 00 08 00 08 00 00 00 08 00 00 00 10 00 00 00 06 00 00 00 70 61 6e ... 3975350 more bytes>, byteLength: 3975450 } aze = { aze : 1 , qsd : \"aze\" } { aze: 1, qsd: 'aze' } t0 = new arrow . Float64 () Float64 [Float] { precision: 2 } f0 = new arrow . Field ( 'signal 0' , t0 , nullable = true , metadata = { label : 'A simple label' , unit : \"A label containing utf-8 character: \u00b5\" }) Field { name: 'signal 0', type: Float64 [Float] { precision: 2 }, nullable: true, metadata: { label: 'A simple label', unit: 'A label containing utf-8 character: \u00b5' } } function sayHello () { console . log ( \"Hello, World!\" ); } var t = setTimeout ( sayHello , 1000 ); LENGTH 2000 Hello, World! $$ . async (); console . log ( \"Hello, World!\" ); setTimeout ( $$ . done , 1000 ); \u0002hzzhzkh:20\u0003 let LENGTH = 2000 $$ . clear ({}) fields = [ { name : 'precipitation' , type : { name : 'floatingpoint' , precision : 'SINGLE' }, nullable : false , children : [] }, { name : 'date' , type : { name : 'date' , unit : 'MILLISECOND' }, nullable : false , children : [] } ] [ { name: 'precipitation', type: { name: 'floatingpoint', precision: 'SINGLE' }, nullable: false, children: [] }, { name: 'date', type: { name: 'date', unit: 'MILLISECOND' }, nullable: false, children: [] } ]","title":"Imports"},{"location":"_nb/%3DJS/Apache%20arrow/#manipulating-flat-arrays-arrow-style","text":"https://observablehq.com/@lmeyerov/manipulating-flat-arrays-arrow-style rainAmounts = Array . from ({ length : LENGTH }, () => Number (( Math . random () * 20 ). toFixed ( 1 ))) rainDates = Array . from ({ length : LENGTH }, ( _ , i ) => Date . now () - 1000 * 60 * 60 * 24 * i ) rainfall = arrow . Table . from ({ schema : { fields : fields }, batches : [{ count : LENGTH , columns : [ { name : \"precipitation\" , count : LENGTH , VALIDITY : [], DATA : rainAmounts }, { name : \"date\" , count : LENGTH , VALIDITY : [], DATA : rainDates } ] }] }) Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 2, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 2000 ], _length: 2000, _numChildren: 2, _schema: Schema { fields: [ [Field], [Field] ], metadata: Map {}, dictionaries: Map {} } } rainfall . count () 2000 rainfall . get ( 10 ). toString () '{ \"precipitation\": 17.899999618530273, \"date\": Wed Oct 30 2019 11:17:50 GMT+0000 (Coordinated Universal Time) }' tab = arrow . Table . from ( new Uint8Array ([ 1 , 2 , 4 , 3 ])) Table { _nullCount: -1, _type: Struct { children: [] }, _chunks: [ _InternalEmptyPlaceholderRecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 0, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 0 ], _length: 0, _numChildren: 0, _schema: Schema { fields: [], metadata: Map {}, dictionaries: Map {} } } tab Schema { fields: [], metadata: Map {}, dictionaries: Map {} } data = fetch ( dataUrl ) . then ( res => { if ( res . status >= 400 ) { throw new Error ( \"Bad response from server\" ); } return res . arrayBuffer (); }) . then ( buffer => { return arrow . Table . from ( new Uint8Array ( buffer )); }) . catch ( err => { console . error ( err ); }); Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 3, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 165567 ], _length: 165567, _numChildren: 3, _schema: Schema { fields: [ [Field], [Field], [Field] ], metadata: Map { 'pandas' => '{\"index_columns\": [\"Date\"], \"columns\": [{\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Latitude\"}, {\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Longitude\"}, {\"numpy_type\": \"datetime64[ns]\", \"pandas_type\": \"datetime\", \"metadata\": null, \"name\": \"Date\"}], \"pandas_version\": \"0.20.3\"}' }, dictionaries: Map {} } } data Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 3, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 165567 ], _length: 165567, _numChildren: 3, _schema: Schema { fields: [ [Field], [Field], [Field] ], metadata: Map { 'pandas' => '{\"index_columns\": [\"Date\"], \"columns\": [{\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Latitude\"}, {\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Longitude\"}, {\"numpy_type\": \"datetime64[ns]\", \"pandas_type\": \"datetime\", \"metadata\": null, \"name\": \"Date\"}], \"pandas_version\": \"0.20.3\"}' }, dictionaries: Map {} } } crimes = loadData ( dataUrl ). then ( buffer => arrow . Table . from ( new Uint8Array ( buffer ))) in script 1 in script 2 Table { _nullCount: -1, _type: Struct { children: [ [Field], [Field], [Field] ] }, _chunks: [ RecordBatch [StructVector<Struct>] { _children: undefined, numChildren: 3, data: [Data], _schema: [Schema] } ], _chunkOffsets: Uint32Array [ 0, 165567 ], _length: 165567, _numChildren: 3, _schema: Schema { fields: [ [Field], [Field], [Field] ], metadata: Map { 'pandas' => '{\"index_columns\": [\"Date\"], \"columns\": [{\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Latitude\"}, {\"numpy_type\": \"float64\", \"pandas_type\": \"float64\", \"metadata\": null, \"name\": \"Longitude\"}, {\"numpy_type\": \"datetime64[ns]\", \"pandas_type\": \"datetime\", \"metadata\": null, \"name\": \"Date\"}], \"pandas_version\": \"0.20.3\"}' }, dictionaries: Map {} } } rowCount = crimes . count () evalmachine.<anonymous>:1 rowCount = crimes.count() ^ TypeError: crimes.count is not a function at evalmachine.<anonymous>:1:19 at Script.runInThisContext (vm.js:124:20) at Object.runInThisContext (vm.js:314:38) at run ([eval]:1054:15) at onRunRequest ([eval]:888:18) at onMessage ([eval]:848:13) at process.emit (events.js:193:13) at emit (internal/child_process.js:848:12) at processTicksAndRejections (internal/process/task_queues.js:81:17) ( new Array ( 1 , 2 )). map (( elt , i ) => elt ) [ 1, 2 ]","title":"Manipulating Flat Arrays, Arrow-Style"},{"location":"_nb/%3DJS/Ramda%20-%20functionnal%20style/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); https://github.com/ramda/ramda","title":"Ramda   functionnal style"},{"location":"_nb/%3DJS/array/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); let foo = [ \"fee\" , \"fi\" , \"fo\" , \"fum\" ]; let numbers = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]; Copying/Cloning an Array: Array.slice \u00b6 doc (fr) let fooCopy = foo . slice (); console . log ( foo == fooCopy , foo === fooCopy ) false false Checking if an Object Is an Array \u00b6 https://tc39.es/ecma262/#sec-relational-operators Array . isArray ( numbers ) true numbers . constructor === Array true let name = \"Homer Simpson\" ; name . constructor === Array false Deleting an Array Item \u00b6 let evenNumbers = [ 0 , 2 , 4 , 6 , 7 , 8 , 10 ]; console . log ( evenNumbers ) console . log ( evenNumbers . length ) // 7 items [ 0, 2, 4, 6, 7, 8, 10 ] 7 Array.splice replace (or remove) element inplace when delete nullate the element delete evenNumbers [ 1 ]; console . log ( evenNumbers ) [ 0, <1 empty item>, 4, 6, 7, 8, 10 ] evenNumbers . splice ( 1 , 2 , 10 , 20 ); console . log ( evenNumbers ) [ 0, 10, 20, 6, 7, 8, 10 ] evenNumbers . splice ( 1 , 2 ); console . log ( evenNumbers ) [ 0, 6, 7, 8, 10 ] Get index of element in Array \u00b6 let removeIndex = evenNumbers . indexOf ( 7 ); if ( removeIndex > - 1 ) { evenNumbers . splice ( removeIndex , 1 ); } console . log ( evenNumbers . length ) console . log ( evenNumbers ) 3 [ 0, 6, 10 ] Emptying an Array \u00b6 let myItems = [ \"apples\" , \"oranges\" , \"bananas\" , \"kiwis\" ]; console . log ( myItems . length ); // 4 myItems . length = 0 ; console . log ( myItems . length ); // 0 4 0 Extract unique elements of Array with Set \u00b6 let names = [ \"Peter\" , \"Joe\" , \"Cleveland\" , \"Quagmire\" , \"Joe\" ]; let uniqueNames = [... new Set ( names )]; console . log ( uniqueNames ); [ 'Peter', 'Joe', 'Cleveland', 'Quagmire' ] new Set ( names ) Set { 'Peter', 'Joe', 'Cleveland', 'Quagmire' } Array sort \u00b6 doc (fr) let numbers8 = [ 3 , 10 , 2 , 14 , 7 , 2 , 9 , 5 ]; let beatles = [ \"Ringo\" , \"George\" , \"Paul\" , \"John\" ]; numbers8 . sort ( compareValues ); beatles . sort ( compareValues ); function compareValues ( a , b ) { if ( a < b ) { // if a less than b return - 1 ; } else if ( a > b ) { // if a greater than b return 1 ; } else { // a and b are equal return 0 ; } } console . log ( numbers8 ); console . log ( beatles ); [ 2, 2, 3, 5, 7, 9, 10, 14 ] [ 'George', 'John', 'Paul', 'Ringo' ]","title":"Array"},{"location":"_nb/%3DJS/array/#copyingcloning-an-array-arrayslice","text":"doc (fr) let fooCopy = foo . slice (); console . log ( foo == fooCopy , foo === fooCopy ) false false","title":"Copying/Cloning an Array: Array.slice"},{"location":"_nb/%3DJS/array/#checking-if-an-object-is-an-array","text":"https://tc39.es/ecma262/#sec-relational-operators Array . isArray ( numbers ) true numbers . constructor === Array true let name = \"Homer Simpson\" ; name . constructor === Array false","title":"Checking if an Object Is an Array"},{"location":"_nb/%3DJS/array/#deleting-an-array-item","text":"let evenNumbers = [ 0 , 2 , 4 , 6 , 7 , 8 , 10 ]; console . log ( evenNumbers ) console . log ( evenNumbers . length ) // 7 items [ 0, 2, 4, 6, 7, 8, 10 ] 7 Array.splice replace (or remove) element inplace when delete nullate the element delete evenNumbers [ 1 ]; console . log ( evenNumbers ) [ 0, <1 empty item>, 4, 6, 7, 8, 10 ] evenNumbers . splice ( 1 , 2 , 10 , 20 ); console . log ( evenNumbers ) [ 0, 10, 20, 6, 7, 8, 10 ] evenNumbers . splice ( 1 , 2 ); console . log ( evenNumbers ) [ 0, 6, 7, 8, 10 ]","title":"Deleting an Array Item"},{"location":"_nb/%3DJS/array/#get-index-of-element-in-array","text":"let removeIndex = evenNumbers . indexOf ( 7 ); if ( removeIndex > - 1 ) { evenNumbers . splice ( removeIndex , 1 ); } console . log ( evenNumbers . length ) console . log ( evenNumbers ) 3 [ 0, 6, 10 ]","title":"Get index of element in Array"},{"location":"_nb/%3DJS/array/#emptying-an-array","text":"let myItems = [ \"apples\" , \"oranges\" , \"bananas\" , \"kiwis\" ]; console . log ( myItems . length ); // 4 myItems . length = 0 ; console . log ( myItems . length ); // 0 4 0","title":"Emptying an Array"},{"location":"_nb/%3DJS/array/#extract-unique-elements-of-array-with-set","text":"let names = [ \"Peter\" , \"Joe\" , \"Cleveland\" , \"Quagmire\" , \"Joe\" ]; let uniqueNames = [... new Set ( names )]; console . log ( uniqueNames ); [ 'Peter', 'Joe', 'Cleveland', 'Quagmire' ] new Set ( names ) Set { 'Peter', 'Joe', 'Cleveland', 'Quagmire' }","title":"Extract unique elements of Array with Set"},{"location":"_nb/%3DJS/array/#array-sort","text":"doc (fr) let numbers8 = [ 3 , 10 , 2 , 14 , 7 , 2 , 9 , 5 ]; let beatles = [ \"Ringo\" , \"George\" , \"Paul\" , \"John\" ]; numbers8 . sort ( compareValues ); beatles . sort ( compareValues ); function compareValues ( a , b ) { if ( a < b ) { // if a less than b return - 1 ; } else if ( a > b ) { // if a greater than b return 1 ; } else { // a and b are equal return 0 ; } } console . log ( numbers8 ); console . log ( beatles ); [ 2, 2, 3, 5, 7, 9, 10, 14 ] [ 'George', 'John', 'Paul', 'Ringo' ]","title":"Array sort"},{"location":"_nb/%3DJS/fetch/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); fetch in node.js const fetch = require ( 'node-fetch' ); fetch ( 'https://httpbin.org/post' , { method : 'POST' , body : 'a=1' }) . then ( res => res . json ()) // expecting a json response . then ( json => console . log ( json . headers )); { Accept: '*/*', 'Accept-Encoding': 'gzip,deflate', 'Content-Length': '3', 'Content-Type': 'text/plain;charset=UTF-8', Host: 'httpbin.org', 'User-Agent': 'node-fetch/1.0 (+https://github.com/bitinn/node-fetch)' }","title":"Fetch"},{"location":"_nb/%3DJS/viz%20-%20vega/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); var vega = require ( 'vega' ); const fetch = require ( 'node-fetch' ); var view ; fetch ( 'https://vega.github.io/vega/examples/bar-chart.vg.json' ) . then ( res => res . json ()) . then ( spec => render ( spec )) . catch ( err => console . error ( err )); function render ( spec ) { view = new vega . View ( vega . parse ( spec ), { renderer : 'canvas' , // renderer (canvas or svg) container : '#view' , // parent DOM container hover : true // enable hover processing }); return view . runAsync (); } ERROR DOM document instance not found. TypeError: Cannot read property 'getContext' of null at resize (/home/jovyan/node_modules/vega-scenegraph/build/vega-scenegraph.js:3442:26) at CanvasRenderer.prototype$6.resize (/home/jovyan/node_modules/vega-scenegraph/build/vega-scenegraph.js:3492:5) at CanvasRenderer.prototype$4.initialize (/home/jovyan/node_modules/vega-scenegraph/build/vega-scenegraph.js:3054:17) at CanvasRenderer.prototype$6.initialize (/home/jovyan/node_modules/vega-scenegraph/build/vega-scenegraph.js:3487:28) at initializeRenderer (/home/jovyan/node_modules/vega-view/build/vega-view.js:630:8) at View.initialize (/home/jovyan/node_modules/vega-view/build/vega-view.js:677:9) at new View (/home/jovyan/node_modules/vega-view/build/vega-view.js:1060:33) at render (evalmachine.<anonymous>:9:10) at fetch.then.then.spec (evalmachine.<anonymous>:5:17) at processTicksAndRejections (internal/process/task_queues.js:86:5) fetch ( 'https://vega.github.io/vega/examples/bar-chart.vg.json' ) . then ( res => res . json ()) { '$schema': 'https://vega.github.io/schema/vega/v5.json', width: 400, height: 200, padding: 5, data: [ { name: 'table', values: [Array] } ], signals: [ { name: 'tooltip', value: {}, on: [Array] } ], scales: [ { name: 'xscale', type: 'band', domain: [Object], range: 'width', padding: 0.05, round: true }, { name: 'yscale', domain: [Object], nice: true, range: 'height' } ], axes: [ { orient: 'bottom', scale: 'xscale' }, { orient: 'left', scale: 'yscale' } ], marks: [ { type: 'rect', from: [Object], encode: [Object] }, { type: 'text', encode: [Object] } ] }","title":"Viz   vega"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is often described as a \"batteries included\" language due to its comprehensive standard library. [src] Design of CPython\u2019s Compiler [src] Philip Guo CPython internals lectures [src] PyOhio PyCamp 2014 :: The Compiler [src] The missing Python AST docs [src] Visualize program execution High-level \u00b6 In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language Wikipedia Interpreted \u00b6 There are three general modes of execution for modern high-level languages: Interpreted : the syntax is read and then executed directly, with no compilation stage. A program called an interpreter reads each program statement, following the program flow, then decides what to do, and does it. Compiled : the code written in a language is compiled, its syntax is transformed into an executable form before running. There are two types of compilation: Machine code generation Intermediate representations : the code written in a language is compiled to an intermediate representation, that representation can be optimized or saved for later execution without the need to re-read the source file. When the intermediate representation is saved, it may be in a form such as bytecode. The intermediate representation must then be interpreted or further compiled to execute it. Source-to-source translated or transcompiled Code structure \u00b6 The code directory structure is described in Python Developer's Guide . Guido van Rossum summarizes it in Yet another guided tour of CPython Include \u2014 header files Objects \u2014 object implementations, from int to type Python \u2014 interpreter, bytecode compiler and other essential infrastructure Parser \u2014 parser, lexer and parser generator Modules \u2014 stdlib extension modules, and main.c Programs \u2014 not much, but has the real main() function cpython/ \u2502 \u251c\u2500\u2500 Doc \u2190 Source for the documentation \u251c\u2500\u2500 Grammar \u2190 The computer-readable language definition \u251c\u2500\u2500 Include \u2190 The C header files \u251c\u2500\u2500 Lib \u2190 Standard library modules written in Python \u251c\u2500\u2500 Mac \u2190 macOS support files \u251c\u2500\u2500 Misc \u2190 Miscellaneous files \u251c\u2500\u2500 Modules \u2190 Standard Library Modules written in C \u251c\u2500\u2500 Objects \u2190 Core types and the object model \u251c\u2500\u2500 Parser \u2190 The Python parser source code \u251c\u2500\u2500 PC \u2190 Windows build support files \u251c\u2500\u2500 PCbuild \u2190 Windows build support files for older Windows versions \u251c\u2500\u2500 Programs \u2190 Source code for the python executable and other binaries \u251c\u2500\u2500 Python \u2190 The CPython interpreter source code \u2514\u2500\u2500 Tools \u2190 Standalone tools useful for building or extending Python In CPython, the compilation from source code to bytecode involves several steps [src] Parse source code into a parse tree (Parser/pgen.c) Transform parse tree into an Abstract Syntax Tree (Python/ast.c) Transform AST into a Control Flow Graph (Python/compile.c) Emit bytecode based on the Control Flow Graph (Python/compile.c) AST \u00b6 Doc The code is parsed, i.e. split up into a list of pieces called tokens. These tokens are based on a set of rules for things that should be treated differently. For instance, the keyword if is a different token than a numeric value like 42 . The list of tokens is transformed to build an Abstract Syntax Tree, AST , collection of nodes which are linked together based on the Python language grammar . In python, everything is an object, AST represents logicaly each element as an object. A third pary documentation on AST import ast from ast import PyCF_ONLY_AST # The code is beeing striped to remove # left and right space before parsing # and limit thee size of the tree code = \"\"\" def hello(who: str) -> None: msg = f'Hello {who} ' print(msg) hello(\"world\") \"\"\" code = code . strip () Execute the code exec ( code ) Hello world tree = ast . parse ( code ) The code is seen as two elements in the tree for i , elt in enumerate ( tree . body ): print ( i , elt ) 0 <_ast.FunctionDef object at 0x7f79c9aa0690> 1 <_ast.Expr object at 0x7f79c9ad1890> # Get the FunctionDef fdef = tree . body [ 0 ] # Get the first function argument arg = fdef . args . args [ 0 ] Some informations from the tree can be retrieved: print ( f ' { arg . arg } : { arg . annotation . id } at col # { arg . col_offset } ' ) who: str at col #10 The code can be compiled lines = [ None ] + code . splitlines () # None at [0] so we can index lines from 1 test_namespace = {} for node in tree . body : wrapper = ast . Module ( body = [ node ]) try : co = compile ( wrapper , \"<ast>\" , 'exec' ) exec ( co , test_namespace ) except AssertionError as e : print ( \"Assertion failed on line\" , node . lineno , \":\" ) print ( lines [ node . lineno ]) # If the error has a message, show it. if e . args : print ( e ) print () Hello world code = compile ( tree , filename = \"<ast>\" , mode = \"exec\" ) class FuncLister ( ast . NodeVisitor ): def visit_FunctionDef ( self , node ): print ( node . name ) self . generic_visit ( node ) FuncLister () . visit ( tree ) hello Bytecode \u00b6 From an abstract syntax tree, the interpreter can produce a lower level form of instructions called bytecode . These instructions are things like BINARY_ADD and are meant to be very generic so that a computer can run them. print ( \"Hello, World!\" ) Hello, World! from dis import dis dis ( 'print(\"Hello, World!\")' ) 1 0 LOAD_NAME 0 (print) 2 LOAD_CONST 0 ('Hello, World!') 4 CALL_FUNCTION 1 6 RETURN_VALUE CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can \"push\" an item onto the \"top\" of the structure, or \"pop\" an item off the \"top\"). With the bytecode instructions available, the interpreter can finally run your code. The bytecode is used to call functions in your operating system which will ultimately interact with a CPU and memory to run the program. Opcodes and main interpreter loop \u00b6 import tempfile from os.path import join as pjoin tmp = tempfile . gettempdir () fName = pjoin ( tmp , 'test.py' ) codeExemple = \"\"\"ppl = ['Alice', 'Bob', 'Carol', 'Doug'] excited_ppl = [e + '!!' for e in ppl] ppl_len = [len(x) for x in ppl]\"\"\" with open ( fName , 'w' ) as f : f . write ( codeExemple ) ! python - m dis { fName } 1 0 LOAD_CONST 0 ('Alice') 2 LOAD_CONST 1 ('Bob') 4 LOAD_CONST 2 ('Carol') 6 LOAD_CONST 3 ('Doug') 8 BUILD_LIST 4 10 STORE_NAME 0 (ppl) 2 12 LOAD_CONST 4 (<code object <listcomp> at 0x7f43f9dc4ed0, file \"/tmp/test.py\", line 2>) 14 LOAD_CONST 5 ('<listcomp>') 16 MAKE_FUNCTION 0 18 LOAD_NAME 0 (ppl) 20 GET_ITER 22 CALL_FUNCTION 1 24 STORE_NAME 1 (excited_ppl) 3 26 LOAD_CONST 6 (<code object <listcomp> at 0x7f43f9dca390, file \"/tmp/test.py\", line 3>) 28 LOAD_CONST 5 ('<listcomp>') 30 MAKE_FUNCTION 0 32 LOAD_NAME 0 (ppl) 34 GET_ITER 36 CALL_FUNCTION 1 38 STORE_NAME 2 (ppl_len) 40 LOAD_CONST 7 (None) 42 RETURN_VALUE Disassembly of <code object <listcomp> at 0x7f43f9dc4ed0, file \"/tmp/test.py\", line 2>: 2 0 BUILD_LIST 0 2 LOAD_FAST 0 (.0) >> 4 FOR_ITER 12 (to 18) 6 STORE_FAST 1 (e) 8 LOAD_FAST 1 (e) 10 LOAD_CONST 0 ('!!') 12 BINARY_ADD 14 LIST_APPEND 2 16 JUMP_ABSOLUTE 4 >> 18 RETURN_VALUE Disassembly of <code object <listcomp> at 0x7f43f9dca390, file \"/tmp/test.py\", line 3>: 3 0 BUILD_LIST 0 2 LOAD_FAST 0 (.0) >> 4 FOR_ITER 12 (to 18) 6 STORE_FAST 1 (x) 8 LOAD_GLOBAL 0 (len) 10 LOAD_FAST 1 (x) 12 CALL_FUNCTION 1 14 LIST_APPEND 2 16 JUMP_ABSOLUTE 4 >> 18 RETURN_VALUE c = compile ( codeExemple , 'test.py' , 'exec' ) c . co_code b'd\\x00d\\x01d\\x02d\\x03g\\x04Z\\x00d\\x04d\\x05\\x84\\x00e\\x00D\\x00\\x83\\x01Z\\x01d\\x06d\\x05\\x84\\x00e\\x00D\\x00\\x83\\x01Z\\x02d\\x07S\\x00' ! cat { fName } ppl = ['Alice', 'Bob', 'Carol', 'Doug'] excited_ppl = [e + '!!' for e in ppl] ppl_len = [len(x) for x in ppl] opcode.h \u00b6 ! wget -- quiet https : // raw . githubusercontent . com / python / cpython / master / Include / opcode . h - O { tmp } / opcode . h ! head - n 30 { tmp } / opcode . h /* Auto-generated by Tools/scripts/generate_opcode_h.py from Lib/opcode.py */ ifndef Py_OPCODE_H \u00b6 define Py_OPCODE_H \u00b6 ifdef __cplusplus \u00b6 extern \"C\" { endif \u00b6 /* Instruction opcodes for compiled code */ define POP_TOP 1 \u00b6 define ROT_TWO 2 \u00b6 define ROT_THREE 3 \u00b6 define DUP_TOP 4 \u00b6 define DUP_TOP_TWO 5 \u00b6 define ROT_FOUR 6 \u00b6 define NOP 9 \u00b6 define UNARY_POSITIVE 10 \u00b6 define UNARY_NEGATIVE 11 \u00b6 define UNARY_NOT 12 \u00b6 define UNARY_INVERT 15 \u00b6 define BINARY_MATRIX_MULTIPLY 16 \u00b6 define INPLACE_MATRIX_MULTIPLY 17 \u00b6 define BINARY_POWER 19 \u00b6 define BINARY_MULTIPLY 20 \u00b6 define BINARY_MODULO 22 \u00b6 define BINARY_ADD 23 \u00b6 define BINARY_SUBTRACT 24 \u00b6 define BINARY_SUBSCR 25 \u00b6 define BINARY_FLOOR_DIVIDE 26 \u00b6 define BINARY_TRUE_DIVIDE 27 \u00b6 ceval.c \u00b6 ! wget -- quiet https : // raw . githubusercontent . com / python / cpython / master / Python / ceval . c - O { tmp } / ceval . c ! head - n 20 { tmp } / ceval . c /* Execute compiled code */ /* XXX TO DO: XXX speed up searching for keywords by using a dictionary XXX document it! */ /* enable more aggressive intra-module optimizations, where available */ define PY_LOCAL_AGGRESSIVE \u00b6 include \"Python.h\" \u00b6 include \"pycore_call.h\" \u00b6 include \"pycore_ceval.h\" \u00b6 include \"pycore_code.h\" \u00b6 include \"pycore_object.h\" \u00b6 include \"pycore_pyerrors.h\" \u00b6 include \"pycore_pylifecycle.h\" \u00b6 include \"pycore_pystate.h\" \u00b6 include \"pycore_tupleobject.h\" \u00b6 General purpose \u00b6 https://awesome-python.com/ : References: - Your Guide to the CPython Source Code - Inside The Python Virtual Machine - An introduction to Python bytecode - Deciphering Python: How to use Abstract Syntax Trees (AST) to understand code Garbage collected \u00b6 https://github.com/python/cpython/blob/ce6a070414ed1e1374d1e6212bfbff61b6d5d755/Include/object.h#L104 from sys import getrefcount a = \"un\" getrefcount ( a ) 2 b = a b is a True getrefcount ( a ) 3 del ( b ) getrefcount ( a ) 2","title":"Intro"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#high-level","text":"In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language Wikipedia","title":"High-level"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#interpreted","text":"There are three general modes of execution for modern high-level languages: Interpreted : the syntax is read and then executed directly, with no compilation stage. A program called an interpreter reads each program statement, following the program flow, then decides what to do, and does it. Compiled : the code written in a language is compiled, its syntax is transformed into an executable form before running. There are two types of compilation: Machine code generation Intermediate representations : the code written in a language is compiled to an intermediate representation, that representation can be optimized or saved for later execution without the need to re-read the source file. When the intermediate representation is saved, it may be in a form such as bytecode. The intermediate representation must then be interpreted or further compiled to execute it. Source-to-source translated or transcompiled","title":"Interpreted"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#code-structure","text":"The code directory structure is described in Python Developer's Guide . Guido van Rossum summarizes it in Yet another guided tour of CPython Include \u2014 header files Objects \u2014 object implementations, from int to type Python \u2014 interpreter, bytecode compiler and other essential infrastructure Parser \u2014 parser, lexer and parser generator Modules \u2014 stdlib extension modules, and main.c Programs \u2014 not much, but has the real main() function cpython/ \u2502 \u251c\u2500\u2500 Doc \u2190 Source for the documentation \u251c\u2500\u2500 Grammar \u2190 The computer-readable language definition \u251c\u2500\u2500 Include \u2190 The C header files \u251c\u2500\u2500 Lib \u2190 Standard library modules written in Python \u251c\u2500\u2500 Mac \u2190 macOS support files \u251c\u2500\u2500 Misc \u2190 Miscellaneous files \u251c\u2500\u2500 Modules \u2190 Standard Library Modules written in C \u251c\u2500\u2500 Objects \u2190 Core types and the object model \u251c\u2500\u2500 Parser \u2190 The Python parser source code \u251c\u2500\u2500 PC \u2190 Windows build support files \u251c\u2500\u2500 PCbuild \u2190 Windows build support files for older Windows versions \u251c\u2500\u2500 Programs \u2190 Source code for the python executable and other binaries \u251c\u2500\u2500 Python \u2190 The CPython interpreter source code \u2514\u2500\u2500 Tools \u2190 Standalone tools useful for building or extending Python In CPython, the compilation from source code to bytecode involves several steps [src] Parse source code into a parse tree (Parser/pgen.c) Transform parse tree into an Abstract Syntax Tree (Python/ast.c) Transform AST into a Control Flow Graph (Python/compile.c) Emit bytecode based on the Control Flow Graph (Python/compile.c)","title":"Code structure"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#ast","text":"Doc The code is parsed, i.e. split up into a list of pieces called tokens. These tokens are based on a set of rules for things that should be treated differently. For instance, the keyword if is a different token than a numeric value like 42 . The list of tokens is transformed to build an Abstract Syntax Tree, AST , collection of nodes which are linked together based on the Python language grammar . In python, everything is an object, AST represents logicaly each element as an object. A third pary documentation on AST import ast from ast import PyCF_ONLY_AST # The code is beeing striped to remove # left and right space before parsing # and limit thee size of the tree code = \"\"\" def hello(who: str) -> None: msg = f'Hello {who} ' print(msg) hello(\"world\") \"\"\" code = code . strip () Execute the code exec ( code ) Hello world tree = ast . parse ( code ) The code is seen as two elements in the tree for i , elt in enumerate ( tree . body ): print ( i , elt ) 0 <_ast.FunctionDef object at 0x7f79c9aa0690> 1 <_ast.Expr object at 0x7f79c9ad1890> # Get the FunctionDef fdef = tree . body [ 0 ] # Get the first function argument arg = fdef . args . args [ 0 ] Some informations from the tree can be retrieved: print ( f ' { arg . arg } : { arg . annotation . id } at col # { arg . col_offset } ' ) who: str at col #10 The code can be compiled lines = [ None ] + code . splitlines () # None at [0] so we can index lines from 1 test_namespace = {} for node in tree . body : wrapper = ast . Module ( body = [ node ]) try : co = compile ( wrapper , \"<ast>\" , 'exec' ) exec ( co , test_namespace ) except AssertionError as e : print ( \"Assertion failed on line\" , node . lineno , \":\" ) print ( lines [ node . lineno ]) # If the error has a message, show it. if e . args : print ( e ) print () Hello world code = compile ( tree , filename = \"<ast>\" , mode = \"exec\" ) class FuncLister ( ast . NodeVisitor ): def visit_FunctionDef ( self , node ): print ( node . name ) self . generic_visit ( node ) FuncLister () . visit ( tree ) hello","title":"AST"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#bytecode","text":"From an abstract syntax tree, the interpreter can produce a lower level form of instructions called bytecode . These instructions are things like BINARY_ADD and are meant to be very generic so that a computer can run them. print ( \"Hello, World!\" ) Hello, World! from dis import dis dis ( 'print(\"Hello, World!\")' ) 1 0 LOAD_NAME 0 (print) 2 LOAD_CONST 0 ('Hello, World!') 4 CALL_FUNCTION 1 6 RETURN_VALUE CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can \"push\" an item onto the \"top\" of the structure, or \"pop\" an item off the \"top\"). With the bytecode instructions available, the interpreter can finally run your code. The bytecode is used to call functions in your operating system which will ultimately interact with a CPU and memory to run the program.","title":"Bytecode"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#opcodes-and-main-interpreter-loop","text":"import tempfile from os.path import join as pjoin tmp = tempfile . gettempdir () fName = pjoin ( tmp , 'test.py' ) codeExemple = \"\"\"ppl = ['Alice', 'Bob', 'Carol', 'Doug'] excited_ppl = [e + '!!' for e in ppl] ppl_len = [len(x) for x in ppl]\"\"\" with open ( fName , 'w' ) as f : f . write ( codeExemple ) ! python - m dis { fName } 1 0 LOAD_CONST 0 ('Alice') 2 LOAD_CONST 1 ('Bob') 4 LOAD_CONST 2 ('Carol') 6 LOAD_CONST 3 ('Doug') 8 BUILD_LIST 4 10 STORE_NAME 0 (ppl) 2 12 LOAD_CONST 4 (<code object <listcomp> at 0x7f43f9dc4ed0, file \"/tmp/test.py\", line 2>) 14 LOAD_CONST 5 ('<listcomp>') 16 MAKE_FUNCTION 0 18 LOAD_NAME 0 (ppl) 20 GET_ITER 22 CALL_FUNCTION 1 24 STORE_NAME 1 (excited_ppl) 3 26 LOAD_CONST 6 (<code object <listcomp> at 0x7f43f9dca390, file \"/tmp/test.py\", line 3>) 28 LOAD_CONST 5 ('<listcomp>') 30 MAKE_FUNCTION 0 32 LOAD_NAME 0 (ppl) 34 GET_ITER 36 CALL_FUNCTION 1 38 STORE_NAME 2 (ppl_len) 40 LOAD_CONST 7 (None) 42 RETURN_VALUE Disassembly of <code object <listcomp> at 0x7f43f9dc4ed0, file \"/tmp/test.py\", line 2>: 2 0 BUILD_LIST 0 2 LOAD_FAST 0 (.0) >> 4 FOR_ITER 12 (to 18) 6 STORE_FAST 1 (e) 8 LOAD_FAST 1 (e) 10 LOAD_CONST 0 ('!!') 12 BINARY_ADD 14 LIST_APPEND 2 16 JUMP_ABSOLUTE 4 >> 18 RETURN_VALUE Disassembly of <code object <listcomp> at 0x7f43f9dca390, file \"/tmp/test.py\", line 3>: 3 0 BUILD_LIST 0 2 LOAD_FAST 0 (.0) >> 4 FOR_ITER 12 (to 18) 6 STORE_FAST 1 (x) 8 LOAD_GLOBAL 0 (len) 10 LOAD_FAST 1 (x) 12 CALL_FUNCTION 1 14 LIST_APPEND 2 16 JUMP_ABSOLUTE 4 >> 18 RETURN_VALUE c = compile ( codeExemple , 'test.py' , 'exec' ) c . co_code b'd\\x00d\\x01d\\x02d\\x03g\\x04Z\\x00d\\x04d\\x05\\x84\\x00e\\x00D\\x00\\x83\\x01Z\\x01d\\x06d\\x05\\x84\\x00e\\x00D\\x00\\x83\\x01Z\\x02d\\x07S\\x00' ! cat { fName } ppl = ['Alice', 'Bob', 'Carol', 'Doug'] excited_ppl = [e + '!!' for e in ppl] ppl_len = [len(x) for x in ppl]","title":"Opcodes and main interpreter loop"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#opcodeh","text":"! wget -- quiet https : // raw . githubusercontent . com / python / cpython / master / Include / opcode . h - O { tmp } / opcode . h ! head - n 30 { tmp } / opcode . h /* Auto-generated by Tools/scripts/generate_opcode_h.py from Lib/opcode.py */","title":"opcode.h"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#ifndef-py_opcode_h","text":"","title":"ifndef Py_OPCODE_H"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-py_opcode_h","text":"","title":"define Py_OPCODE_H"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#ifdef-__cplusplus","text":"extern \"C\" {","title":"ifdef __cplusplus"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#endif","text":"/* Instruction opcodes for compiled code */","title":"endif"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-pop_top-1","text":"","title":"define POP_TOP                   1"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-rot_two-2","text":"","title":"define ROT_TWO                   2"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-rot_three-3","text":"","title":"define ROT_THREE                 3"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-dup_top-4","text":"","title":"define DUP_TOP                   4"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-dup_top_two-5","text":"","title":"define DUP_TOP_TWO               5"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-rot_four-6","text":"","title":"define ROT_FOUR                  6"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-nop-9","text":"","title":"define NOP                       9"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-unary_positive-10","text":"","title":"define UNARY_POSITIVE           10"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-unary_negative-11","text":"","title":"define UNARY_NEGATIVE           11"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-unary_not-12","text":"","title":"define UNARY_NOT                12"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-unary_invert-15","text":"","title":"define UNARY_INVERT             15"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_matrix_multiply-16","text":"","title":"define BINARY_MATRIX_MULTIPLY   16"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-inplace_matrix_multiply-17","text":"","title":"define INPLACE_MATRIX_MULTIPLY  17"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_power-19","text":"","title":"define BINARY_POWER             19"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_multiply-20","text":"","title":"define BINARY_MULTIPLY          20"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_modulo-22","text":"","title":"define BINARY_MODULO            22"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_add-23","text":"","title":"define BINARY_ADD               23"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_subtract-24","text":"","title":"define BINARY_SUBTRACT          24"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_subscr-25","text":"","title":"define BINARY_SUBSCR            25"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_floor_divide-26","text":"","title":"define BINARY_FLOOR_DIVIDE      26"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-binary_true_divide-27","text":"","title":"define BINARY_TRUE_DIVIDE       27"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#cevalc","text":"! wget -- quiet https : // raw . githubusercontent . com / python / cpython / master / Python / ceval . c - O { tmp } / ceval . c ! head - n 20 { tmp } / ceval . c /* Execute compiled code */ /* XXX TO DO: XXX speed up searching for keywords by using a dictionary XXX document it! */ /* enable more aggressive intra-module optimizations, where available */","title":"ceval.c"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#define-py_local_aggressive","text":"","title":"define PY_LOCAL_AGGRESSIVE"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pythonh","text":"","title":"include &#34;Python.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_callh","text":"","title":"include &#34;pycore_call.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_cevalh","text":"","title":"include &#34;pycore_ceval.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_codeh","text":"","title":"include &#34;pycore_code.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_objecth","text":"","title":"include &#34;pycore_object.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_pyerrorsh","text":"","title":"include &#34;pycore_pyerrors.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_pylifecycleh","text":"","title":"include &#34;pycore_pylifecycle.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_pystateh","text":"","title":"include &#34;pycore_pystate.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#include-pycore_tupleobjecth","text":"","title":"include &#34;pycore_tupleobject.h&#34;"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#general-purpose","text":"https://awesome-python.com/ : References: - Your Guide to the CPython Source Code - Inside The Python Virtual Machine - An introduction to Python bytecode - Deciphering Python: How to use Abstract Syntax Trees (AST) to understand code","title":"General purpose"},{"location":"_nb/%3DPy/01_Python/01%20-%20Python%20-%20CPython%20design/#garbage-collected","text":"https://github.com/python/cpython/blob/ce6a070414ed1e1374d1e6212bfbff61b6d5d755/Include/object.h#L104 from sys import getrefcount a = \"un\" getrefcount ( a ) 2 b = a b is a True getrefcount ( a ) 3 del ( b ) getrefcount ( a ) 2","title":"Garbage collected"},{"location":"_nb/%3DPy/01_Python_intro/01%20-%20Python%20-%20def/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Python is often described as a \"batteries included\" language due to its comprehensive standard library. High-level \u00b6 In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language Wikipedia Interpreted \u00b6 There are three general modes of execution for modern high-level languages: Interpreted : the syntax is read and then executed directly, with no compilation stage. A program called an interpreter reads each program statement, following the program flow, then decides what to do, and does it. Compiled : the code written in a language is compiled, its syntax is transformed into an executable form before running. There are two types of compilation: Machine code generation Intermediate representations : the code written in a language is compiled to an intermediate representation, that representation can be optimized or saved for later execution without the need to re-read the source file. When the intermediate representation is saved, it may be in a form such as bytecode. The intermediate representation must then be interpreted or further compiled to execute it. Source-to-source translated or transcompiled First, consider only the upper direct path. The code is parsed, i.e. split up into a list of pieces called tokens. These tokens are based on a set of rules for things that should be treated differently. For instance, the keyword if is a different token than a numeric value like 42 . The list of tokens is transformed to build an Abstract Syntax Tree, AST , collection of nodes which are linked together based on the Python language grammar . From an abstract syntax tree, the interpreter can produce a lower level form of instructions called bytecode . These instructions are things like BINARY_ADD and are meant to be very generic so that a computer can run them. With the bytecode instructions available, the interpreter can finally run your code. The bytecode is used to call functions in your operating system which will ultimately interact with a CPU and memory to run the program. Bytecode example \u00b6 print ( \"Hello, World!\" ) Hello, World! from dis import dis dis ( 'print(\"Hello, World!\")' ) 1 0 LOAD_NAME 0 (print) 2 LOAD_CONST 0 ('Hello, World!') 4 CALL_FUNCTION 1 6 RETURN_VALUE CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can \"push\" an item onto the \"top\" of the structure, or \"pop\" an item off the \"top\"). General purpose \u00b6 https://awesome-python.com/ : References: - Your Guide to the CPython Source Code - Inside The Python Virtual Machine - An introduction to Python bytecode - Deciphering Python: How to use Abstract Syntax Trees (AST) to understand code cpython/ \u2502 \u251c\u2500\u2500 Doc \u2190 Source for the documentation \u251c\u2500\u2500 Grammar \u2190 The computer-readable language definition \u251c\u2500\u2500 Include \u2190 The C header files \u251c\u2500\u2500 Lib \u2190 Standard library modules written in Python \u251c\u2500\u2500 Mac \u2190 macOS support files \u251c\u2500\u2500 Misc \u2190 Miscellaneous files \u251c\u2500\u2500 Modules \u2190 Standard Library Modules written in C \u251c\u2500\u2500 Objects \u2190 Core types and the object model \u251c\u2500\u2500 Parser \u2190 The Python parser source code \u251c\u2500\u2500 PC \u2190 Windows build support files \u251c\u2500\u2500 PCbuild \u2190 Windows build support files for older Windows versions \u251c\u2500\u2500 Programs \u2190 Source code for the python executable and other binaries \u251c\u2500\u2500 Python \u2190 The CPython interpreter source code \u2514\u2500\u2500 Tools \u2190 Standalone tools useful for building or extending Python Garbage collected \u00b6 https://github.com/python/cpython/blob/ce6a070414ed1e1374d1e6212bfbff61b6d5d755/Include/object.h#L104 from sys import getrefcount a = \"un\" getrefcount ( a ) 2 b = a b is a True getrefcount ( a ) 3 del ( b ) getrefcount ( a ) 2","title":"01   Python   def"},{"location":"_nb/%3DPy/01_Python_intro/01%20-%20Python%20-%20def/#high-level","text":"In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language Wikipedia","title":"High-level"},{"location":"_nb/%3DPy/01_Python_intro/01%20-%20Python%20-%20def/#interpreted","text":"There are three general modes of execution for modern high-level languages: Interpreted : the syntax is read and then executed directly, with no compilation stage. A program called an interpreter reads each program statement, following the program flow, then decides what to do, and does it. Compiled : the code written in a language is compiled, its syntax is transformed into an executable form before running. There are two types of compilation: Machine code generation Intermediate representations : the code written in a language is compiled to an intermediate representation, that representation can be optimized or saved for later execution without the need to re-read the source file. When the intermediate representation is saved, it may be in a form such as bytecode. The intermediate representation must then be interpreted or further compiled to execute it. Source-to-source translated or transcompiled First, consider only the upper direct path. The code is parsed, i.e. split up into a list of pieces called tokens. These tokens are based on a set of rules for things that should be treated differently. For instance, the keyword if is a different token than a numeric value like 42 . The list of tokens is transformed to build an Abstract Syntax Tree, AST , collection of nodes which are linked together based on the Python language grammar . From an abstract syntax tree, the interpreter can produce a lower level form of instructions called bytecode . These instructions are things like BINARY_ADD and are meant to be very generic so that a computer can run them. With the bytecode instructions available, the interpreter can finally run your code. The bytecode is used to call functions in your operating system which will ultimately interact with a CPU and memory to run the program.","title":"Interpreted"},{"location":"_nb/%3DPy/01_Python_intro/01%20-%20Python%20-%20def/#bytecode-example","text":"print ( \"Hello, World!\" ) Hello, World! from dis import dis dis ( 'print(\"Hello, World!\")' ) 1 0 LOAD_NAME 0 (print) 2 LOAD_CONST 0 ('Hello, World!') 4 CALL_FUNCTION 1 6 RETURN_VALUE CPython uses a stack-based virtual machine. That is, it's oriented entirely around stack data structures (where you can \"push\" an item onto the \"top\" of the structure, or \"pop\" an item off the \"top\").","title":"Bytecode example"},{"location":"_nb/%3DPy/01_Python_intro/01%20-%20Python%20-%20def/#general-purpose","text":"https://awesome-python.com/ : References: - Your Guide to the CPython Source Code - Inside The Python Virtual Machine - An introduction to Python bytecode - Deciphering Python: How to use Abstract Syntax Trees (AST) to understand code cpython/ \u2502 \u251c\u2500\u2500 Doc \u2190 Source for the documentation \u251c\u2500\u2500 Grammar \u2190 The computer-readable language definition \u251c\u2500\u2500 Include \u2190 The C header files \u251c\u2500\u2500 Lib \u2190 Standard library modules written in Python \u251c\u2500\u2500 Mac \u2190 macOS support files \u251c\u2500\u2500 Misc \u2190 Miscellaneous files \u251c\u2500\u2500 Modules \u2190 Standard Library Modules written in C \u251c\u2500\u2500 Objects \u2190 Core types and the object model \u251c\u2500\u2500 Parser \u2190 The Python parser source code \u251c\u2500\u2500 PC \u2190 Windows build support files \u251c\u2500\u2500 PCbuild \u2190 Windows build support files for older Windows versions \u251c\u2500\u2500 Programs \u2190 Source code for the python executable and other binaries \u251c\u2500\u2500 Python \u2190 The CPython interpreter source code \u2514\u2500\u2500 Tools \u2190 Standalone tools useful for building or extending Python","title":"General purpose"},{"location":"_nb/%3DPy/01_Python_intro/01%20-%20Python%20-%20def/#garbage-collected","text":"https://github.com/python/cpython/blob/ce6a070414ed1e1374d1e6212bfbff61b6d5d755/Include/object.h#L104 from sys import getrefcount a = \"un\" getrefcount ( a ) 2 b = a b is a True getrefcount ( a ) 3 del ( b ) getrefcount ( a ) 2","title":"Garbage collected"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); The Python Interpreter \u00b6 $ python Python 3.7 . 3 ( default , Apr 3 2019 , 05 : 39 : 12 ) [ GCC 8.3 . 0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> >>> a = 1 >>> print ( a ) 1 >>> exit () %% writefile hello_world . py print ( \"Hello world\" ) Overwriting hello_world.py $ python hello_world . py Hello world $ ipython Python 3 .6.0 | packaged by conda-forge | ( default, Jan 13 2017 , 23 :17:12 ) Type \"copyright\" , \"credits\" or \"license\" for more information. IPython 5 .1.0 -- An enhanced Interactive Python. ? -> Introduction and overview of IPython 's features. %quickref -> Quick reference. help -> Python' s own help system. object? -> Details about 'object' , use 'object??' for extra details. In [ 1 ] : %run hello_world.py Hello world In [ 2 ] : IPython Basics \u00b6 Running the IPython Shell \u00b6 from numpy.random import randn data = {i : randn() for i in range(7)} print(data) {0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295, 3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216, 6: 0.3308507317325902} Running the Jupyter Notebook \u00b6 $ jupyter notebook [ I 15 :20:52.739 NotebookApp ] Serving notebooks from local directory: /home/wesm/code/pydata-book [ I 15 :20:52.739 NotebookApp ] 0 active kernels [ I 15 :20:52.739 NotebookApp ] The Jupyter Notebook is running at: http://localhost:8888/ [ I 15 :20:52.740 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . Created new window in existing browser session. Tab Completion \u00b6 In [1]: an_apple = 27 In [2]: an_example = 42 In [3]: an In [3]: b = [1, 2, 3] In [4]: b. In [1]: import datetime In [2]: datetime. In [7]: datasets/movielens/ Introspection \u00b6 In [8]: b = [1, 2, 3] In [9]: b? Type: list String Form:[1, 2, 3] Length: 3 Docstring: list() -> new empty list list(iterable) -> new list initialized from iterable's items In [10]: print? Docstring: print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. Type: builtin_function_or_method def add_numbers ( a , b ): \"\"\" Add two numbers together Returns ------- the_sum : type of arguments \"\"\" return a + b In [ 11 ]: add_numbers ? Signature : add_numbers ( a , b ) Docstring : Add two numbers together Returns ------- the_sum : type of arguments File : < ipython - input - 9 - 6 a548a216e27 > Type : function In [ 12 ]: add_numbers ?? Signature : add_numbers ( a , b ) Source : def add_numbers ( a , b ): \"\"\" Add two numbers together Returns ------- the_sum : type of arguments \"\"\" return a + b File : < ipython - input - 9 - 6 a548a216e27 > Type : function In [ 13 ]: np .* load * ? np . __loader__ np . load np . loads np . loadtxt np . pkgload Interrupting running code \u00b6 Python Language Basics \u00b6 Language Semantics \u00b6 Indentation, not braces \u00b6 for x in array : if x < pivot : less . append ( x ) else : greater . append ( x ) a = 5 ; b = 6 ; c = 7 %% javascript ( new Array ([ 1 , 2 ])). then ( elt => elt ) var element = $('#89441118-ba50-4494-954c-78d36a56e9ab'); (new Array([1, 2])).then(elt => elt) Everything is an object \u00b6 A = dict ( first = 1 , second = 2 ) A {'first': 1, 'second': 2} type ( A ) dict type ( type ( A )) type type ( type ( type ( A ))) type Comments \u00b6 results = [] for line in file_handle : # keep the empty lines for now # if len(line) == 0: # continue results . append ( line . replace ( 'foo' , 'bar' )) print ( \"Reached this line\" ) # Simple status report Function and object method calls \u00b6 result = f(x, y, z) g() obj.some_method(x, y, z) result = f ( a , b , c , d = 5 , e = 'foo' ) Variables and argument passing \u00b6 a = [ 1 , 2 , 3 ] b = a a . append ( 4 ) b [1, 2, 3, 4] a [1, 2, 3, 4] def append_element ( some_list , element ): some_list . append ( element ) In [ 27 ]: data = [ 1 , 2 , 3 ] In [ 28 ]: append_element ( data , 4 ) In [ 29 ]: data Out [ 29 ]: [ 1 , 2 , 3 , 4 ] Dynamic references, strong types \u00b6 a = 5 type ( a ) int a = 'foo' type ( a ) str '5' + 5 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-73-4dd8efb5fac1> in <module> ----> 1 '5' + 5 TypeError : can only concatenate str (not \"int\") to str a = 4.5 b = 2 # String formatting, to be visited later print ( f 'a is { type ( a ) } , b is { type ( b ) } ' ) a / b a is <class 'float'>, b is <class 'int'> 2.25 a = 5 isinstance ( a , int ) True a = 5 ; b = 4.5 isinstance ( a , ( int , float )) True isinstance ( b , ( int , float )) True Attributes and methods \u00b6 In [ 1 ]: a = 'foo' In [ 2 ]: a .< Press Tab > a . capitalize a . format a . isupper a . rindex a . strip a . center a . index a . join a . rjust a . swapcase a . count a . isalnum a . ljust a . rpartition a . title a . decode a . isalpha a . lower a . rsplit a . translate a . encode a . isdigit a . lstrip a . rstrip a . upper a . endswith a . islower a . partition a . split a . zfill a . expandtabs a . isspace a . replace a . splitlines a . find a . istitle a . rfind a . startswith a = 'foo' getattr ( a , 'split' ) <function str.split(sep=None, maxsplit=-1)> Duck typing \u00b6 def isiterable ( obj ): try : iter ( obj ) return True except TypeError : # not iterable return False isiterable ( 'a string' ) isiterable ([ 1 , 2 , 3 ]) isiterable ( 5 ) False if not isinstance(x, list) and isiterable(x): x = list(x) Imports \u00b6 # some_module.py PI = 3.14159 def f ( x ): return x + 2 def g ( a , b ): return a + b import some_module result = some_module.f(5) pi = some_module.PI from some_module import f, g, PI result = g(5, PI) import some_module as sm from some_module import PI as pi, g as gf r1 = sm.f(pi) r2 = gf(6, pi) Binary operators and comparisons \u00b6 5 - 7 12 + 21.5 5 <= 2 a = [ 1 , 2 , 3 ] b = a c = list ( a ) a is b a is not c a == c a = None a is None Mutable and immutable objects \u00b6 a_list = [ 'foo' , 2 , [ 4 , 5 ]] a_list [ 2 ] = ( 3 , 4 ) a_list a_tuple = ( 3 , 5 , ( 4 , 5 )) a_tuple [ 1 ] = 'four' Scalar Types \u00b6 Numeric types \u00b6 ival = 17239871 ival ** 6 fval = 7.243 fval2 = 6.78e-5 3 / 2 3 // 2 Strings \u00b6 a = 'one way of writing a string' b = \"another way\" c = \"\"\" This is a longer string that spans multiple lines \"\"\" c . count ( ' \\n ' ) a = 'this is a string' a [ 10 ] = 'f' b = a . replace ( 'string' , 'longer string' ) b a a = 5.6 s = str ( a ) print ( s ) s = 'python' list ( s ) s [: 3 ] s = '12 \\\\ 34' print ( s ) s = r 'this\\has\\no\\special\\characters' s a = 'this is the first half ' b = 'and this is the second half' a + b template = ' {0:.2f} {1:s} are worth US$ {2:d} ' template . format ( 4.5560 , 'Argentine Pesos' , 1 ) Bytes and Unicode \u00b6 val = \"espa\u00f1ol\" val val_utf8 = val . encode ( 'utf-8' ) val_utf8 type ( val_utf8 ) val_utf8 . decode ( 'utf-8' ) val . encode ( 'latin1' ) val . encode ( 'utf-16' ) val . encode ( 'utf-16le' ) bytes_val = b 'this is bytes' bytes_val decoded = bytes_val . decode ( 'utf8' ) decoded # this is str (Unicode) now Booleans \u00b6 True and True False or True Type casting \u00b6 s = '3.14159' fval = float ( s ) type ( fval ) int ( fval ) bool ( fval ) bool ( 0 ) None \u00b6 a = None a is None b = 5 b is not None def add_and_maybe_multiply(a, b, c=None): result = a + b if c is not None: result = result * c return result type ( None ) Dates and times \u00b6 from datetime import datetime , date , time dt = datetime ( 2011 , 10 , 29 , 20 , 30 , 21 ) dt . day dt . minute dt . date () dt . time () dt . strftime ( '%m/ %d /%Y %H:%M' ) datetime . strptime ( '20091031' , '%Y%m %d ' ) dt . replace ( minute = 0 , second = 0 ) dt2 = datetime ( 2011 , 11 , 15 , 22 , 30 ) delta = dt2 - dt delta type ( delta ) dt dt + delta Control Flow \u00b6 if, elif, and else \u00b6 if x < 0: print('It's negative') if x < 0: print('It's negative') elif x == 0: print('Equal to zero') elif 0 < x < 5: print('Positive but smaller than 5') else: print('Positive and larger than or equal to 5') a = 5 ; b = 7 c = 8 ; d = 4 if a < b or c > d : print ( 'Made it' ) 4 > 3 > 2 > 1 for loops \u00b6 for value in collection: # do something with value sequence = [1, 2, None, 4, None, 5] total = 0 for value in sequence: if value is None: continue total += value sequence = [1, 2, 0, 4, 6, 5, 2, 1] total_until_5 = 0 for value in sequence: if value == 5: break total_until_5 += value for i in range ( 4 ): for j in range ( 4 ): if j > i : break print (( i , j )) for a, b, c in iterator: # do something while loops \u00b6 x = 256 total = 0 while x > 0: if total > 500: break total += x x = x // 2 pass \u00b6 if x < 0: print('negative!') elif x == 0: # TODO: put something smart here pass else: print('positive!') range \u00b6 range ( 10 ) list ( range ( 10 )) list ( range ( 0 , 20 , 2 )) list ( range ( 5 , 0 , - 1 )) seq = [1, 2, 3, 4] for i in range(len(seq)): val = seq[i] sum = 0 for i in range(100000): # % is the modulo operator if i % 3 == 0 or i % 5 == 0: sum += i Ternary expressions \u00b6 value = if x = 5 'Non-negative' if x >= 0 else 'Negative' References: - Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media","title":"02   Python   intro"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#the-python-interpreter","text":"$ python Python 3.7 . 3 ( default , Apr 3 2019 , 05 : 39 : 12 ) [ GCC 8.3 . 0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> >>> a = 1 >>> print ( a ) 1 >>> exit () %% writefile hello_world . py print ( \"Hello world\" ) Overwriting hello_world.py $ python hello_world . py Hello world $ ipython Python 3 .6.0 | packaged by conda-forge | ( default, Jan 13 2017 , 23 :17:12 ) Type \"copyright\" , \"credits\" or \"license\" for more information. IPython 5 .1.0 -- An enhanced Interactive Python. ? -> Introduction and overview of IPython 's features. %quickref -> Quick reference. help -> Python' s own help system. object? -> Details about 'object' , use 'object??' for extra details. In [ 1 ] : %run hello_world.py Hello world In [ 2 ] :","title":"The Python Interpreter"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#ipython-basics","text":"","title":"IPython Basics"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#running-the-ipython-shell","text":"from numpy.random import randn data = {i : randn() for i in range(7)} print(data) {0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295, 3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216, 6: 0.3308507317325902}","title":"Running the IPython Shell"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#running-the-jupyter-notebook","text":"$ jupyter notebook [ I 15 :20:52.739 NotebookApp ] Serving notebooks from local directory: /home/wesm/code/pydata-book [ I 15 :20:52.739 NotebookApp ] 0 active kernels [ I 15 :20:52.739 NotebookApp ] The Jupyter Notebook is running at: http://localhost:8888/ [ I 15 :20:52.740 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . Created new window in existing browser session.","title":"Running the Jupyter Notebook"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#tab-completion","text":"In [1]: an_apple = 27 In [2]: an_example = 42 In [3]: an In [3]: b = [1, 2, 3] In [4]: b. In [1]: import datetime In [2]: datetime. In [7]: datasets/movielens/","title":"Tab Completion"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#introspection","text":"In [8]: b = [1, 2, 3] In [9]: b? Type: list String Form:[1, 2, 3] Length: 3 Docstring: list() -> new empty list list(iterable) -> new list initialized from iterable's items In [10]: print? Docstring: print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. Type: builtin_function_or_method def add_numbers ( a , b ): \"\"\" Add two numbers together Returns ------- the_sum : type of arguments \"\"\" return a + b In [ 11 ]: add_numbers ? Signature : add_numbers ( a , b ) Docstring : Add two numbers together Returns ------- the_sum : type of arguments File : < ipython - input - 9 - 6 a548a216e27 > Type : function In [ 12 ]: add_numbers ?? Signature : add_numbers ( a , b ) Source : def add_numbers ( a , b ): \"\"\" Add two numbers together Returns ------- the_sum : type of arguments \"\"\" return a + b File : < ipython - input - 9 - 6 a548a216e27 > Type : function In [ 13 ]: np .* load * ? np . __loader__ np . load np . loads np . loadtxt np . pkgload","title":"Introspection"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#interrupting-running-code","text":"","title":"Interrupting running code"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#python-language-basics","text":"","title":"Python Language Basics"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#language-semantics","text":"","title":"Language Semantics"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#indentation-not-braces","text":"for x in array : if x < pivot : less . append ( x ) else : greater . append ( x ) a = 5 ; b = 6 ; c = 7 %% javascript ( new Array ([ 1 , 2 ])). then ( elt => elt ) var element = $('#89441118-ba50-4494-954c-78d36a56e9ab'); (new Array([1, 2])).then(elt => elt)","title":"Indentation, not braces"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#everything-is-an-object","text":"A = dict ( first = 1 , second = 2 ) A {'first': 1, 'second': 2} type ( A ) dict type ( type ( A )) type type ( type ( type ( A ))) type","title":"Everything is an object"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#comments","text":"results = [] for line in file_handle : # keep the empty lines for now # if len(line) == 0: # continue results . append ( line . replace ( 'foo' , 'bar' )) print ( \"Reached this line\" ) # Simple status report","title":"Comments"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#function-and-object-method-calls","text":"result = f(x, y, z) g() obj.some_method(x, y, z) result = f ( a , b , c , d = 5 , e = 'foo' )","title":"Function and object method calls"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#variables-and-argument-passing","text":"a = [ 1 , 2 , 3 ] b = a a . append ( 4 ) b [1, 2, 3, 4] a [1, 2, 3, 4] def append_element ( some_list , element ): some_list . append ( element ) In [ 27 ]: data = [ 1 , 2 , 3 ] In [ 28 ]: append_element ( data , 4 ) In [ 29 ]: data Out [ 29 ]: [ 1 , 2 , 3 , 4 ]","title":"Variables and argument passing"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#dynamic-references-strong-types","text":"a = 5 type ( a ) int a = 'foo' type ( a ) str '5' + 5 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-73-4dd8efb5fac1> in <module> ----> 1 '5' + 5 TypeError : can only concatenate str (not \"int\") to str a = 4.5 b = 2 # String formatting, to be visited later print ( f 'a is { type ( a ) } , b is { type ( b ) } ' ) a / b a is <class 'float'>, b is <class 'int'> 2.25 a = 5 isinstance ( a , int ) True a = 5 ; b = 4.5 isinstance ( a , ( int , float )) True isinstance ( b , ( int , float )) True","title":"Dynamic references, strong types"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#attributes-and-methods","text":"In [ 1 ]: a = 'foo' In [ 2 ]: a .< Press Tab > a . capitalize a . format a . isupper a . rindex a . strip a . center a . index a . join a . rjust a . swapcase a . count a . isalnum a . ljust a . rpartition a . title a . decode a . isalpha a . lower a . rsplit a . translate a . encode a . isdigit a . lstrip a . rstrip a . upper a . endswith a . islower a . partition a . split a . zfill a . expandtabs a . isspace a . replace a . splitlines a . find a . istitle a . rfind a . startswith a = 'foo' getattr ( a , 'split' ) <function str.split(sep=None, maxsplit=-1)>","title":"Attributes and methods"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#duck-typing","text":"def isiterable ( obj ): try : iter ( obj ) return True except TypeError : # not iterable return False isiterable ( 'a string' ) isiterable ([ 1 , 2 , 3 ]) isiterable ( 5 ) False if not isinstance(x, list) and isiterable(x): x = list(x)","title":"Duck typing"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#imports","text":"# some_module.py PI = 3.14159 def f ( x ): return x + 2 def g ( a , b ): return a + b import some_module result = some_module.f(5) pi = some_module.PI from some_module import f, g, PI result = g(5, PI) import some_module as sm from some_module import PI as pi, g as gf r1 = sm.f(pi) r2 = gf(6, pi)","title":"Imports"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#binary-operators-and-comparisons","text":"5 - 7 12 + 21.5 5 <= 2 a = [ 1 , 2 , 3 ] b = a c = list ( a ) a is b a is not c a == c a = None a is None","title":"Binary operators and comparisons"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#mutable-and-immutable-objects","text":"a_list = [ 'foo' , 2 , [ 4 , 5 ]] a_list [ 2 ] = ( 3 , 4 ) a_list a_tuple = ( 3 , 5 , ( 4 , 5 )) a_tuple [ 1 ] = 'four'","title":"Mutable and immutable objects"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#scalar-types","text":"","title":"Scalar Types"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#numeric-types","text":"ival = 17239871 ival ** 6 fval = 7.243 fval2 = 6.78e-5 3 / 2 3 // 2","title":"Numeric types"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#strings","text":"a = 'one way of writing a string' b = \"another way\" c = \"\"\" This is a longer string that spans multiple lines \"\"\" c . count ( ' \\n ' ) a = 'this is a string' a [ 10 ] = 'f' b = a . replace ( 'string' , 'longer string' ) b a a = 5.6 s = str ( a ) print ( s ) s = 'python' list ( s ) s [: 3 ] s = '12 \\\\ 34' print ( s ) s = r 'this\\has\\no\\special\\characters' s a = 'this is the first half ' b = 'and this is the second half' a + b template = ' {0:.2f} {1:s} are worth US$ {2:d} ' template . format ( 4.5560 , 'Argentine Pesos' , 1 )","title":"Strings"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#bytes-and-unicode","text":"val = \"espa\u00f1ol\" val val_utf8 = val . encode ( 'utf-8' ) val_utf8 type ( val_utf8 ) val_utf8 . decode ( 'utf-8' ) val . encode ( 'latin1' ) val . encode ( 'utf-16' ) val . encode ( 'utf-16le' ) bytes_val = b 'this is bytes' bytes_val decoded = bytes_val . decode ( 'utf8' ) decoded # this is str (Unicode) now","title":"Bytes and Unicode"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#booleans","text":"True and True False or True","title":"Booleans"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#type-casting","text":"s = '3.14159' fval = float ( s ) type ( fval ) int ( fval ) bool ( fval ) bool ( 0 )","title":"Type casting"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#none","text":"a = None a is None b = 5 b is not None def add_and_maybe_multiply(a, b, c=None): result = a + b if c is not None: result = result * c return result type ( None )","title":"None"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#dates-and-times","text":"from datetime import datetime , date , time dt = datetime ( 2011 , 10 , 29 , 20 , 30 , 21 ) dt . day dt . minute dt . date () dt . time () dt . strftime ( '%m/ %d /%Y %H:%M' ) datetime . strptime ( '20091031' , '%Y%m %d ' ) dt . replace ( minute = 0 , second = 0 ) dt2 = datetime ( 2011 , 11 , 15 , 22 , 30 ) delta = dt2 - dt delta type ( delta ) dt dt + delta","title":"Dates and times"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#control-flow","text":"","title":"Control Flow"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#if-elif-and-else","text":"if x < 0: print('It's negative') if x < 0: print('It's negative') elif x == 0: print('Equal to zero') elif 0 < x < 5: print('Positive but smaller than 5') else: print('Positive and larger than or equal to 5') a = 5 ; b = 7 c = 8 ; d = 4 if a < b or c > d : print ( 'Made it' ) 4 > 3 > 2 > 1","title":"if, elif, and else"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#for-loops","text":"for value in collection: # do something with value sequence = [1, 2, None, 4, None, 5] total = 0 for value in sequence: if value is None: continue total += value sequence = [1, 2, 0, 4, 6, 5, 2, 1] total_until_5 = 0 for value in sequence: if value == 5: break total_until_5 += value for i in range ( 4 ): for j in range ( 4 ): if j > i : break print (( i , j )) for a, b, c in iterator: # do something","title":"for loops"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#while-loops","text":"x = 256 total = 0 while x > 0: if total > 500: break total += x x = x // 2","title":"while loops"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#pass","text":"if x < 0: print('negative!') elif x == 0: # TODO: put something smart here pass else: print('positive!')","title":"pass"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#range","text":"range ( 10 ) list ( range ( 10 )) list ( range ( 0 , 20 , 2 )) list ( range ( 5 , 0 , - 1 )) seq = [1, 2, 3, 4] for i in range(len(seq)): val = seq[i] sum = 0 for i in range(100000): # % is the modulo operator if i % 3 == 0 or i % 5 == 0: sum += i","title":"range"},{"location":"_nb/%3DPy/01_Python_intro/02%20-%20Python%20-%20intro/#ternary-expressions","text":"value = if x = 5 'Non-negative' if x >= 0 else 'Negative' References: - Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media","title":"Ternary expressions"},{"location":"_nb/%3DPy/02_SQL/00_Init/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import intake from numpy import int8 accounts = intake . open_csv ( \"../data/accounts/*.csv\" , csv_kwargs = { \"dtype\" : { 'id' : int8 , 'names' : str } } ) accounts . shape = ( 3_000_000 , 3 ) accounts . discover () {'datashape': None, 'dtype': {'id': 'int8', 'names': 'object', 'amount': 'int64'}, 'shape': (None, 3), 'npartitions': 3, 'metadata': {}} accounts . dtype [ 'names' ] = str df = accounts . to_dask () from numpy import arange df . memory_usage () Dask Series Structure: npartitions=1 int64 ... dtype: int64 Dask Name: series-groupby-sum-agg, 19 tasks df . memory_usage () . compute () Index 240 amount 24000000 id 3000000 names 24000000 dtype: int64 gp = df . groupby ( 'id' ) gp = df . compute () . groupby ( 'id' ) import numpy as np gp . agg ( lambda x : len ( np . unique ( x ))) . names . max () 2 gp . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount 0 59 Tim -330 1 43 Yvonne 2731 2 58 Hannah 888 3 65 Wendy 1670 4 82 Zelda -1114 5 -108 Ray 2221 6 122 Victor 1068 7 96 Charlie 373 8 2 Ray 45 9 -85 Ingrid 1684 10 104 Yvonne 454 11 28 Quinn 559 12 -84 Hannah 583 13 -104 Tim 0 14 -72 Ray 1836 15 87 Jerry -16 16 71 George 50 17 -61 Patricia 675 18 94 Yvonne 1581 19 94 Laura 669 20 -39 Quinn 1938 21 -9 Frank 117 22 -24 Ursula 479 23 110 Xavier 24 24 72 Laura 3620 25 85 Kevin 274 26 25 Edith 125 27 28 Quinn 606 28 -56 Charlie 1565 29 -120 Dan 1560 ... ... ... ... 7293 7 Zelda -176 7305 -101 Norbert 1047 7496 40 Ingrid 2373 7500 -19 Quinn 25 7516 -101 Norbert 1083 7716 -98 Yvonne 603 7823 -1 Yvonne 4222 7835 -1 Yvonne 4501 7841 -117 Dan 483 8015 -105 Norbert 1715 8043 -25 Ray 4943 8565 7 Zelda -183 8653 40 Ingrid 2466 8671 -53 Charlie 1051 9268 -105 Norbert 1694 9301 -19 Quinn 32 9824 -4 Michael 2126 10776 -8 Frank 116 10809 -117 Dan 517 10987 -105 Norbert 1581 11002 -117 Dan 519 11345 -25 Ray 4029 12017 -12 Michael 74 14472 -12 Michael 52 14700 -12 Michael -35 14955 -12 Michael 73 15474 -8 Frank 116 15895 -105 Norbert 1598 17015 -4 Michael 2070 17635 -105 Norbert 1781 1280 rows \u00d7 3 columns newIndex = np . arange ( df . shape [ 0 ]) newIndex . shape (3000000,) df . reindex ( axis = 0 , method = \"pad\" ) . tail () --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-36-df976a66ae41> in <module> ----> 1 df . reindex ( axis = 0 , method = \"pad\" ) . tail ( ) /opt/conda/lib/python3.7/site-packages/dask/dataframe/core.py in __getattr__ (self, key) 2586 return self [ key ] 2587 else : -> 2588 raise AttributeError ( \"'DataFrame' object has no attribute %r\" % key ) 2589 2590 def __dir__ ( self ) : AttributeError : 'DataFrame' object has no attribute 'reindex' # %timeit df.names.map(lambda x: len(x)).max() import pandas as pd dfp = pd . DataFrame ( df . values . compute (), columns = df . columns ) # dfp.names = dfp.names.astype(np.dtype('|S8')) dfp . id = dfp . id . astype ( np . int32 ) dfp . amount = dfp . amount . astype ( np . int64 ) # dfp.names = dfp.names.astype(np.dtype('|S16')) dfp . names = dfp . names . astype ( str ) dfp . dtypes id int32 names object amount int64 dtype: object dfp . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount 0 59 Tim -330 1 43 Yvonne 2731 2 58 Hannah 888 3 65 Wendy 1670 4 82 Zelda -1114 dfp . memory_usage () Index 80 id 12000000 names 24000000 amount 24000000 dtype: int64 dfp . to_sql ? Signature: dfp . to_sql ( name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None , ) Docstring: Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : string Name of SQL table. con : sqlalchemy.engine.Engine or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. schema : string, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : string or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Rows will be written in batches of this size at a time. By default, all rows will be written at once. dtype : dict, optional Specifying the datatype for columns. The keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. method : {None, 'multi', callable}, default None Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] http://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) >>> df1.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5')] Overwrite the table with just ``df1``. >>> df1.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 4'), (1, 'User 5')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\"A\": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) >>> engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] File: /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py Type: method from sqlalchemy.types import String String ( 16 ) String(length=16) from sqlalchemy import create_engine uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com/postgres\" dfp . to_sql ( \"accounts\" , con = create_engine ( uri ) . connect (), index = False , if_exists = 'replace' ) % timeit dfp . names . map ( lambda x : len ( x )) . max () 1.16 s \u00b1 8.62 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity 0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY 3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY 4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY ibis . schema ? Signature: ibis . schema ( pairs = None , names = None , types = None ) Docstring: Validate and return an Ibis Schema object Ibis uses its own type aliases that map onto database types. See, for example, the correspondence between Ibis type names and Impala type names: Ibis type Impala Type ~~~~~~~~~ ~~~~~~~~~~~ int8 TINYINT int16 SMALLINT int32 INT int64 BIGINT float FLOAT double DOUBLE boolean BOOLEAN string STRING timestamp TIMESTAMP decimal(p, s) DECIMAL(p,s) interval(u) INTERVAL(u) Parameters ---------- pairs : list of (name, type) tuples Mutually exclusive with names/types names : list of string Field names types : list of string Field types Examples -------- >>> from ibis import schema >>> sc = schema([('foo', 'string'), ... ('bar', 'int64'), ... ('baz', 'boolean')]) >>> sc2 = schema(names=['foo', 'bar', 'baz'], ... types=['string', 'int64', 'boolean']) Returns ------- schema : Schema File: /opt/conda/lib/python3.7/site-packages/ibis/expr/api.py Type: function accounts = con . table ( 'accounts' ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-66-b976ae21ec75> in <module> ----> 1 accounts = con . table ( 'accounts' ) NameError : name 'con' is not defined accounts . names ? Call signature: accounts . names ( f , * args , ** kwargs ) Type: StringColumn String form: ref_0 PostgreSQLTable[table] name: accounts schema: index : int64 id : int32 names : string amount : int64 names = Column[string*] 'names' from table ref_0 File: /opt/conda/lib/python3.7/site-packages/ibis/expr/types.py Docstring: <no docstring> Class docstring: Base class for a data generating expression having a fixed and known type, either a single value (scalar) Call docstring: Generic composition function to enable expression pipelining. Parameters ---------- f : function or (function, arg_name) tuple If the expression needs to be passed as anything other than the first argument to the function, pass a tuple with the argument name. For example, (f, 'data') if the function f expects a 'data' keyword args : positional arguments kwargs : keyword arguments Examples -------- >>> import ibis >>> t = ibis.table([('a', 'int64'), ('b', 'string')], name='t') >>> f = lambda a: (a + 1).name('a') >>> g = lambda a: (a * 2).name('a') >>> result1 = t.a.pipe(f).pipe(g) >>> result1 # doctest: +NORMALIZE_WHITESPACE ref_0 UnboundTable[table] name: t schema: a : int64 b : string a = Multiply[int64*] left: a = Add[int64*] left: a = Column[int64*] 'a' from table ref_0 right: Literal[int8] 1 right: Literal[int8] 2 >>> result2 = g(f(t.a)) # equivalent to the above >>> result1.equals(result2) True Returns ------- result : result type of passed function","title":"00 Init"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); S01 Relational Databases \u00b6 For a simple tutorial on database design, see Introduction to Database Design For a deep dive, see Database Design for Mere Mortals 0. Packages for working with relational databases in Python \u00b6 - Python Database API Specification v2.0 - The standard Python Database API - sqlite3 - API for builit-in sqlite3 package - Database drivers - For connecting to other databases - ipython-sql - SQL magic in Jupyter - SQLAlchemy - Most well-known Object Relational Mapper (ORM) - Pony ORM - Alternative ORM 1. Motivation \u00b6 Why relational databases and SQL? History of databases ACID Data integrity Schema 2. RDBMS \u00b6 Memory Storage Dictionary Query language 3. Anatomy \u00b6 Table (Relation): Represents a subject or an event . Column (Attribute): Represents a single variable or feature .- Row (Tuple): represents an observation . 4. Concepts \u00b6 Constraints \u00b6 You can impose constraints that values in a column have to take. For example, you can specify that values are compulsory (NOT NULL), or UNIQUE or fall within a certain range. Referential integrity \u00b6 - Primary key represents a unique identifier of a row. It may be simple or composite. - Unique - Non-null - Never optional - Foreign key is a column containing the primary key of a different table. It enforces referential integrity . Relationships \u00b6 - One to one - One to many - Many to many What happens on delete? Restrict Cascade Indexes \u00b6 An index is a data structure that allows fast search of a column (typically from linear to log time complexity). Most databases will automatically build an index for every primary key column, but you can also manually specify columns to build indexes for. Views \u00b6 - Temporary virtual table returned as a result of a query . - Views only specify the strucutre of a table - the contents are constructed on the fly from existing tables. - Queries return a Result Set 5. Design \u00b6 Columns \u00b6 - Use singlular form for name - Use informative names - Use unique names not shared by any other table (except foreign keys) - Column must be an attribute of the table's subject - Eliminate multi-part columns - Eliminate multi-value columsn - Eliminate redundant columns Tables \u00b6 - Use singular/plural forms for name (controversial) - Enusre every table has a primary key - Eliminate duplicate columns Relationships \u00b6 - Establish participation type and degree of relationship - One to one - One to many - Many to many 6. Example \u00b6 Use sqlmagic as alternative to using sqlite3 driver. % env DATABASE_URL = postgresql + psycopg2 : // postgres : postgres @db . postgres . app . com env: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db.postgres.app.com % load_ext sql Connect to Postgres % config SqlMagic SqlMagic options -------------- SqlMagic.autocommit=<Bool> Current: True Set autocommit mode SqlMagic.autolimit=<Int> Current: 0 Automatically limit the size of the returned result sets SqlMagic.autopandas=<Bool> Current: False Return Pandas DataFrames instead of regular result sets SqlMagic.column_local_vars=<Bool> Current: False Return data into local variables from column names SqlMagic.displaylimit=<Int> Current: None Automatically limit the number of rows displayed (full result set is still stored) SqlMagic.dsn_filename=<Unicode> Current: 'odbc.ini' Path to DSN file. When the first argument is of the form [section], a sqlalchemy connection string is formed from the matching section in the DSN file. SqlMagic.feedback=<Bool> Current: True Print number of rows affected by DML SqlMagic.short_errors=<Bool> Current: True Don't display the full traceback on SQL Programming Error SqlMagic.style=<Unicode> Current: 'DEFAULT' Set the table printing style to any of prettytable's defined styles (currently DEFAULT, MSWORD_FRIENDLY, PLAIN_COLUMNS, RANDOM) %% sql \\ d 6 rows affected. Schema Name Type Owner public accounts table postgres public flat table postgres public t_cities table postgres public t_cities_id_seq sequence postgres public t_weather_observations table postgres public t_weather_observations_id_seq sequence postgres uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com\" % sql { uri } 'Connected: postgres@None' SQL for table deletion and creation %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , person_first varchar ( 255 ), person_last varchar ( 255 ), country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ) ); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres Done. Done. Done. Done. [] %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema' ; * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False True False public person postgres None True False True False SQL to insert rows. %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] The pg_relation_size() function returns the size of the table only, not included indexes or additional objects. %% sql SELECT pg_size_pretty ( pg_relation_size ( 'Country' )); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 1 rows affected. pg_size_pretty 8192 bytes %% sql INSERT INTO Person ( person_first , person_last , country_id ) VALUES ( 'Napolean' , 'Bonaparte' , 'FR' ), ( 'Luis' , 'Alvarez' , 'CU' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] Accessing the RDBMS dictionary. %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 9 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False True False public person postgres None True False True False information_schema sql_features postgres None False False False False information_schema sql_implementation_info postgres None False False False False information_schema sql_languages postgres None False False False False information_schema sql_packages postgres None False False False False information_schema sql_parts postgres None False False False False information_schema sql_sizing postgres None False False False False information_schema sql_sizing_profiles postgres None False False False False %% sql SELECT sql FROM postgres WHERE name = 'Person' ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres (psycopg2.errors.UndefinedTable) relation \"postgres\" does not exist LINE 1: SELECT sql FROM postgres ^ [SQL: SELECT sql FROM postgres WHERE name='Person';] (Background on this error at: http://sqlalche.me/e/f405) SQL as a Query Language. %% sql SELECT person_first as first , person_last AS last , country_name AS nationality FROM Person INNER JOIN country ON Person . country_id = Country . country_id ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. first last nationality Napolean Bonaparte France Luis Alvarez CUBA Visualizing the entitry-relationship diagram (ERd). % config ? Docstring: configure IPython %config Class[.trait=value] This magic exposes most of the IPython config system. Any Configurable class should be able to be configured with the simple line:: %config Class.trait=value Where `value` will be resolved in the user's namespace, if it is an expression or variable name. Examples -------- To see what classes are available for config, pass no arguments:: In [1]: %config Available objects for config: TerminalInteractiveShell HistoryManager PrefilterManager AliasManager IPCompleter DisplayFormatter To view what is configurable on a given class, just pass the class name:: In [2]: %config IPCompleter IPCompleter options ----------------- IPCompleter.omit__names=<Enum> Current: 2 Choices: (0, 1, 2) Instruct the completer to omit private method names Specifically, when completing on ``object.<tab>``. When 2 [default]: all names that start with '_' will be excluded. When 1: all 'magic' names (``__foo__``) will be excluded. When 0: nothing will be excluded. IPCompleter.merge_completions=<CBool> Current: True Whether to merge completion results into a single list If False, only the completion results from the first non-empty completer will be returned. IPCompleter.limit_to__all__=<CBool> Current: False Instruct the completer to use __all__ for the completion Specifically, when completing on ``object.<tab>``. When True: only those names in obj.__all__ will be included. When False [default]: the __all__ attribute is ignored IPCompleter.greedy=<CBool> Current: False Activate greedy completion This will enable completion on elements of lists, results of function calls, etc., but can be unsafe because the code is actually evaluated on TAB. but the real use is in setting values:: In [3]: %config IPCompleter.greedy = True and these values are read from the user_ns if they are variables:: In [4]: feeling_greedy=False In [5]: %config IPCompleter.greedy = feeling_greedy File: /opt/conda/lib/python3.7/site-packages/IPython/core/magics/config.py import ibis import eralchemy from sqlalchemy import create_engine engine = create_engine ( uri ) conn = engine . connect () import os from eralchemy import render_er if not os . path . exists ( 'erd_from_sqlalchemy.png' ): render_er ( uri , 'erd_from_sqlalchemy.png' ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/base.py:2972: SAWarning: Did not recognize type 'point' of column 'location' \"Did not recognize type '%s' of column '%s'\" % (attype, name) Homework walk-through \u00b6 Convert the flat file data in data/flat.csv into a well-structured relational database in SQLite3 stored as data/faculty.db . Note - salary information is confidential and should be kept in a separate table from other personal data. import pandas as pd flat = pd . read_csv ( '../data/flat.csv' , keep_default_na = False ) flat . sample ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name gender age height weight salary nationality code country language1 language2 language3 first last 899 Lucien Pittman Male 61 1.87 58 73000 Danish DK Denmark AutoIt Dylan Transact-SQL Lucien Pittman 1516 Zane Calhoun Male 60 1.93 48 127000 Greek GR Greece Io Java Zane Calhoun 827 Lauran Willis Female 34 1.92 75 89000 Romanian RO Romania Lauran Willis flat . to_sql ( 'flat' , conn ) %% sql \\ d * postgresql+psycopg2://postgres:***@db.postgres.app.com 6 rows affected. Schema Name Type Owner public accounts table postgres public flat table postgres public t_cities table postgres public t_cities_id_seq sequence postgres public t_weather_observations table postgres public t_weather_observations_id_seq sequence postgres %% sql USE faculty ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres (psycopg2.errors.SyntaxError) syntax error at or near \"USE\" LINE 1: USE faculty; ^ [SQL: USE faculty;] (Background on this error at: http://sqlalche.me/e/f405) %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , name varchar ( 255 ), age INTEGER NOT NULL , country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ) ); * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres Done. Done. Done. Done. [] %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] %% sql DELETE FROM Country * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] %% sql SELECT * FROM Country * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 0 rows affected. country_id country_name from sqlalchemy import create_engine engine = create_engine ( uri ) conn = engine . connect () flat . columns Index(['name', 'gender', 'age', 'height', 'weight', 'salary', 'nationality', 'code', 'country', 'language1', 'language2', 'language3', 'first', 'last'], dtype='object') flat . rename ( mapper = { 'code' : 'country_id' , 'country' : 'country_name' }, inplace = True ) country = flat[['country_id', 'country_name']]country.set_index('country_id').to_sql('Country', engine, if_exists='append') %%sql \u00b6 SELECT * FROM Country flat . to_sql ? Signature: flat . to_sql ( name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None , ) Docstring: Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : string Name of SQL table. con : sqlalchemy.engine.Engine or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. schema : string, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : string or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Rows will be written in batches of this size at a time. By default, all rows will be written at once. dtype : dict, optional Specifying the datatype for columns. The keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. method : {None, 'multi', callable}, default None Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] http://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) >>> df1.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5')] Overwrite the table with just ``df1``. >>> df1.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 4'), (1, 'User 5')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\"A\": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) >>> engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] File: /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py Type: method","title":"01 RDBMS"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#s01-relational-databases","text":"For a simple tutorial on database design, see Introduction to Database Design For a deep dive, see Database Design for Mere Mortals","title":"S01 Relational Databases"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#0-packages-for-working-with-relational-databases-in-python","text":"- Python Database API Specification v2.0 - The standard Python Database API - sqlite3 - API for builit-in sqlite3 package - Database drivers - For connecting to other databases - ipython-sql - SQL magic in Jupyter - SQLAlchemy - Most well-known Object Relational Mapper (ORM) - Pony ORM - Alternative ORM","title":"0. Packages for working with relational databases in Python"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#1-motivation","text":"Why relational databases and SQL? History of databases ACID Data integrity Schema","title":"1. Motivation"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#2-rdbms","text":"Memory Storage Dictionary Query language","title":"2. RDBMS"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#3-anatomy","text":"Table (Relation): Represents a subject or an event . Column (Attribute): Represents a single variable or feature .- Row (Tuple): represents an observation .","title":"3. Anatomy"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#4-concepts","text":"","title":"4. Concepts"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#constraints","text":"You can impose constraints that values in a column have to take. For example, you can specify that values are compulsory (NOT NULL), or UNIQUE or fall within a certain range.","title":"Constraints"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#referential-integrity","text":"- Primary key represents a unique identifier of a row. It may be simple or composite. - Unique - Non-null - Never optional - Foreign key is a column containing the primary key of a different table. It enforces referential integrity .","title":"Referential integrity"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#relationships","text":"- One to one - One to many - Many to many What happens on delete? Restrict Cascade","title":"Relationships"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#indexes","text":"An index is a data structure that allows fast search of a column (typically from linear to log time complexity). Most databases will automatically build an index for every primary key column, but you can also manually specify columns to build indexes for.","title":"Indexes"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#views","text":"- Temporary virtual table returned as a result of a query . - Views only specify the strucutre of a table - the contents are constructed on the fly from existing tables. - Queries return a Result Set","title":"Views"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#5-design","text":"","title":"5. Design"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#columns","text":"- Use singlular form for name - Use informative names - Use unique names not shared by any other table (except foreign keys) - Column must be an attribute of the table's subject - Eliminate multi-part columns - Eliminate multi-value columsn - Eliminate redundant columns","title":"Columns"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#tables","text":"- Use singular/plural forms for name (controversial) - Enusre every table has a primary key - Eliminate duplicate columns","title":"Tables"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#relationships_1","text":"- Establish participation type and degree of relationship - One to one - One to many - Many to many","title":"Relationships"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#6-example","text":"Use sqlmagic as alternative to using sqlite3 driver. % env DATABASE_URL = postgresql + psycopg2 : // postgres : postgres @db . postgres . app . com env: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db.postgres.app.com % load_ext sql Connect to Postgres % config SqlMagic SqlMagic options -------------- SqlMagic.autocommit=<Bool> Current: True Set autocommit mode SqlMagic.autolimit=<Int> Current: 0 Automatically limit the size of the returned result sets SqlMagic.autopandas=<Bool> Current: False Return Pandas DataFrames instead of regular result sets SqlMagic.column_local_vars=<Bool> Current: False Return data into local variables from column names SqlMagic.displaylimit=<Int> Current: None Automatically limit the number of rows displayed (full result set is still stored) SqlMagic.dsn_filename=<Unicode> Current: 'odbc.ini' Path to DSN file. When the first argument is of the form [section], a sqlalchemy connection string is formed from the matching section in the DSN file. SqlMagic.feedback=<Bool> Current: True Print number of rows affected by DML SqlMagic.short_errors=<Bool> Current: True Don't display the full traceback on SQL Programming Error SqlMagic.style=<Unicode> Current: 'DEFAULT' Set the table printing style to any of prettytable's defined styles (currently DEFAULT, MSWORD_FRIENDLY, PLAIN_COLUMNS, RANDOM) %% sql \\ d 6 rows affected. Schema Name Type Owner public accounts table postgres public flat table postgres public t_cities table postgres public t_cities_id_seq sequence postgres public t_weather_observations table postgres public t_weather_observations_id_seq sequence postgres uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com\" % sql { uri } 'Connected: postgres@None' SQL for table deletion and creation %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , person_first varchar ( 255 ), person_last varchar ( 255 ), country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ) ); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres Done. Done. Done. Done. [] %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema' ; * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False True False public person postgres None True False True False SQL to insert rows. %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] The pg_relation_size() function returns the size of the table only, not included indexes or additional objects. %% sql SELECT pg_size_pretty ( pg_relation_size ( 'Country' )); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 1 rows affected. pg_size_pretty 8192 bytes %% sql INSERT INTO Person ( person_first , person_last , country_id ) VALUES ( 'Napolean' , 'Bonaparte' , 'FR' ), ( 'Luis' , 'Alvarez' , 'CU' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] Accessing the RDBMS dictionary. %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 9 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False True False public person postgres None True False True False information_schema sql_features postgres None False False False False information_schema sql_implementation_info postgres None False False False False information_schema sql_languages postgres None False False False False information_schema sql_packages postgres None False False False False information_schema sql_parts postgres None False False False False information_schema sql_sizing postgres None False False False False information_schema sql_sizing_profiles postgres None False False False False %% sql SELECT sql FROM postgres WHERE name = 'Person' ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres (psycopg2.errors.UndefinedTable) relation \"postgres\" does not exist LINE 1: SELECT sql FROM postgres ^ [SQL: SELECT sql FROM postgres WHERE name='Person';] (Background on this error at: http://sqlalche.me/e/f405) SQL as a Query Language. %% sql SELECT person_first as first , person_last AS last , country_name AS nationality FROM Person INNER JOIN country ON Person . country_id = Country . country_id ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. first last nationality Napolean Bonaparte France Luis Alvarez CUBA Visualizing the entitry-relationship diagram (ERd). % config ? Docstring: configure IPython %config Class[.trait=value] This magic exposes most of the IPython config system. Any Configurable class should be able to be configured with the simple line:: %config Class.trait=value Where `value` will be resolved in the user's namespace, if it is an expression or variable name. Examples -------- To see what classes are available for config, pass no arguments:: In [1]: %config Available objects for config: TerminalInteractiveShell HistoryManager PrefilterManager AliasManager IPCompleter DisplayFormatter To view what is configurable on a given class, just pass the class name:: In [2]: %config IPCompleter IPCompleter options ----------------- IPCompleter.omit__names=<Enum> Current: 2 Choices: (0, 1, 2) Instruct the completer to omit private method names Specifically, when completing on ``object.<tab>``. When 2 [default]: all names that start with '_' will be excluded. When 1: all 'magic' names (``__foo__``) will be excluded. When 0: nothing will be excluded. IPCompleter.merge_completions=<CBool> Current: True Whether to merge completion results into a single list If False, only the completion results from the first non-empty completer will be returned. IPCompleter.limit_to__all__=<CBool> Current: False Instruct the completer to use __all__ for the completion Specifically, when completing on ``object.<tab>``. When True: only those names in obj.__all__ will be included. When False [default]: the __all__ attribute is ignored IPCompleter.greedy=<CBool> Current: False Activate greedy completion This will enable completion on elements of lists, results of function calls, etc., but can be unsafe because the code is actually evaluated on TAB. but the real use is in setting values:: In [3]: %config IPCompleter.greedy = True and these values are read from the user_ns if they are variables:: In [4]: feeling_greedy=False In [5]: %config IPCompleter.greedy = feeling_greedy File: /opt/conda/lib/python3.7/site-packages/IPython/core/magics/config.py import ibis import eralchemy from sqlalchemy import create_engine engine = create_engine ( uri ) conn = engine . connect () import os from eralchemy import render_er if not os . path . exists ( 'erd_from_sqlalchemy.png' ): render_er ( uri , 'erd_from_sqlalchemy.png' ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/base.py:2972: SAWarning: Did not recognize type 'point' of column 'location' \"Did not recognize type '%s' of column '%s'\" % (attype, name)","title":"6. Example"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#homework-walk-through","text":"Convert the flat file data in data/flat.csv into a well-structured relational database in SQLite3 stored as data/faculty.db . Note - salary information is confidential and should be kept in a separate table from other personal data. import pandas as pd flat = pd . read_csv ( '../data/flat.csv' , keep_default_na = False ) flat . sample ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name gender age height weight salary nationality code country language1 language2 language3 first last 899 Lucien Pittman Male 61 1.87 58 73000 Danish DK Denmark AutoIt Dylan Transact-SQL Lucien Pittman 1516 Zane Calhoun Male 60 1.93 48 127000 Greek GR Greece Io Java Zane Calhoun 827 Lauran Willis Female 34 1.92 75 89000 Romanian RO Romania Lauran Willis flat . to_sql ( 'flat' , conn ) %% sql \\ d * postgresql+psycopg2://postgres:***@db.postgres.app.com 6 rows affected. Schema Name Type Owner public accounts table postgres public flat table postgres public t_cities table postgres public t_cities_id_seq sequence postgres public t_weather_observations table postgres public t_weather_observations_id_seq sequence postgres %% sql USE faculty ; * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres (psycopg2.errors.SyntaxError) syntax error at or near \"USE\" LINE 1: USE faculty; ^ [SQL: USE faculty;] (Background on this error at: http://sqlalche.me/e/f405) %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , name varchar ( 255 ), age INTEGER NOT NULL , country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ) ); * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres Done. Done. Done. Done. [] %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] %% sql DELETE FROM Country * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 2 rows affected. [] %% sql SELECT * FROM Country * postgresql+psycopg2://postgres:***@db.postgres.app.com/postgres 0 rows affected. country_id country_name from sqlalchemy import create_engine engine = create_engine ( uri ) conn = engine . connect () flat . columns Index(['name', 'gender', 'age', 'height', 'weight', 'salary', 'nationality', 'code', 'country', 'language1', 'language2', 'language3', 'first', 'last'], dtype='object') flat . rename ( mapper = { 'code' : 'country_id' , 'country' : 'country_name' }, inplace = True ) country = flat[['country_id', 'country_name']]country.set_index('country_id').to_sql('Country', engine, if_exists='append')","title":"Homework walk-through"},{"location":"_nb/%3DPy/02_SQL/01_RDBMS/#sql","text":"SELECT * FROM Country flat . to_sql ? Signature: flat . to_sql ( name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None , ) Docstring: Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : string Name of SQL table. con : sqlalchemy.engine.Engine or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. schema : string, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : string or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Rows will be written in batches of this size at a time. By default, all rows will be written at once. dtype : dict, optional Specifying the datatype for columns. The keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. method : {None, 'multi', callable}, default None Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] http://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) >>> df1.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5')] Overwrite the table with just ``df1``. >>> df1.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 4'), (1, 'User 5')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\"A\": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) >>> engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] File: /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py Type: method","title":"%%sql"},{"location":"_nb/%3DPy/02_SQL/02_SQL/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RDBMS For more SQL examples in the SQLite3 dialect, seee SQLite3 tutorial . For a deep dive, see SQL Queries for Mere Mortals . RDBMS concepts \u00b6 Codd's 12 rules OLTP and OLAP \u00b6 OLTP Normalized schema OLAP (Online analytical processing): At the core is an OLAP cube (also called a 'multidimensional cube' or a hypercube) numeric facts called measures that are categorized by dimensions denormalized schema multi-dimensional analytical (MDA) queries typically stored in a star schema or snowflake schema mostly optimized for read Generated from OLTP databases by ETL (Extract-Transform-Load) operations consolidation (roll-up), drill-down, and slicing and dicing Rotate (or Pivot ) : s\u00e9lection du couple de dimensions qui formera le r\u00e9sultat de la requ\u00eate, Slicing : extraction d'une tranche d'information, Scoping (or Dicing ) : extraction d'un bloc de [[donn\u00e9e]]s (op\u00e9ration plus g\u00e9n\u00e9rale que le ''slicing''), Drill-up : synth\u00e8se des informations en fonction d'une dimension (exemple de ''drill-up'' sur l'axe temps : passer de la pr\u00e9sentation de l'information jour par jour sur une ann\u00e9e, \u00e0 une valeur synth\u00e9tique pour l'ann\u00e9e), Drill-down : c'est l'\u00e9quivalent d'un \u00ab zoom \u00bb, op\u00e9ration inverse du ''drill-up'', Drill-through : lorsqu'on ne dispose que de [[donn\u00e9e]]s agr\u00e9g\u00e9es (indicateurs totalis\u00e9s), le ''drill through'' permet d'acc\u00e9der au d\u00e9tail \u00e9l\u00e9mentaire des informations (voir notamment les outils H-OLAP). Types of REBMS \u00b6 Data lake Data warehouse Data mart Data marts typically use a star schema that is customized for the analysis needs. For example, the finance department in a hospital may be most interested in Facts about Claims. Robustness and scaling \u00b6 Replication Sharding B. Basic SQL queries \u00b6 Data we will work with in Part B \u00b6 % load_ext sql % sql sqlite : /// data / faculty . db 'Connected: @data/faculty.db' %% sql SELECT * FROM sqlite_master WHERE type = 'table' ; * sqlite:///data/faculty.db Done. type name tbl_name rootpage sql table person person 2 CREATE TABLE person ( \"index\" BIGINT, person_id BIGINT, first TEXT, last TEXT, age BIGINT, height FLOAT, weight BIGINT, country_id TEXT, gender_id BIGINT ) table confidential confidential 18 CREATE TABLE confidential ( \"index\" BIGINT, person_id BIGINT, salary BIGINT ) table person_language person_language 33 CREATE TABLE person_language ( \"index\" BIGINT, person_id BIGINT, language_id BIGINT ) table language language 50 CREATE TABLE language ( \"index\" BIGINT, language_id BIGINT, language_name TEXT ) table gender gender 55 CREATE TABLE gender ( \"index\" BIGINT, gender_id BIGINT, gender TEXT ) table country country 57 CREATE TABLE country ( \"index\" BIGINT, country_id TEXT, country TEXT, nationality TEXT ) table df df 59 CREATE TABLE df ( \"index\" BIGINT, person TEXT, time BIGINT, bsl BIGINT ) Basic Structure \u00b6 SELECT DISTINCT value_expression AS alias FROM tables AS alias WHERE predicate ORDER BY value_expression Types \u00b6 - Character (Fixed width, variable width) - National Character (Fixed width, variable width) - Binary - Numeric (Exact, Arpproximate) - Boolean - DateTime - Interval The SQL standard specifies that character strings and datetime literals are enclosed by single quotes. Two single quotes wihtin a string is intepreted as a literal single quote. 'Gilligan''s island' The CAST function \u00b6 CAST ( X as CHARACTER ( 10 )) Value expreesion \u00b6 Literal Column reference Function CASES (Value expression) (SELECT expression) which may be prefixed with unary operaors - and + and combined with binary operators appropriate for the data type. Bineary operators \u00b6 Concatenation \u00b6 A || B Mathematical \u00b6 A + B A - B A * B A / B Data and time arithmetic \u00b6 '2018-08-29' + 3 '11:59' + '00:01' %% sql SELECT DISTINCT language_name FROM language LIMIT 5 ; * sqlite:///data/faculty.db Done. language_name PHP Clojure Dylan GNU Octave D Sorting \u00b6 SELECT DISTINCT value_expression AS alias FROM tables AS alias ORDER BY value_expression %% sql SELECT DISTINCT language_name FROM language ORDER BY language_name ASC LIMIT 5 ; * sqlite:///data/faculty.db Done. language_name ASP Assembly AutoIt Awk Bash Filtering \u00b6 For efficiency, place the most stringent filters first. SELECT DISTINCT value_expression AS alias FROM tables AS alias WHERE predicate ORDER BY value_expression Predicates for filtering rows \u00b6 - Comparison operators (=, <>, <, >, <=, >=) - BETWEEN start AND end - IN(A, B, C) - LIKE - IS NULL - REGEX Use NOT prefix for negation Combining predicates \u00b6 AND OR USe parenthesis to indicate order of evaluation for compound statements. %% sql SELECT first , last , age FROM person WHERE age BETWEEN 16 AND 17 LIMIT 5 ; * sqlite:///data/faculty.db Done. first last age Antoine Beard 16 Augustine Mejia 16 Boris Mejia 16 Brain Haney 16 Burl Mayo 17 Joins \u00b6 Joins combine data from 1 or more tables to form a new result set. Natural join \u00b6 Uses all common columns in Tables 1 and 2 for JOIN FROM Table1 NATURAL INNER JOIN Table 2 Inner join \u00b6 General form of INNER JOIN uisng ON FROM Table1 INNER JOIN Table2 ON Table1 . Column = Table2 . Column If there is a common column in both tables FROM Table1 INNER JOIN Table2 USING Column Joining more than two tables From ( Table1 INNER JOIN Table2 ON Table1 . column1 = Table2 . Column1 ) INNER JOIN Table3 ON Table3 . column2 = Table2 . Column2 Outer join \u00b6 General form of OUTER JOIN uisng ON FROM Table1 RIGHT OUTER JOIN Table2 ON Table1 . Column = Table2 . Column FROM Table1 LEFT OUTER JOIN Table2 ON Table1 . Column = Table2 . Column FROM Table1 FULL OUTER JOIN Table2 ON Table1 . Column = Table2 . Column %% sql SELECT first , last , language_name FROM person INNER JOIN person_language ON person . person_id = person_language . person_id INNER JOIN language ON language . language_id = person_language . language_id LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Aaron Kirby GNU Octave Aaron Kirby haXe Aaron Kirby Falcon Abram Allen TypeScript Abram Boyer Io Abram Boyer Lua Abram Boyer Falcon Adan Brown F# Adolph Dalton Dart Set operations \u00b6 SELECT a , b FROM table1 SetOp SELECT a , b FROM table2 wehre SetOp is INTERSECT , EXCEPT , UNION or UNION ALL . Intersection \u00b6 INTERSECT Alternative using INNER JOIN Union \u00b6 UNION UNION ALL ( does not eliminate duplicate rows ) Difference \u00b6 EXCEPT Alternative using OUTER JOIN with test for NULL %% sql DROP VIEW IF EXISTS language_view ; CREATE VIEW language_view AS SELECT first , last , language_name FROM person INNER JOIN person_language ON person . person_id = person_language . person_id INNER JOIN language ON language . language_id = person_language . language_id ; * sqlite:///data/faculty.db Done. Done. [] %% sql SELECt * FROM language_view LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Aaron Kirby GNU Octave Aaron Kirby haXe Aaron Kirby Falcon Abram Allen TypeScript Abram Boyer Io Abram Boyer Lua Abram Boyer Falcon Adan Brown F# Adolph Dalton Dart %% sql SELECt * FROM language_view WHERE language_name = 'Python' UNION SELECt * FROM language_view WHERE language_name = 'Haskell' LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Andree Douglas Haskell Arlie Terrell Python Boyd Blackwell Haskell Buck Howe Haskell Carlton Richard Haskell Carylon Zamora Python Clarisa Rodgers Python Dinorah O'brien Haskell Dorian Lloyd Haskell %% sql SELECt * FROM language_view WHERE language_name IN ( 'Python' , 'Haskell' ) ORDER BY first LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Andree Douglas Haskell Arlie Terrell Python Boyd Blackwell Haskell Buck Howe Haskell Carlton Richard Haskell Carylon Zamora Python Clarisa Rodgers Python Dinorah O'brien Haskell Dorian Lloyd Haskell Subqueries \u00b6 As column expresions \u00b6 SELECT a , b , ( SELECT MAX ( c ) FROM table2 INNER JOIN table1 USING column1 ) as max_c FROM table1 As filters \u00b6 SELECT a , b , FROM table1 WHERE b > ( SELECT AVG ( b ) FROM table1 ) Quantified Subqueires \u00b6 ALl SOME ANY EXISTS SELECT a , b , FROM table1 WHERE EXISTS ( SELECT c FROM table2 ) %% sql SELECT first , last , language_name FROM person , language WHERE language_name IN ( SELECT language_name FROM language_view WHERe first = 'Abram' AND last = 'Boyer' ) LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Io Aaron Kirby Io Abram Allen Io Abram Boyer Io Adan Brown Io Adolph Dalton Io Adrian Blevins Io Agustin Fulton Io Agustin Mcdonald Io Alberto Dudley Io Aggregate functions \u00b6 COUNT MIN MAX AVG SUM %% sql SELECT count ( language_name ) FROM language_view ; * sqlite:///data/faculty.db Done. count(language_name) 2297 Grouping \u00b6 SELECT a , MIN ( b ) AS min_b , MAX ( b ) AS max_b , AVG ( b ) AS mean_b FROM table GROUP BY a HAVING mean_b > 5 The HAVING is analagous to the WHERE clause, but filters on aggregate conditions. Note that the WHERE statement filters rows BEFORE the grouping is done. Note: Any variable in the SELECT part that is not an aggregte function needs to be in the GROUP BY part. SELECT a , b , c , COUNT ( d ) FROM table GROUP BY a , b , c %% sql SELECT language_name , count ( * ) AS n FROM language_view GROUP BY language_name HAVING n > 45 ; * sqlite:///data/faculty.db Done. language_name n AutoIt 61 Bash 48 ECMAScript 48 GNU Octave 49 JavaScript 48 Perl 55 PowerShell 50 Prolog 50 The CASE switch \u00b6 Simple CASE \u00b6 SELECT name , ( CASE sex WHEN 'M' THEN 1 . 5 * dose WHEN 'F' THEN dose END ) as adjusted_dose FROM table Searched CASE \u00b6 SELECT name , ( CASE WHEN sex = 'M' THEN 1 . 5 * dose WHEN sex = 'F' THEN dose END ) as adjusted_dose FROM table %% sql SELECT first , last , language_name , ( CASE WHEN language_name LIKE 'H%' THEN 'Hire' ELSE 'FIRE' END ) AS outcome FROM language_view LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name outcome Aaron Alexander Haskell Hire Aaron Kirby GNU Octave FIRE Aaron Kirby haXe Hire Aaron Kirby Falcon FIRE Abram Allen TypeScript FIRE Abram Boyer Io FIRE Abram Boyer Lua FIRE Abram Boyer Falcon FIRE Adan Brown F# FIRE Adolph Dalton Dart FIRE C. Window Functions \u00b6 We use the PostgreSQL databsaee because window functions are not supported in SQLite3 yet % load_ext sql % sql postgresql : // postgres : postgres @postgres . app . com / postgres 'Connected: postgres@postgres' import pandas as pd import numpy as np from collections import OrderedDict np . random . seed ( 23 ) n = 10 df = pd . DataFrame ( OrderedDict ( person = np . random . choice ([ 'A' , 'B' , 'C' , 'D' ], n ,), time = np . random . randint ( 0 , 10 , n ), bsl = np . random . randint ( 50 , 400 , n ))) df . sort_values ([ 'person' , 'time' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person time bsl 8 A 0 115 5 A 2 237 2 A 3 129 7 B 5 86 3 B 6 396 4 C 1 107 1 C 9 347 6 D 5 89 9 D 5 221 0 D 7 98 % sql DROP TABLE IF EXISTS df * postgresql://postgres:***@172.26.0.3/postgres Done. [] Magic shortcut to creating a database table from pandas DataFrame. % sql persist df * postgresql://postgres:***@172.26.0.3/postgres 'Persisted df' Over creates widows \u00b6 %% sql SELECT person , time , bsl , row_number () OVER () FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number D 7 98 1 C 9 347 2 A 3 129 3 B 6 396 4 C 1 107 5 A 2 237 6 D 5 89 7 B 5 86 8 A 0 115 9 D 5 221 10 Order by \u00b6 %% sql SELECT person , time , bsl , row_number () OVER ( ORDER BY person , time ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number A 0 115 1 A 2 237 2 A 3 129 3 B 5 86 4 B 6 396 5 C 1 107 6 C 9 347 7 D 5 221 8 D 5 89 9 D 7 98 10 Partition by \u00b6 %% sql SELECT person , time , bsl , row_number () OVER ( PARTITION BY person ORDER BY time ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number A 0 115 1 A 2 237 2 A 3 129 3 B 5 86 1 B 6 396 2 C 1 107 1 C 9 347 2 D 5 221 1 D 5 89 2 D 7 98 3 %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER ( PARTITION BY person ORDER BY time ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl string_agg A 0 115 115 A 2 237 115, 237 A 3 129 115, 237, 129 B 5 86 86 B 6 396 86, 396 C 1 107 107 C 9 347 107, 347 D 5 221 221, 89 D 5 89 221, 89 D 7 98 221, 89, 98 Specifying rows in window \u00b6 %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER ( PARTITION BY person ORDER BY time ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl string_agg A 0 115 115, 237 A 2 237 115, 237, 129 A 3 129 237, 129 B 5 86 86, 396 B 6 396 86, 396 C 1 107 107, 347 C 9 347 107, 347 D 5 221 221, 89 D 5 89 221, 89, 98 D 7 98 89, 98 Using window functions \u00b6 %% sql SELECT person , time , bsl , row_number () OVER win AS row_number , rank () OVER win AS rank , dense_rank () OVER win AS dense_rank , percent_rank () OVER win AS percent_rank , cume_dist () OVER win AS cume_dist FROM df WINDOW win AS ( ORDER BY person ); * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number rank dense_rank percent_rank cume_dist A 0 115 1 1 1 0.0 0.3 A 2 237 2 1 1 0.0 0.3 A 3 129 3 1 1 0.0 0.3 B 6 396 4 4 2 0.333333333333333 0.5 B 5 86 5 4 2 0.333333333333333 0.5 C 1 107 6 6 3 0.555555555555556 0.7 C 9 347 7 6 3 0.555555555555556 0.7 D 5 221 8 8 4 0.777777777777778 1.0 D 5 89 9 8 4 0.777777777777778 1.0 D 7 98 10 8 4 0.777777777777778 1.0 Using aggregate functions \u00b6 %% sql SELECT person , time , bsl , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg , MIN ( bsl ) OVER win AS bsl_min , MAX ( bsl ) over win as bsl_max , FIRST_VALUE ( bsl ) OVER win as bsl_start , LAST_VALUE ( bsl ) OVER win as bsl_end FROM df WINDOW win AS ( PARTITION BY person ORDER BY time ); * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl bsl_sum bsl_avg bsl_min bsl_max bsl_start bsl_end A 0 115 115 115.0000000000000000 115 115 115 115 A 2 237 352 176.0000000000000000 115 237 115 237 A 3 129 481 160.3333333333333333 115 237 115 129 B 5 86 86 86.0000000000000000 86 86 86 86 B 6 396 482 241.0000000000000000 86 396 86 396 C 1 107 107 107.0000000000000000 107 107 107 107 C 9 347 454 227.0000000000000000 107 347 107 347 D 5 221 310 155.0000000000000000 89 221 221 89 D 5 89 310 155.0000000000000000 89 221 221 89 D 7 98 408 136.0000000000000000 89 221 221 98 Using rows and range to constrain windows \u00b6 %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER win AS vals , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg FROM df WINDOW win AS ( PARTITION BY person ORDER BY time ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING ) ORDER BY person , time ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl vals bsl_sum bsl_avg A 0 115 115, 237 352 176.0000000000000000 A 2 237 115, 237, 129 481 160.3333333333333333 A 3 129 237, 129 366 183.0000000000000000 B 5 86 86, 396 482 241.0000000000000000 B 6 396 86, 396 482 241.0000000000000000 C 1 107 107, 347 454 227.0000000000000000 C 9 347 107, 347 454 227.0000000000000000 D 5 221 221, 89 310 155.0000000000000000 D 5 89 221, 89, 98 408 136.0000000000000000 D 7 98 89, 98 187 93.5000000000000000 Frames using Rows and Range \u00b6 For Range, all rows with the same ORDER BY value are considered peers. %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER win AS vals , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg FROM df WINDOW win AS ( ORDER BY person ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) ORDER BY person , time ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl vals bsl_sum bsl_avg A 0 115 115 115 115.0000000000000000 A 2 237 115, 237 352 176.0000000000000000 A 3 129 115, 237, 129 481 160.3333333333333333 B 5 86 115, 237, 129, 396, 86 963 192.6000000000000000 B 6 396 115, 237, 129, 396 877 219.2500000000000000 C 1 107 115, 237, 129, 396, 86, 107 1070 178.3333333333333333 C 9 347 115, 237, 129, 396, 86, 107, 347 1417 202.4285714285714286 D 5 221 115, 237, 129, 396, 86, 107, 347, 221 1638 204.7500000000000000 D 5 89 115, 237, 129, 396, 86, 107, 347, 221, 89 1727 191.8888888888888889 D 7 98 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000 %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER win AS vals , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg FROM df WINDOW win AS ( ORDER BY person RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) ORDER BY person , time ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl vals bsl_sum bsl_avg A 0 115 115, 237, 129 481 160.3333333333333333 A 2 237 115, 237, 129 481 160.3333333333333333 A 3 129 115, 237, 129 481 160.3333333333333333 B 5 86 115, 237, 129, 396, 86 963 192.6000000000000000 B 6 396 115, 237, 129, 396, 86 963 192.6000000000000000 C 1 107 115, 237, 129, 396, 86, 107, 347 1417 202.4285714285714286 C 9 347 115, 237, 129, 396, 86, 107, 347 1417 202.4285714285714286 D 5 221 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000 D 5 89 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000 D 7 98 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000","title":"02 SQL"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#rdbms-concepts","text":"Codd's 12 rules","title":"RDBMS concepts"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#oltp-and-olap","text":"OLTP Normalized schema OLAP (Online analytical processing): At the core is an OLAP cube (also called a 'multidimensional cube' or a hypercube) numeric facts called measures that are categorized by dimensions denormalized schema multi-dimensional analytical (MDA) queries typically stored in a star schema or snowflake schema mostly optimized for read Generated from OLTP databases by ETL (Extract-Transform-Load) operations consolidation (roll-up), drill-down, and slicing and dicing Rotate (or Pivot ) : s\u00e9lection du couple de dimensions qui formera le r\u00e9sultat de la requ\u00eate, Slicing : extraction d'une tranche d'information, Scoping (or Dicing ) : extraction d'un bloc de [[donn\u00e9e]]s (op\u00e9ration plus g\u00e9n\u00e9rale que le ''slicing''), Drill-up : synth\u00e8se des informations en fonction d'une dimension (exemple de ''drill-up'' sur l'axe temps : passer de la pr\u00e9sentation de l'information jour par jour sur une ann\u00e9e, \u00e0 une valeur synth\u00e9tique pour l'ann\u00e9e), Drill-down : c'est l'\u00e9quivalent d'un \u00ab zoom \u00bb, op\u00e9ration inverse du ''drill-up'', Drill-through : lorsqu'on ne dispose que de [[donn\u00e9e]]s agr\u00e9g\u00e9es (indicateurs totalis\u00e9s), le ''drill through'' permet d'acc\u00e9der au d\u00e9tail \u00e9l\u00e9mentaire des informations (voir notamment les outils H-OLAP).","title":"OLTP and OLAP"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#types-of-rebms","text":"Data lake Data warehouse Data mart Data marts typically use a star schema that is customized for the analysis needs. For example, the finance department in a hospital may be most interested in Facts about Claims.","title":"Types of REBMS"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#robustness-and-scaling","text":"Replication Sharding","title":"Robustness and scaling"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#b-basic-sql-queries","text":"","title":"B. Basic SQL queries"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#data-we-will-work-with-in-part-b","text":"% load_ext sql % sql sqlite : /// data / faculty . db 'Connected: @data/faculty.db' %% sql SELECT * FROM sqlite_master WHERE type = 'table' ; * sqlite:///data/faculty.db Done. type name tbl_name rootpage sql table person person 2 CREATE TABLE person ( \"index\" BIGINT, person_id BIGINT, first TEXT, last TEXT, age BIGINT, height FLOAT, weight BIGINT, country_id TEXT, gender_id BIGINT ) table confidential confidential 18 CREATE TABLE confidential ( \"index\" BIGINT, person_id BIGINT, salary BIGINT ) table person_language person_language 33 CREATE TABLE person_language ( \"index\" BIGINT, person_id BIGINT, language_id BIGINT ) table language language 50 CREATE TABLE language ( \"index\" BIGINT, language_id BIGINT, language_name TEXT ) table gender gender 55 CREATE TABLE gender ( \"index\" BIGINT, gender_id BIGINT, gender TEXT ) table country country 57 CREATE TABLE country ( \"index\" BIGINT, country_id TEXT, country TEXT, nationality TEXT ) table df df 59 CREATE TABLE df ( \"index\" BIGINT, person TEXT, time BIGINT, bsl BIGINT )","title":"Data we will work with in Part B"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#basic-structure","text":"SELECT DISTINCT value_expression AS alias FROM tables AS alias WHERE predicate ORDER BY value_expression","title":"Basic Structure"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#types","text":"- Character (Fixed width, variable width) - National Character (Fixed width, variable width) - Binary - Numeric (Exact, Arpproximate) - Boolean - DateTime - Interval The SQL standard specifies that character strings and datetime literals are enclosed by single quotes. Two single quotes wihtin a string is intepreted as a literal single quote. 'Gilligan''s island'","title":"Types"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#the-cast-function","text":"CAST ( X as CHARACTER ( 10 ))","title":"The CAST function"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#value-expreesion","text":"Literal Column reference Function CASES (Value expression) (SELECT expression) which may be prefixed with unary operaors - and + and combined with binary operators appropriate for the data type.","title":"Value expreesion"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#bineary-operators","text":"","title":"Bineary operators"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#concatenation","text":"A || B","title":"Concatenation"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#mathematical","text":"A + B A - B A * B A / B","title":"Mathematical"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#data-and-time-arithmetic","text":"'2018-08-29' + 3 '11:59' + '00:01' %% sql SELECT DISTINCT language_name FROM language LIMIT 5 ; * sqlite:///data/faculty.db Done. language_name PHP Clojure Dylan GNU Octave D","title":"Data and time arithmetic"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#sorting","text":"SELECT DISTINCT value_expression AS alias FROM tables AS alias ORDER BY value_expression %% sql SELECT DISTINCT language_name FROM language ORDER BY language_name ASC LIMIT 5 ; * sqlite:///data/faculty.db Done. language_name ASP Assembly AutoIt Awk Bash","title":"Sorting"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#filtering","text":"For efficiency, place the most stringent filters first. SELECT DISTINCT value_expression AS alias FROM tables AS alias WHERE predicate ORDER BY value_expression","title":"Filtering"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#predicates-for-filtering-rows","text":"- Comparison operators (=, <>, <, >, <=, >=) - BETWEEN start AND end - IN(A, B, C) - LIKE - IS NULL - REGEX Use NOT prefix for negation","title":"Predicates for filtering rows"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#combining-predicates","text":"AND OR USe parenthesis to indicate order of evaluation for compound statements. %% sql SELECT first , last , age FROM person WHERE age BETWEEN 16 AND 17 LIMIT 5 ; * sqlite:///data/faculty.db Done. first last age Antoine Beard 16 Augustine Mejia 16 Boris Mejia 16 Brain Haney 16 Burl Mayo 17","title":"Combining predicates"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#joins","text":"Joins combine data from 1 or more tables to form a new result set.","title":"Joins"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#natural-join","text":"Uses all common columns in Tables 1 and 2 for JOIN FROM Table1 NATURAL INNER JOIN Table 2","title":"Natural join"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#inner-join","text":"General form of INNER JOIN uisng ON FROM Table1 INNER JOIN Table2 ON Table1 . Column = Table2 . Column If there is a common column in both tables FROM Table1 INNER JOIN Table2 USING Column Joining more than two tables From ( Table1 INNER JOIN Table2 ON Table1 . column1 = Table2 . Column1 ) INNER JOIN Table3 ON Table3 . column2 = Table2 . Column2","title":"Inner join"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#outer-join","text":"General form of OUTER JOIN uisng ON FROM Table1 RIGHT OUTER JOIN Table2 ON Table1 . Column = Table2 . Column FROM Table1 LEFT OUTER JOIN Table2 ON Table1 . Column = Table2 . Column FROM Table1 FULL OUTER JOIN Table2 ON Table1 . Column = Table2 . Column %% sql SELECT first , last , language_name FROM person INNER JOIN person_language ON person . person_id = person_language . person_id INNER JOIN language ON language . language_id = person_language . language_id LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Aaron Kirby GNU Octave Aaron Kirby haXe Aaron Kirby Falcon Abram Allen TypeScript Abram Boyer Io Abram Boyer Lua Abram Boyer Falcon Adan Brown F# Adolph Dalton Dart","title":"Outer join"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#set-operations","text":"SELECT a , b FROM table1 SetOp SELECT a , b FROM table2 wehre SetOp is INTERSECT , EXCEPT , UNION or UNION ALL .","title":"Set operations"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#intersection","text":"INTERSECT Alternative using INNER JOIN","title":"Intersection"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#union","text":"UNION UNION ALL ( does not eliminate duplicate rows )","title":"Union"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#difference","text":"EXCEPT Alternative using OUTER JOIN with test for NULL %% sql DROP VIEW IF EXISTS language_view ; CREATE VIEW language_view AS SELECT first , last , language_name FROM person INNER JOIN person_language ON person . person_id = person_language . person_id INNER JOIN language ON language . language_id = person_language . language_id ; * sqlite:///data/faculty.db Done. Done. [] %% sql SELECt * FROM language_view LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Aaron Kirby GNU Octave Aaron Kirby haXe Aaron Kirby Falcon Abram Allen TypeScript Abram Boyer Io Abram Boyer Lua Abram Boyer Falcon Adan Brown F# Adolph Dalton Dart %% sql SELECt * FROM language_view WHERE language_name = 'Python' UNION SELECt * FROM language_view WHERE language_name = 'Haskell' LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Andree Douglas Haskell Arlie Terrell Python Boyd Blackwell Haskell Buck Howe Haskell Carlton Richard Haskell Carylon Zamora Python Clarisa Rodgers Python Dinorah O'brien Haskell Dorian Lloyd Haskell %% sql SELECt * FROM language_view WHERE language_name IN ( 'Python' , 'Haskell' ) ORDER BY first LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Haskell Andree Douglas Haskell Arlie Terrell Python Boyd Blackwell Haskell Buck Howe Haskell Carlton Richard Haskell Carylon Zamora Python Clarisa Rodgers Python Dinorah O'brien Haskell Dorian Lloyd Haskell","title":"Difference"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#subqueries","text":"","title":"Subqueries"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#as-column-expresions","text":"SELECT a , b , ( SELECT MAX ( c ) FROM table2 INNER JOIN table1 USING column1 ) as max_c FROM table1","title":"As column expresions"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#as-filters","text":"SELECT a , b , FROM table1 WHERE b > ( SELECT AVG ( b ) FROM table1 )","title":"As filters"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#quantified-subqueires","text":"ALl SOME ANY EXISTS SELECT a , b , FROM table1 WHERE EXISTS ( SELECT c FROM table2 ) %% sql SELECT first , last , language_name FROM person , language WHERE language_name IN ( SELECT language_name FROM language_view WHERe first = 'Abram' AND last = 'Boyer' ) LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name Aaron Alexander Io Aaron Kirby Io Abram Allen Io Abram Boyer Io Adan Brown Io Adolph Dalton Io Adrian Blevins Io Agustin Fulton Io Agustin Mcdonald Io Alberto Dudley Io","title":"Quantified Subqueires"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#aggregate-functions","text":"COUNT MIN MAX AVG SUM %% sql SELECT count ( language_name ) FROM language_view ; * sqlite:///data/faculty.db Done. count(language_name) 2297","title":"Aggregate functions"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#grouping","text":"SELECT a , MIN ( b ) AS min_b , MAX ( b ) AS max_b , AVG ( b ) AS mean_b FROM table GROUP BY a HAVING mean_b > 5 The HAVING is analagous to the WHERE clause, but filters on aggregate conditions. Note that the WHERE statement filters rows BEFORE the grouping is done. Note: Any variable in the SELECT part that is not an aggregte function needs to be in the GROUP BY part. SELECT a , b , c , COUNT ( d ) FROM table GROUP BY a , b , c %% sql SELECT language_name , count ( * ) AS n FROM language_view GROUP BY language_name HAVING n > 45 ; * sqlite:///data/faculty.db Done. language_name n AutoIt 61 Bash 48 ECMAScript 48 GNU Octave 49 JavaScript 48 Perl 55 PowerShell 50 Prolog 50","title":"Grouping"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#the-case-switch","text":"","title":"The CASE switch"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#simple-case","text":"SELECT name , ( CASE sex WHEN 'M' THEN 1 . 5 * dose WHEN 'F' THEN dose END ) as adjusted_dose FROM table","title":"Simple CASE"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#searched-case","text":"SELECT name , ( CASE WHEN sex = 'M' THEN 1 . 5 * dose WHEN sex = 'F' THEN dose END ) as adjusted_dose FROM table %% sql SELECT first , last , language_name , ( CASE WHEN language_name LIKE 'H%' THEN 'Hire' ELSE 'FIRE' END ) AS outcome FROM language_view LIMIT 10 ; * sqlite:///data/faculty.db Done. first last language_name outcome Aaron Alexander Haskell Hire Aaron Kirby GNU Octave FIRE Aaron Kirby haXe Hire Aaron Kirby Falcon FIRE Abram Allen TypeScript FIRE Abram Boyer Io FIRE Abram Boyer Lua FIRE Abram Boyer Falcon FIRE Adan Brown F# FIRE Adolph Dalton Dart FIRE","title":"Searched CASE"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#c-window-functions","text":"We use the PostgreSQL databsaee because window functions are not supported in SQLite3 yet % load_ext sql % sql postgresql : // postgres : postgres @postgres . app . com / postgres 'Connected: postgres@postgres' import pandas as pd import numpy as np from collections import OrderedDict np . random . seed ( 23 ) n = 10 df = pd . DataFrame ( OrderedDict ( person = np . random . choice ([ 'A' , 'B' , 'C' , 'D' ], n ,), time = np . random . randint ( 0 , 10 , n ), bsl = np . random . randint ( 50 , 400 , n ))) df . sort_values ([ 'person' , 'time' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person time bsl 8 A 0 115 5 A 2 237 2 A 3 129 7 B 5 86 3 B 6 396 4 C 1 107 1 C 9 347 6 D 5 89 9 D 5 221 0 D 7 98 % sql DROP TABLE IF EXISTS df * postgresql://postgres:***@172.26.0.3/postgres Done. [] Magic shortcut to creating a database table from pandas DataFrame. % sql persist df * postgresql://postgres:***@172.26.0.3/postgres 'Persisted df'","title":"C.  Window Functions"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#over-creates-widows","text":"%% sql SELECT person , time , bsl , row_number () OVER () FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number D 7 98 1 C 9 347 2 A 3 129 3 B 6 396 4 C 1 107 5 A 2 237 6 D 5 89 7 B 5 86 8 A 0 115 9 D 5 221 10","title":"Over  creates widows"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#order-by","text":"%% sql SELECT person , time , bsl , row_number () OVER ( ORDER BY person , time ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number A 0 115 1 A 2 237 2 A 3 129 3 B 5 86 4 B 6 396 5 C 1 107 6 C 9 347 7 D 5 221 8 D 5 89 9 D 7 98 10","title":"Order by"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#partition-by","text":"%% sql SELECT person , time , bsl , row_number () OVER ( PARTITION BY person ORDER BY time ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number A 0 115 1 A 2 237 2 A 3 129 3 B 5 86 1 B 6 396 2 C 1 107 1 C 9 347 2 D 5 221 1 D 5 89 2 D 7 98 3 %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER ( PARTITION BY person ORDER BY time ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl string_agg A 0 115 115 A 2 237 115, 237 A 3 129 115, 237, 129 B 5 86 86 B 6 396 86, 396 C 1 107 107 C 9 347 107, 347 D 5 221 221, 89 D 5 89 221, 89 D 7 98 221, 89, 98","title":"Partition by"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#specifying-rows-in-window","text":"%% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER ( PARTITION BY person ORDER BY time ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING ) FROM df ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl string_agg A 0 115 115, 237 A 2 237 115, 237, 129 A 3 129 237, 129 B 5 86 86, 396 B 6 396 86, 396 C 1 107 107, 347 C 9 347 107, 347 D 5 221 221, 89 D 5 89 221, 89, 98 D 7 98 89, 98","title":"Specifying rows in window"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#using-window-functions","text":"%% sql SELECT person , time , bsl , row_number () OVER win AS row_number , rank () OVER win AS rank , dense_rank () OVER win AS dense_rank , percent_rank () OVER win AS percent_rank , cume_dist () OVER win AS cume_dist FROM df WINDOW win AS ( ORDER BY person ); * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl row_number rank dense_rank percent_rank cume_dist A 0 115 1 1 1 0.0 0.3 A 2 237 2 1 1 0.0 0.3 A 3 129 3 1 1 0.0 0.3 B 6 396 4 4 2 0.333333333333333 0.5 B 5 86 5 4 2 0.333333333333333 0.5 C 1 107 6 6 3 0.555555555555556 0.7 C 9 347 7 6 3 0.555555555555556 0.7 D 5 221 8 8 4 0.777777777777778 1.0 D 5 89 9 8 4 0.777777777777778 1.0 D 7 98 10 8 4 0.777777777777778 1.0","title":"Using window functions"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#using-aggregate-functions","text":"%% sql SELECT person , time , bsl , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg , MIN ( bsl ) OVER win AS bsl_min , MAX ( bsl ) over win as bsl_max , FIRST_VALUE ( bsl ) OVER win as bsl_start , LAST_VALUE ( bsl ) OVER win as bsl_end FROM df WINDOW win AS ( PARTITION BY person ORDER BY time ); * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl bsl_sum bsl_avg bsl_min bsl_max bsl_start bsl_end A 0 115 115 115.0000000000000000 115 115 115 115 A 2 237 352 176.0000000000000000 115 237 115 237 A 3 129 481 160.3333333333333333 115 237 115 129 B 5 86 86 86.0000000000000000 86 86 86 86 B 6 396 482 241.0000000000000000 86 396 86 396 C 1 107 107 107.0000000000000000 107 107 107 107 C 9 347 454 227.0000000000000000 107 347 107 347 D 5 221 310 155.0000000000000000 89 221 221 89 D 5 89 310 155.0000000000000000 89 221 221 89 D 7 98 408 136.0000000000000000 89 221 221 98","title":"Using aggregate functions"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#using-rows-and-range-to-constrain-windows","text":"%% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER win AS vals , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg FROM df WINDOW win AS ( PARTITION BY person ORDER BY time ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING ) ORDER BY person , time ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl vals bsl_sum bsl_avg A 0 115 115, 237 352 176.0000000000000000 A 2 237 115, 237, 129 481 160.3333333333333333 A 3 129 237, 129 366 183.0000000000000000 B 5 86 86, 396 482 241.0000000000000000 B 6 396 86, 396 482 241.0000000000000000 C 1 107 107, 347 454 227.0000000000000000 C 9 347 107, 347 454 227.0000000000000000 D 5 221 221, 89 310 155.0000000000000000 D 5 89 221, 89, 98 408 136.0000000000000000 D 7 98 89, 98 187 93.5000000000000000","title":"Using rows and range to constrain windows"},{"location":"_nb/%3DPy/02_SQL/02_SQL/#frames-using-rows-and-range","text":"For Range, all rows with the same ORDER BY value are considered peers. %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER win AS vals , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg FROM df WINDOW win AS ( ORDER BY person ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) ORDER BY person , time ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl vals bsl_sum bsl_avg A 0 115 115 115 115.0000000000000000 A 2 237 115, 237 352 176.0000000000000000 A 3 129 115, 237, 129 481 160.3333333333333333 B 5 86 115, 237, 129, 396, 86 963 192.6000000000000000 B 6 396 115, 237, 129, 396 877 219.2500000000000000 C 1 107 115, 237, 129, 396, 86, 107 1070 178.3333333333333333 C 9 347 115, 237, 129, 396, 86, 107, 347 1417 202.4285714285714286 D 5 221 115, 237, 129, 396, 86, 107, 347, 221 1638 204.7500000000000000 D 5 89 115, 237, 129, 396, 86, 107, 347, 221, 89 1727 191.8888888888888889 D 7 98 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000 %% sql SELECT person , time , bsl , STRING_AGG ( CAST ( bsl AS TEXT ), ', ' ) OVER win AS vals , SUM ( bsl ) OVER win AS bsl_sum , AVG ( bsl ) OVER win AS bsl_avg FROM df WINDOW win AS ( ORDER BY person RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) ORDER BY person , time ; * postgresql://postgres:***@172.26.0.3/postgres 10 rows affected. person time bsl vals bsl_sum bsl_avg A 0 115 115, 237, 129 481 160.3333333333333333 A 2 237 115, 237, 129 481 160.3333333333333333 A 3 129 115, 237, 129 481 160.3333333333333333 B 5 86 115, 237, 129, 396, 86 963 192.6000000000000000 B 6 396 115, 237, 129, 396, 86 963 192.6000000000000000 C 1 107 115, 237, 129, 396, 86, 107, 347 1417 202.4285714285714286 C 9 347 115, 237, 129, 396, 86, 107, 347 1417 202.4285714285714286 D 5 221 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000 D 5 89 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000 D 7 98 115, 237, 129, 396, 86, 107, 347, 221, 89, 98 1825 182.5000000000000000","title":"Frames using Rows and Range"},{"location":"_nb/%3DPy/02_SQL/03_Postgres/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Tutorials: - http://www.postgresqltutorial.com/ - https://www.postgresql.org/docs/11/tutorial-sql.html Postgres jupyter kernel: https://github.com/bgschiller/postgres_kernel pgspecial must be installed for running special command postgres command in this notebook %pip install pgspecial Use sqlmagic and set sqlmagic compatible uri % load_ext sql The sql extension is already loaded. To reload it, use: %reload_ext sql Initialisation \u00b6 uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com\" % sql { uri } 'Connected: postgres@None' SQL for table deletion and creation % sql select current_database () * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. current_database postgres %% sql SELECT * FROM pg_tables WHERE schemaname NOT IN ( 'pg_catalog' , 'information_schema' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False False False %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; * postgresql+psycopg2://postgres: * @db.postgres.app.com Done. Done. [] Create table \u00b6 Primary key , auto-increment (see post ): - usage of the SERIAL or BIGSERIAL data types when CREATING a new table. - creating a custom SEQUENCE %% sql CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , person_first varchar ( 255 ), person_last varchar ( 255 ), country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ), CON ); * postgresql+psycopg2://postgres: * @db.postgres.app.com Done. (psycopg2.errors.SyntaxError) syntax error at or near \")\" LINE 8: ); ^ [SQL: CREATE TABLE Person ( person_id SERIAL PRIMARY KEY, person_first varchar(255), person_last varchar(255), country_id varchar(2) NOT NULL, FOREIGN KEY (country_id) REFERENCES Country(country_id), CON );] (Background on this error at: http://sqlalche.me/e/f405) View table list, either with postgres special command \\dt or from pg_catalog %% sql \\ dt * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. Schema Name Type Owner public country table postgres %% sql DESCRIBE accounts ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.SyntaxError) syntax error at or near \"DESCRIBE\" LINE 1: DESCRIBE accounts; ^ [SQL: DESCRIBE accounts;] (Background on this error at: http://sqlalche.me/e/f405) %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema' ; * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False False False Insert rows \u00b6 %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com 2 rows affected. [] The pg_relation_size() function returns the size of the table only, not included indexes or additional objects. %% sql SELECT pg_size_pretty ( pg_relation_size ( 'Country' )); * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. pg_size_pretty 8192 bytes %% sql INSERT INTO Person ( person_first , person_last , country_id ) VALUES ( 'Napolean' , 'Bonaparte' , 'FR' ), ( 'Luis' , 'Alvarez' , 'CU' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.UndefinedTable) relation \"person\" does not exist LINE 1: INSERT INTO Person (person_first, person_last, country_id) ^ [SQL: INSERT INTO Person (person_first, person_last, country_id) VALUES ('Napolean', 'Bonaparte', 'FR'), ('Luis','Alvarez', 'CU');] (Background on this error at: http://sqlalche.me/e/f405) Accessing the RDBMS dictionary. %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' ; * postgresql+psycopg2://postgres: * @db.postgres.app.com 8 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False False False information_schema sql_features postgres None False False False False information_schema sql_implementation_info postgres None False False False False information_schema sql_languages postgres None False False False False information_schema sql_packages postgres None False False False False information_schema sql_parts postgres None False False False False information_schema sql_sizing postgres None False False False False information_schema sql_sizing_profiles postgres None False False False False %% sql SELECT sql FROM postgres WHERE name = 'Person' ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.UndefinedTable) relation \"postgres\" does not exist LINE 1: SELECT sql FROM postgres ^ [SQL: SELECT sql FROM postgres WHERE name='Person';] (Background on this error at: http://sqlalche.me/e/f405) SQL as a Query Language. %% sql SELECT person_first as first , person_last AS last , country_name AS nationality FROM Person INNER JOIN country ON Person . country_id = Country . country_id ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.UndefinedTable) relation \"person\" does not exist LINE 2: FROM Person ^ [SQL: SELECT person_first as first, person_last AS last, country_name AS nationality FROM Person INNER JOIN country ON Person.country_id = Country.country_id;] (Background on this error at: http://sqlalche.me/e/f405) Visualizing the entitry-relationship diagram (ERd). import ibis import eralchemy import os from eralchemy import render_er if not os.path.exists('erd_from_sqlalchemy.png'): render_er(uri, 'erd_from_sqlalchemy.png') Homework walk-through \u00b6 Convert the flat file data in data/flat.csv into a well-structured relational database in SQLite3 stored as data/faculty.db . Note - salary information is confidential and should be kept in a separate table from other personal data. import pandas as pd flat = pd . read_csv ( '../data/flat.csv' , keep_default_na = False ) flat . sample ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name gender age height weight salary nationality code country language1 language2 language3 first last 915 Lynwood Pope Male 20 1.51 40 88000 Jordanian JO Jordan ASP Scala Lynwood Pope 1 Aaron Kirby Male 59 1.69 43 80000 Spanish SP Spain Falcon haXe GNU Octave Aaron Kirby 239 Clarita Carver Female 35 1.66 71 84000 Finnish FI Finland Prolog Erlang Smalltalk Clarita Carver %% sql USE faculty ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.SyntaxError) syntax error at or near \"USE\" LINE 1: USE faculty; ^ [SQL: USE faculty;] (Background on this error at: http://sqlalche.me/e/f405) %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , name varchar ( 255 ), age INTEGER NOT NULL , country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ) ); * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres Done. Done. Done. Done. [] %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres 2 rows affected. [] %% sql DELETE FROM Country * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres 2 rows affected. [] %% sql SELECT * FROM Country * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres 0 rows affected. country_id country_name from sqlalchemy import create_engine engine = create_engine ( uri ) conn = engine . connect () flat . columns Index(['name', 'gender', 'age', 'height', 'weight', 'salary', 'nationality', 'code', 'country', 'language1', 'language2', 'language3', 'first', 'last'], dtype='object') flat . rename ( mapper = { 'code' : 'country_id' , 'country' : 'country_name' }, inplace = True ) country = flat [[ 'country_id' , 'country_name' ]] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-31-f9749b54edd5> in <module> ----> 1 country = flat [ [ 'country_id' , 'country_name' ] ] /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__ (self, key) 2932 key = list ( key ) 2933 indexer = self.loc._convert_to_indexer(key, axis=1, -> 2934 raise_missing=True) 2935 2936 # take() does not accept boolean indexers /opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in _convert_to_indexer (self, obj, axis, is_setter, raise_missing) 1352 kwargs = {'raise_missing': True if is_setter else 1353 raise_missing} -> 1354 return self . _get_listlike_indexer ( obj , axis , ** kwargs ) [ 1 ] 1355 else : 1356 try : /opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in _get_listlike_indexer (self, key, axis, raise_missing) 1159 self._validate_read_indexer(keyarr, indexer, 1160 o . _get_axis_number ( axis ) , -> 1161 raise_missing=raise_missing) 1162 return keyarr , indexer 1163 /opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in _validate_read_indexer (self, key, indexer, axis, raise_missing) 1244 raise KeyError( 1245 u\"None of [{key}] are in the [{axis}]\".format( -> 1246 key=key, axis=self.obj._get_axis_name(axis))) 1247 1248 # We (temporarily) allow for some missing keys with .loc, except in KeyError : \"None of [Index(['country_id', 'country_name'], dtype='object')] are in the [columns]\" country . set_index ( 'country_id' ) . to_sql ( 'Country' , engine , if_exists = 'append' ) --------------------------------------------------------------------------- UndefinedColumn Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1223 self.dialect.do_executemany( -> 1224 cursor , statement , parameters , context 1225 ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py in do_executemany (self, cursor, statement, parameters, context) 751 else : --> 752 cursor . executemany ( statement , parameters ) 753 UndefinedColumn : column \"country_id\" of relation \"Country\" does not exist LINE 1: INSERT INTO \"Country\" (country_id, country_name) VALUES ('GB... ^ The above exception was the direct cause of the following exception: ProgrammingError Traceback (most recent call last) <ipython-input-103-42976342ffd9> in <module> ----> 1 country . set_index ( 'country_id' ) . to_sql ( 'Country' , engine , if_exists = 'append' ) /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py in to_sql (self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method) 2529 sql.to_sql(self, name, con, schema=schema, if_exists=if_exists, 2530 index = index , index_label = index_label , chunksize = chunksize , -> 2531 dtype=dtype, method=method) 2532 2533 def to_pickle(self, path, compression='infer', /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in to_sql (frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method) 458 pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index, 459 index_label = index_label , schema = schema , --> 460 chunksize=chunksize, dtype=dtype, method=method) 461 462 /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in to_sql (self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method) 1172 schema=schema, dtype=dtype) 1173 table . create ( ) -> 1174 table . insert ( chunksize , method = method ) 1175 if ( not name . isdigit ( ) and not name . islower ( ) ) : 1176 # check for potentially case sensitivity issues (GH7815) /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in insert (self, chunksize, method) 684 685 chunk_iter = zip ( * [ arr [ start_i : end_i ] for arr in data_list ] ) --> 686 exec_insert ( conn , keys , chunk_iter ) 687 688 def _query_iterator(self, result, chunksize, columns, coerce_float=True, /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in _execute_insert (self, conn, keys, data_iter) 597 \"\"\" 598 data = [ dict ( zip ( keys , row ) ) for row in data_iter ] --> 599 conn . execute ( self . table . insert ( ) , data ) 600 601 def _execute_insert_multi ( self , conn , keys , data_iter ) : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in execute (self, object_, *multiparams, **params) 986 raise exc . ObjectNotExecutableError ( object_ ) 987 else : --> 988 return meth ( self , multiparams , params ) 989 990 def _execute_function ( self , func , multiparams , params ) : /opt/conda/lib/python3.7/site-packages/sqlalchemy/sql/elements.py in _execute_on_connection (self, connection, multiparams, params) 285 def _execute_on_connection ( self , connection , multiparams , params ) : 286 if self . supports_execution : --> 287 return connection . _execute_clauseelement ( self , multiparams , params ) 288 else : 289 raise exc . ObjectNotExecutableError ( self ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_clauseelement (self, elem, multiparams, params) 1105 distilled_params , 1106 compiled_sql , -> 1107 distilled_params , 1108 ) 1109 if self . _has_events or self . engine . _has_events : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1246 except BaseException as e : 1247 self._handle_dbapi_exception( -> 1248 e , statement , parameters , cursor , context 1249 ) 1250 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception (self, e, statement, parameters, cursor, context) 1464 util . raise_from_cause ( newraise , exc_info ) 1465 elif should_wrap : -> 1466 util . raise_from_cause ( sqlalchemy_exception , exc_info ) 1467 else : 1468 util . reraise ( * exc_info ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in raise_from_cause (exception, exc_info) 381 exc_type , exc_value , exc_tb = exc_info 382 cause = exc_value if exc_value is not exception else None --> 383 reraise ( type ( exception ) , exception , tb = exc_tb , cause = cause ) 384 385 /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in reraise (tp, value, tb, cause) 126 value . __cause__ = cause 127 if value . __traceback__ is not tb : --> 128 raise value . with_traceback ( tb ) 129 raise value 130 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1222 if not evt_handled : 1223 self.dialect.do_executemany( -> 1224 cursor , statement , parameters , context 1225 ) 1226 elif not parameters and context . no_parameters : /opt/conda/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py in do_executemany (self, cursor, statement, parameters, context) 750 extras . execute_batch ( cursor , statement , parameters ) 751 else : --> 752 cursor . executemany ( statement , parameters ) 753 754 @ util . memoized_instancemethod ProgrammingError : (psycopg2.errors.UndefinedColumn) column \"country_id\" of relation \"Country\" does not exist LINE 1: INSERT INTO \"Country\" (country_id, country_name) VALUES ('GB... ^ [SQL: INSERT INTO \"Country\" (country_id, country_name) VALUES (%(country_id)s, %(country_name)s)] [parameters: ({'country_id': 'GB', 'country_name': 'United Kingdom'}, {'country_id': 'SP', 'country_name': 'Spain'}, {'country_id': 'IT', 'country_name': 'Italy'}, {'country_id': 'IT', 'country_name': 'Italy'}, {'country_id': 'UY', 'country_name': 'Uruguay'}, {'country_id': 'CM', 'country_name': 'Cambodia'}, {'country_id': 'CM', 'country_name': 'Cameroon'}, {'country_id': 'BE', 'country_name': 'Belgium'} ... displaying 10 of 1523 total bound parameter sets ... {'country_id': 'ET', 'country_name': 'Ethiopia'}, {'country_id': 'VE', 'country_name': 'Venezuela'})] (Background on this error at: http://sqlalche.me/e/f405) %%sql \u00b6 SELECT * FROM Country flat . to_sql ? Signature: flat . to_sql ( name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None , ) Docstring: Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : string Name of SQL table. con : sqlalchemy.engine.Engine or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. schema : string, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : string or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Rows will be written in batches of this size at a time. By default, all rows will be written at once. dtype : dict, optional Specifying the datatype for columns. The keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. method : {None, 'multi', callable}, default None Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] http://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) >>> df1.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5')] Overwrite the table with just ``df1``. >>> df1.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 4'), (1, 'User 5')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\"A\": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) >>> engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] File: /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py Type: method import asyncio async def coro ( int ): print ( f \"running { int } \" ) await asyncio . sleep ( 1 ) print ( f \"continuing { int } \" ) return int asyncio . run ( coro ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-109-550412da50e5> in <module> ----> 1 asyncio . run ( coro ) /opt/conda/lib/python3.7/asyncio/runners.py in run (main, debug) 32 if events . _get_running_loop ( ) is not None : 33 raise RuntimeError( ---> 34 \"asyncio.run() cannot be called from a running event loop\") 35 36 if not coroutines . iscoroutine ( main ) : RuntimeError : asyncio.run() cannot be called from a running event loop","title":"03 Postgres"},{"location":"_nb/%3DPy/02_SQL/03_Postgres/#initialisation","text":"uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com\" % sql { uri } 'Connected: postgres@None' SQL for table deletion and creation % sql select current_database () * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. current_database postgres %% sql SELECT * FROM pg_tables WHERE schemaname NOT IN ( 'pg_catalog' , 'information_schema' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False False False %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; * postgresql+psycopg2://postgres: * @db.postgres.app.com Done. Done. []","title":"Initialisation"},{"location":"_nb/%3DPy/02_SQL/03_Postgres/#create-table","text":"Primary key , auto-increment (see post ): - usage of the SERIAL or BIGSERIAL data types when CREATING a new table. - creating a custom SEQUENCE %% sql CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , person_first varchar ( 255 ), person_last varchar ( 255 ), country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ), CON ); * postgresql+psycopg2://postgres: * @db.postgres.app.com Done. (psycopg2.errors.SyntaxError) syntax error at or near \")\" LINE 8: ); ^ [SQL: CREATE TABLE Person ( person_id SERIAL PRIMARY KEY, person_first varchar(255), person_last varchar(255), country_id varchar(2) NOT NULL, FOREIGN KEY (country_id) REFERENCES Country(country_id), CON );] (Background on this error at: http://sqlalche.me/e/f405) View table list, either with postgres special command \\dt or from pg_catalog %% sql \\ dt * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. Schema Name Type Owner public country table postgres %% sql DESCRIBE accounts ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.SyntaxError) syntax error at or near \"DESCRIBE\" LINE 1: DESCRIBE accounts; ^ [SQL: DESCRIBE accounts;] (Background on this error at: http://sqlalche.me/e/f405) %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema' ; * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False False False","title":"Create table"},{"location":"_nb/%3DPy/02_SQL/03_Postgres/#insert-rows","text":"%% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com 2 rows affected. [] The pg_relation_size() function returns the size of the table only, not included indexes or additional objects. %% sql SELECT pg_size_pretty ( pg_relation_size ( 'Country' )); * postgresql+psycopg2://postgres: * @db.postgres.app.com 1 rows affected. pg_size_pretty 8192 bytes %% sql INSERT INTO Person ( person_first , person_last , country_id ) VALUES ( 'Napolean' , 'Bonaparte' , 'FR' ), ( 'Luis' , 'Alvarez' , 'CU' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.UndefinedTable) relation \"person\" does not exist LINE 1: INSERT INTO Person (person_first, person_last, country_id) ^ [SQL: INSERT INTO Person (person_first, person_last, country_id) VALUES ('Napolean', 'Bonaparte', 'FR'), ('Luis','Alvarez', 'CU');] (Background on this error at: http://sqlalche.me/e/f405) Accessing the RDBMS dictionary. %% sql SELECT * FROM pg_catalog . pg_tables WHERE schemaname != 'pg_catalog' ; * postgresql+psycopg2://postgres: * @db.postgres.app.com 8 rows affected. schemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity public country postgres None True False False False information_schema sql_features postgres None False False False False information_schema sql_implementation_info postgres None False False False False information_schema sql_languages postgres None False False False False information_schema sql_packages postgres None False False False False information_schema sql_parts postgres None False False False False information_schema sql_sizing postgres None False False False False information_schema sql_sizing_profiles postgres None False False False False %% sql SELECT sql FROM postgres WHERE name = 'Person' ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.UndefinedTable) relation \"postgres\" does not exist LINE 1: SELECT sql FROM postgres ^ [SQL: SELECT sql FROM postgres WHERE name='Person';] (Background on this error at: http://sqlalche.me/e/f405) SQL as a Query Language. %% sql SELECT person_first as first , person_last AS last , country_name AS nationality FROM Person INNER JOIN country ON Person . country_id = Country . country_id ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.UndefinedTable) relation \"person\" does not exist LINE 2: FROM Person ^ [SQL: SELECT person_first as first, person_last AS last, country_name AS nationality FROM Person INNER JOIN country ON Person.country_id = Country.country_id;] (Background on this error at: http://sqlalche.me/e/f405) Visualizing the entitry-relationship diagram (ERd). import ibis import eralchemy import os from eralchemy import render_er if not os.path.exists('erd_from_sqlalchemy.png'): render_er(uri, 'erd_from_sqlalchemy.png')","title":"Insert rows"},{"location":"_nb/%3DPy/02_SQL/03_Postgres/#homework-walk-through","text":"Convert the flat file data in data/flat.csv into a well-structured relational database in SQLite3 stored as data/faculty.db . Note - salary information is confidential and should be kept in a separate table from other personal data. import pandas as pd flat = pd . read_csv ( '../data/flat.csv' , keep_default_na = False ) flat . sample ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name gender age height weight salary nationality code country language1 language2 language3 first last 915 Lynwood Pope Male 20 1.51 40 88000 Jordanian JO Jordan ASP Scala Lynwood Pope 1 Aaron Kirby Male 59 1.69 43 80000 Spanish SP Spain Falcon haXe GNU Octave Aaron Kirby 239 Clarita Carver Female 35 1.66 71 84000 Finnish FI Finland Prolog Erlang Smalltalk Clarita Carver %% sql USE faculty ; * postgresql+psycopg2://postgres: * @db.postgres.app.com (psycopg2.errors.SyntaxError) syntax error at or near \"USE\" LINE 1: USE faculty; ^ [SQL: USE faculty;] (Background on this error at: http://sqlalche.me/e/f405) %% sql DROP TABLE IF EXISTS Person ; DROP TABLE IF EXISTS Country ; CREATE TABLE Country ( country_id varchar ( 2 ) PRIMARY KEY , country_name varchar ( 255 ) ); CREATE TABLE Person ( person_id SERIAL PRIMARY KEY , name varchar ( 255 ), age INTEGER NOT NULL , country_id varchar ( 2 ) NOT NULL , FOREIGN KEY ( country_id ) REFERENCES Country ( country_id ) ); * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres Done. Done. Done. Done. [] %% sql INSERT INTO Country ( country_id , country_name ) VALUES ( 'FR' , 'France' ), ( 'CU' , 'CUBA' ); * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres 2 rows affected. [] %% sql DELETE FROM Country * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres 2 rows affected. [] %% sql SELECT * FROM Country * postgresql+psycopg2://postgres: * @db.postgres.app.com/postgres 0 rows affected. country_id country_name from sqlalchemy import create_engine engine = create_engine ( uri ) conn = engine . connect () flat . columns Index(['name', 'gender', 'age', 'height', 'weight', 'salary', 'nationality', 'code', 'country', 'language1', 'language2', 'language3', 'first', 'last'], dtype='object') flat . rename ( mapper = { 'code' : 'country_id' , 'country' : 'country_name' }, inplace = True ) country = flat [[ 'country_id' , 'country_name' ]] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-31-f9749b54edd5> in <module> ----> 1 country = flat [ [ 'country_id' , 'country_name' ] ] /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__ (self, key) 2932 key = list ( key ) 2933 indexer = self.loc._convert_to_indexer(key, axis=1, -> 2934 raise_missing=True) 2935 2936 # take() does not accept boolean indexers /opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in _convert_to_indexer (self, obj, axis, is_setter, raise_missing) 1352 kwargs = {'raise_missing': True if is_setter else 1353 raise_missing} -> 1354 return self . _get_listlike_indexer ( obj , axis , ** kwargs ) [ 1 ] 1355 else : 1356 try : /opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in _get_listlike_indexer (self, key, axis, raise_missing) 1159 self._validate_read_indexer(keyarr, indexer, 1160 o . _get_axis_number ( axis ) , -> 1161 raise_missing=raise_missing) 1162 return keyarr , indexer 1163 /opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py in _validate_read_indexer (self, key, indexer, axis, raise_missing) 1244 raise KeyError( 1245 u\"None of [{key}] are in the [{axis}]\".format( -> 1246 key=key, axis=self.obj._get_axis_name(axis))) 1247 1248 # We (temporarily) allow for some missing keys with .loc, except in KeyError : \"None of [Index(['country_id', 'country_name'], dtype='object')] are in the [columns]\" country . set_index ( 'country_id' ) . to_sql ( 'Country' , engine , if_exists = 'append' ) --------------------------------------------------------------------------- UndefinedColumn Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1223 self.dialect.do_executemany( -> 1224 cursor , statement , parameters , context 1225 ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py in do_executemany (self, cursor, statement, parameters, context) 751 else : --> 752 cursor . executemany ( statement , parameters ) 753 UndefinedColumn : column \"country_id\" of relation \"Country\" does not exist LINE 1: INSERT INTO \"Country\" (country_id, country_name) VALUES ('GB... ^ The above exception was the direct cause of the following exception: ProgrammingError Traceback (most recent call last) <ipython-input-103-42976342ffd9> in <module> ----> 1 country . set_index ( 'country_id' ) . to_sql ( 'Country' , engine , if_exists = 'append' ) /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py in to_sql (self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method) 2529 sql.to_sql(self, name, con, schema=schema, if_exists=if_exists, 2530 index = index , index_label = index_label , chunksize = chunksize , -> 2531 dtype=dtype, method=method) 2532 2533 def to_pickle(self, path, compression='infer', /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in to_sql (frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method) 458 pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index, 459 index_label = index_label , schema = schema , --> 460 chunksize=chunksize, dtype=dtype, method=method) 461 462 /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in to_sql (self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method) 1172 schema=schema, dtype=dtype) 1173 table . create ( ) -> 1174 table . insert ( chunksize , method = method ) 1175 if ( not name . isdigit ( ) and not name . islower ( ) ) : 1176 # check for potentially case sensitivity issues (GH7815) /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in insert (self, chunksize, method) 684 685 chunk_iter = zip ( * [ arr [ start_i : end_i ] for arr in data_list ] ) --> 686 exec_insert ( conn , keys , chunk_iter ) 687 688 def _query_iterator(self, result, chunksize, columns, coerce_float=True, /opt/conda/lib/python3.7/site-packages/pandas/io/sql.py in _execute_insert (self, conn, keys, data_iter) 597 \"\"\" 598 data = [ dict ( zip ( keys , row ) ) for row in data_iter ] --> 599 conn . execute ( self . table . insert ( ) , data ) 600 601 def _execute_insert_multi ( self , conn , keys , data_iter ) : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in execute (self, object_, *multiparams, **params) 986 raise exc . ObjectNotExecutableError ( object_ ) 987 else : --> 988 return meth ( self , multiparams , params ) 989 990 def _execute_function ( self , func , multiparams , params ) : /opt/conda/lib/python3.7/site-packages/sqlalchemy/sql/elements.py in _execute_on_connection (self, connection, multiparams, params) 285 def _execute_on_connection ( self , connection , multiparams , params ) : 286 if self . supports_execution : --> 287 return connection . _execute_clauseelement ( self , multiparams , params ) 288 else : 289 raise exc . ObjectNotExecutableError ( self ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_clauseelement (self, elem, multiparams, params) 1105 distilled_params , 1106 compiled_sql , -> 1107 distilled_params , 1108 ) 1109 if self . _has_events or self . engine . _has_events : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1246 except BaseException as e : 1247 self._handle_dbapi_exception( -> 1248 e , statement , parameters , cursor , context 1249 ) 1250 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception (self, e, statement, parameters, cursor, context) 1464 util . raise_from_cause ( newraise , exc_info ) 1465 elif should_wrap : -> 1466 util . raise_from_cause ( sqlalchemy_exception , exc_info ) 1467 else : 1468 util . reraise ( * exc_info ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in raise_from_cause (exception, exc_info) 381 exc_type , exc_value , exc_tb = exc_info 382 cause = exc_value if exc_value is not exception else None --> 383 reraise ( type ( exception ) , exception , tb = exc_tb , cause = cause ) 384 385 /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in reraise (tp, value, tb, cause) 126 value . __cause__ = cause 127 if value . __traceback__ is not tb : --> 128 raise value . with_traceback ( tb ) 129 raise value 130 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1222 if not evt_handled : 1223 self.dialect.do_executemany( -> 1224 cursor , statement , parameters , context 1225 ) 1226 elif not parameters and context . no_parameters : /opt/conda/lib/python3.7/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py in do_executemany (self, cursor, statement, parameters, context) 750 extras . execute_batch ( cursor , statement , parameters ) 751 else : --> 752 cursor . executemany ( statement , parameters ) 753 754 @ util . memoized_instancemethod ProgrammingError : (psycopg2.errors.UndefinedColumn) column \"country_id\" of relation \"Country\" does not exist LINE 1: INSERT INTO \"Country\" (country_id, country_name) VALUES ('GB... ^ [SQL: INSERT INTO \"Country\" (country_id, country_name) VALUES (%(country_id)s, %(country_name)s)] [parameters: ({'country_id': 'GB', 'country_name': 'United Kingdom'}, {'country_id': 'SP', 'country_name': 'Spain'}, {'country_id': 'IT', 'country_name': 'Italy'}, {'country_id': 'IT', 'country_name': 'Italy'}, {'country_id': 'UY', 'country_name': 'Uruguay'}, {'country_id': 'CM', 'country_name': 'Cambodia'}, {'country_id': 'CM', 'country_name': 'Cameroon'}, {'country_id': 'BE', 'country_name': 'Belgium'} ... displaying 10 of 1523 total bound parameter sets ... {'country_id': 'ET', 'country_name': 'Ethiopia'}, {'country_id': 'VE', 'country_name': 'Venezuela'})] (Background on this error at: http://sqlalche.me/e/f405)","title":"Homework walk-through"},{"location":"_nb/%3DPy/02_SQL/03_Postgres/#sql","text":"SELECT * FROM Country flat . to_sql ? Signature: flat . to_sql ( name , con , schema = None , if_exists = 'fail' , index = True , index_label = None , chunksize = None , dtype = None , method = None , ) Docstring: Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : string Name of SQL table. con : sqlalchemy.engine.Engine or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. schema : string, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : string or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Rows will be written in batches of this size at a time. By default, all rows will be written at once. dtype : dict, optional Specifying the datatype for columns. The keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. method : {None, 'multi', callable}, default None Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] http://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) >>> df1.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5')] Overwrite the table with just ``df1``. >>> df1.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 4'), (1, 'User 5')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\"A\": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) >>> engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] File: /opt/conda/lib/python3.7/site-packages/pandas/core/generic.py Type: method import asyncio async def coro ( int ): print ( f \"running { int } \" ) await asyncio . sleep ( 1 ) print ( f \"continuing { int } \" ) return int asyncio . run ( coro ) --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) <ipython-input-109-550412da50e5> in <module> ----> 1 asyncio . run ( coro ) /opt/conda/lib/python3.7/asyncio/runners.py in run (main, debug) 32 if events . _get_running_loop ( ) is not None : 33 raise RuntimeError( ---> 34 \"asyncio.run() cannot be called from a running event loop\") 35 36 if not coroutines . iscoroutine ( main ) : RuntimeError : asyncio.run() cannot be called from a running event loop","title":"%%sql"},{"location":"_nb/%3DPy/02_SQL/03_Postgres_tutorial/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext sql % env DATABASE_URL = postgresql + psycopg2 : // postgres : postgres @db . postgres . app . com uri = % env DATABASE_URL env: DATABASE_URL=postgresql+psycopg2://postgres:postgres@db.postgres.app.com or uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com\" Create tables \u00b6 %% sql \\ d 6 rows affected. Schema Name Type Owner public accounts table postgres public flat table postgres public t_cities table postgres public t_cities_id_seq sequence postgres public t_weather_observations table postgres public t_weather_observations_id_seq sequence postgres %% sql DROP TABLE IF EXISTS t_weather_observations , t_cities ; CREATE TABLE t_cities ( name varchar ( 80 ) PRIMARY KEY , location point ); CREATE TABLE t_weather_observations ( id SERIAL PRIMARY KEY , city varchar ( 80 ), temp_lo int , -- low temperature temp_hi int , -- high temperature prcp real , -- precipitation date date , FOREIGN KEY ( city ) REFERENCES t_cities ( name ) -- or city varchar ( 80 ) REFERENCES cities ( name ), ); Done. Done. Done. [] Populate a table with a row \u00b6 %% sql INSERT INTO t_cities ( name , location ) VALUES ( 'San Francisco' , '(-194.2, 53.0)' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com 1 rows affected. [] % sql SELECT * from Cities ; * postgresql+psycopg2://postgres:***@db.postgres.app.com 1 rows affected. name location San Francisco (-194.2,53) %% sql INSERT INTO weather ( city , temp_lo , temp_hi , prcp , date ) VALUES ( 'San Francisco' , 43 , 57 , 0.0 , '1994-11-29' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com 1 rows affected. [] % sql SELECT * from weather ; * postgresql+psycopg2://postgres:***@db.postgres.app.com 2 rows affected. id city temp_lo temp_hi prcp date 1 San Francisco 43 57 0.0 1994-11-29 3 San Francisco 43 57 0.0 1994-11-29 %% sql INSERT INTO weather ( date , city , temp_hi , temp_lo ) VALUES ( '1994-11-29' , 'Hayward' , 54 , 37 ); * postgresql+psycopg2://postgres:***@db.postgres.app.com --------------------------------------------------------------------------- ForeignKeyViolation Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1243 self.dialect.do_execute( -> 1244 cursor , statement , parameters , context 1245 ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/default.py in do_execute (self, cursor, statement, parameters, context) 549 def do_execute ( self , cursor , statement , parameters , context = None ) : --> 550 cursor . execute ( statement , parameters ) 551 ForeignKeyViolation : insert or update on table \"weather\" violates foreign key constraint \"weather_city_fkey\" DETAIL: Key (city)=(Hayward) is not present in table \"cities\". The above exception was the direct cause of the following exception: IntegrityError Traceback (most recent call last) <ipython-input-93-3bccff404bf3> in <module> ----> 1 get_ipython ( ) . run_cell_magic ( 'sql' , '' , \"\\nINSERT INTO weather (date, city, temp_hi, temp_lo)\\nVALUES ('1994-11-29', 'Hayward', 54, 37);\\n\" ) /opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_cell_magic (self, magic_name, line, cell) 2356 with self . builtin_trap : 2357 args = ( magic_arg_s , cell ) -> 2358 result = fn ( * args , ** kwargs ) 2359 return result 2360 </opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-157> in execute (self, line, cell, local_ns) /opt/conda/lib/python3.7/site-packages/IPython/core/magic.py in <lambda> (f, *a, **k) 185 # but it's overkill for just that one bit of state. 186 def magic_deco ( arg ) : --> 187 call = lambda f , * a , ** k : f ( * a , ** k ) 188 189 if callable ( arg ) : </opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-156> in execute (self, line, cell, local_ns) /opt/conda/lib/python3.7/site-packages/IPython/core/magic.py in <lambda> (f, *a, **k) 185 # but it's overkill for just that one bit of state. 186 def magic_deco ( arg ) : --> 187 call = lambda f , * a , ** k : f ( * a , ** k ) 188 189 if callable ( arg ) : /opt/conda/lib/python3.7/site-packages/sql/magic.py in execute (self, line, cell, local_ns) 93 94 try : ---> 95 result = sql . run . run ( conn , parsed [ 'sql' ] , self , user_ns ) 96 97 if result is not None and not isinstance ( result , str ) and self . column_local_vars : /opt/conda/lib/python3.7/site-packages/sql/run.py in run (conn, sql, config, user_namespace) 338 else : 339 txt = sqlalchemy . sql . text ( statement ) --> 340 result = conn . session . execute ( txt , user_namespace ) 341 _commit ( conn = conn , config = config ) 342 if result and config . feedback : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in execute (self, object_, *multiparams, **params) 986 raise exc . ObjectNotExecutableError ( object_ ) 987 else : --> 988 return meth ( self , multiparams , params ) 989 990 def _execute_function ( self , func , multiparams , params ) : /opt/conda/lib/python3.7/site-packages/sqlalchemy/sql/elements.py in _execute_on_connection (self, connection, multiparams, params) 285 def _execute_on_connection ( self , connection , multiparams , params ) : 286 if self . supports_execution : --> 287 return connection . _execute_clauseelement ( self , multiparams , params ) 288 else : 289 raise exc . ObjectNotExecutableError ( self ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_clauseelement (self, elem, multiparams, params) 1105 distilled_params , 1106 compiled_sql , -> 1107 distilled_params , 1108 ) 1109 if self . _has_events or self . engine . _has_events : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1246 except BaseException as e : 1247 self._handle_dbapi_exception( -> 1248 e , statement , parameters , cursor , context 1249 ) 1250 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception (self, e, statement, parameters, cursor, context) 1464 util . raise_from_cause ( newraise , exc_info ) 1465 elif should_wrap : -> 1466 util . raise_from_cause ( sqlalchemy_exception , exc_info ) 1467 else : 1468 util . reraise ( * exc_info ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in raise_from_cause (exception, exc_info) 397 exc_type , exc_value , exc_tb = exc_info 398 cause = exc_value if exc_value is not exception else None --> 399 reraise ( type ( exception ) , exception , tb = exc_tb , cause = cause ) 400 401 /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in reraise (tp, value, tb, cause) 151 value . __cause__ = cause 152 if value . __traceback__ is not tb : --> 153 raise value . with_traceback ( tb ) 154 raise value 155 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1242 if not evt_handled : 1243 self.dialect.do_execute( -> 1244 cursor , statement , parameters , context 1245 ) 1246 except BaseException as e : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/default.py in do_execute (self, cursor, statement, parameters, context) 548 549 def do_execute ( self , cursor , statement , parameters , context = None ) : --> 550 cursor . execute ( statement , parameters ) 551 552 def do_execute_no_params ( self , cursor , statement , context = None ) : IntegrityError : (psycopg2.errors.ForeignKeyViolation) insert or update on table \"weather\" violates foreign key constraint \"weather_city_fkey\" DETAIL: Key (city)=(Hayward) is not present in table \"cities\". [SQL: INSERT INTO weather (date, city, temp_hi, temp_lo) VALUES ('1994-11-29', 'Hayward', 54, 37);] (Background on this error at: http://sqlalche.me/e/gkpj)","title":"03 Postgres tutorial"},{"location":"_nb/%3DPy/02_SQL/03_Postgres_tutorial/#create-tables","text":"%% sql \\ d 6 rows affected. Schema Name Type Owner public accounts table postgres public flat table postgres public t_cities table postgres public t_cities_id_seq sequence postgres public t_weather_observations table postgres public t_weather_observations_id_seq sequence postgres %% sql DROP TABLE IF EXISTS t_weather_observations , t_cities ; CREATE TABLE t_cities ( name varchar ( 80 ) PRIMARY KEY , location point ); CREATE TABLE t_weather_observations ( id SERIAL PRIMARY KEY , city varchar ( 80 ), temp_lo int , -- low temperature temp_hi int , -- high temperature prcp real , -- precipitation date date , FOREIGN KEY ( city ) REFERENCES t_cities ( name ) -- or city varchar ( 80 ) REFERENCES cities ( name ), ); Done. Done. Done. []","title":"Create tables"},{"location":"_nb/%3DPy/02_SQL/03_Postgres_tutorial/#populate-a-table-with-a-row","text":"%% sql INSERT INTO t_cities ( name , location ) VALUES ( 'San Francisco' , '(-194.2, 53.0)' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com 1 rows affected. [] % sql SELECT * from Cities ; * postgresql+psycopg2://postgres:***@db.postgres.app.com 1 rows affected. name location San Francisco (-194.2,53) %% sql INSERT INTO weather ( city , temp_lo , temp_hi , prcp , date ) VALUES ( 'San Francisco' , 43 , 57 , 0.0 , '1994-11-29' ); * postgresql+psycopg2://postgres:***@db.postgres.app.com 1 rows affected. [] % sql SELECT * from weather ; * postgresql+psycopg2://postgres:***@db.postgres.app.com 2 rows affected. id city temp_lo temp_hi prcp date 1 San Francisco 43 57 0.0 1994-11-29 3 San Francisco 43 57 0.0 1994-11-29 %% sql INSERT INTO weather ( date , city , temp_hi , temp_lo ) VALUES ( '1994-11-29' , 'Hayward' , 54 , 37 ); * postgresql+psycopg2://postgres:***@db.postgres.app.com --------------------------------------------------------------------------- ForeignKeyViolation Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1243 self.dialect.do_execute( -> 1244 cursor , statement , parameters , context 1245 ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/default.py in do_execute (self, cursor, statement, parameters, context) 549 def do_execute ( self , cursor , statement , parameters , context = None ) : --> 550 cursor . execute ( statement , parameters ) 551 ForeignKeyViolation : insert or update on table \"weather\" violates foreign key constraint \"weather_city_fkey\" DETAIL: Key (city)=(Hayward) is not present in table \"cities\". The above exception was the direct cause of the following exception: IntegrityError Traceback (most recent call last) <ipython-input-93-3bccff404bf3> in <module> ----> 1 get_ipython ( ) . run_cell_magic ( 'sql' , '' , \"\\nINSERT INTO weather (date, city, temp_hi, temp_lo)\\nVALUES ('1994-11-29', 'Hayward', 54, 37);\\n\" ) /opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_cell_magic (self, magic_name, line, cell) 2356 with self . builtin_trap : 2357 args = ( magic_arg_s , cell ) -> 2358 result = fn ( * args , ** kwargs ) 2359 return result 2360 </opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-157> in execute (self, line, cell, local_ns) /opt/conda/lib/python3.7/site-packages/IPython/core/magic.py in <lambda> (f, *a, **k) 185 # but it's overkill for just that one bit of state. 186 def magic_deco ( arg ) : --> 187 call = lambda f , * a , ** k : f ( * a , ** k ) 188 189 if callable ( arg ) : </opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-156> in execute (self, line, cell, local_ns) /opt/conda/lib/python3.7/site-packages/IPython/core/magic.py in <lambda> (f, *a, **k) 185 # but it's overkill for just that one bit of state. 186 def magic_deco ( arg ) : --> 187 call = lambda f , * a , ** k : f ( * a , ** k ) 188 189 if callable ( arg ) : /opt/conda/lib/python3.7/site-packages/sql/magic.py in execute (self, line, cell, local_ns) 93 94 try : ---> 95 result = sql . run . run ( conn , parsed [ 'sql' ] , self , user_ns ) 96 97 if result is not None and not isinstance ( result , str ) and self . column_local_vars : /opt/conda/lib/python3.7/site-packages/sql/run.py in run (conn, sql, config, user_namespace) 338 else : 339 txt = sqlalchemy . sql . text ( statement ) --> 340 result = conn . session . execute ( txt , user_namespace ) 341 _commit ( conn = conn , config = config ) 342 if result and config . feedback : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in execute (self, object_, *multiparams, **params) 986 raise exc . ObjectNotExecutableError ( object_ ) 987 else : --> 988 return meth ( self , multiparams , params ) 989 990 def _execute_function ( self , func , multiparams , params ) : /opt/conda/lib/python3.7/site-packages/sqlalchemy/sql/elements.py in _execute_on_connection (self, connection, multiparams, params) 285 def _execute_on_connection ( self , connection , multiparams , params ) : 286 if self . supports_execution : --> 287 return connection . _execute_clauseelement ( self , multiparams , params ) 288 else : 289 raise exc . ObjectNotExecutableError ( self ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_clauseelement (self, elem, multiparams, params) 1105 distilled_params , 1106 compiled_sql , -> 1107 distilled_params , 1108 ) 1109 if self . _has_events or self . engine . _has_events : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1246 except BaseException as e : 1247 self._handle_dbapi_exception( -> 1248 e , statement , parameters , cursor , context 1249 ) 1250 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception (self, e, statement, parameters, cursor, context) 1464 util . raise_from_cause ( newraise , exc_info ) 1465 elif should_wrap : -> 1466 util . raise_from_cause ( sqlalchemy_exception , exc_info ) 1467 else : 1468 util . reraise ( * exc_info ) /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in raise_from_cause (exception, exc_info) 397 exc_type , exc_value , exc_tb = exc_info 398 cause = exc_value if exc_value is not exception else None --> 399 reraise ( type ( exception ) , exception , tb = exc_tb , cause = cause ) 400 401 /opt/conda/lib/python3.7/site-packages/sqlalchemy/util/compat.py in reraise (tp, value, tb, cause) 151 value . __cause__ = cause 152 if value . __traceback__ is not tb : --> 153 raise value . with_traceback ( tb ) 154 raise value 155 /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context (self, dialect, constructor, statement, parameters, *args) 1242 if not evt_handled : 1243 self.dialect.do_execute( -> 1244 cursor , statement , parameters , context 1245 ) 1246 except BaseException as e : /opt/conda/lib/python3.7/site-packages/sqlalchemy/engine/default.py in do_execute (self, cursor, statement, parameters, context) 548 549 def do_execute ( self , cursor , statement , parameters , context = None ) : --> 550 cursor . execute ( statement , parameters ) 551 552 def do_execute_no_params ( self , cursor , statement , context = None ) : IntegrityError : (psycopg2.errors.ForeignKeyViolation) insert or update on table \"weather\" violates foreign key constraint \"weather_city_fkey\" DETAIL: Key (city)=(Hayward) is not present in table \"cities\". [SQL: INSERT INTO weather (date, city, temp_hi, temp_lo) VALUES ('1994-11-29', 'Hayward', 54, 37);] (Background on this error at: http://sqlalche.me/e/gkpj)","title":"Populate a table with a row"},{"location":"_nb/%3DPy/02_SQL/04_ibis/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Initialisation \u00b6 import ibis uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com/postgres\" con = ibis . postgres . connect ( url = uri ) con . list_tables () ['country'] ibis . options {'bigquery': {'partition_col': 'PARTITIONTIME'}, 'clickhouse': {'temp_db': '__ibis_tmp'}, 'default_backend': None, 'graphviz_repr': True, 'impala': {'temp_db': '__ibis_tmp', 'temp_hdfs_path': '/tmp/ibis'}, 'interactive': False, 'sql': {'default_limit': 10000}, 'verbose': False, 'verbose_log': None} ibis . options . interactive = False sql = lambda proj : print ( ibis . postgres . compile ( proj )) Table operations \u00b6 t = con . table ( \"accounts\" ) sql ( t . head ( 10 )) SELECT t0.id, t0.names, t0.amount FROM accounts AS t0 LIMIT %(param_1)s t . head () . execute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount 0 59 Tim -330 1 43 Yvonne 2731 2 58 Hannah 888 3 65 Wendy 1670 4 82 Zelda -1114 t . namesdistinct () ref_0 PostgreSQLTable[table] name: accounts schema: id : int32 names : string amount : int64 names = Column[string*] 'names' from table ref_0 t . mutate ( new = t . names ) . execute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount new 0 59 Tim -330 Tim 1 43 Yvonne 2731 Yvonne 2 58 Hannah 888 Hannah 3 65 Wendy 1670 Wendy 4 82 Zelda -1114 Zelda 5 -108 Ray 2221 Ray 6 122 Victor 1068 Victor 7 96 Charlie 373 Charlie 8 2 Ray 45 Ray 9 -85 Ingrid 1684 Ingrid 10 104 Yvonne 454 Yvonne 11 28 Quinn 559 Quinn 12 -84 Hannah 583 Hannah 13 -104 Tim 0 Tim 14 -72 Ray 1836 Ray 15 87 Jerry -16 Jerry 16 71 George 50 George 17 -61 Patricia 675 Patricia 18 94 Yvonne 1581 Yvonne 19 94 Laura 669 Laura 20 -39 Quinn 1938 Quinn 21 -9 Frank 117 Frank 22 -24 Ursula 479 Ursula 23 110 Xavier 24 Xavier 24 72 Laura 3620 Laura 25 85 Kevin 274 Kevin 26 25 Edith 125 Edith 27 28 Quinn 606 Quinn 28 -56 Charlie 1565 Charlie 29 -120 Dan 1560 Dan ... ... ... ... ... 9970 -99 Sarah 988 Sarah 9971 -116 Jerry 734 Jerry 9972 50 Quinn 1824 Quinn 9973 25 Edith 126 Edith 9974 -56 Charlie 1754 Charlie 9975 -77 Patricia 73 Patricia 9976 -36 Ursula 43 Ursula 9977 -14 Sarah -1 Sarah 9978 14 Laura 4770 Laura 9979 -66 George 32 George 9980 60 Ursula 967 Ursula 9981 -66 George 38 George 9982 -91 Patricia 188 Patricia 9983 -112 Alice -333 Alice 9984 96 Charlie 823 Charlie 9985 17 Victor 269 Victor 9986 16 Victor 232 Victor 9987 83 Michael 3144 Michael 9988 82 Ursula 3251 Ursula 9989 54 Norbert 63 Norbert 9990 -55 Zelda 893 Zelda 9991 35 Patricia -23 Patricia 9992 66 Ray 3333 Ray 9993 42 George -481 George 9994 111 Xavier 3164 Xavier 9995 -41 Oliver 26082 Oliver 9996 96 Wendy -6534 Wendy 9997 96 Charlie 1014 Charlie 9998 22 George 2196 George 9999 -2 Xavier 112 Xavier 10000 rows \u00d7 4 columns t . head () . execute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount 0 -57 Xavier 251 1 -113 Michael 1082 2 45 Michael 1642 3 -61 Patricia 583 4 -89 Frank 445 Type \u00b6 Ibis uses its own type aliases that map onto database types. See, for example, the correspondence between Ibis type names and Impala type names: Ibis type Impala Type ~~~~~~~~~ ~~~~~~~~~~~ int8 TINYINT int16 SMALLINT int32 INT int64 BIGINT float FLOAT double DOUBLE boolean BOOLEAN string STRING timestamp TIMESTAMP decimal(p, s) DECIMAL(p,s) interval(u) INTERVAL(u)","title":"04 ibis"},{"location":"_nb/%3DPy/02_SQL/04_ibis/#initialisation","text":"import ibis uri = \"postgresql+psycopg2://postgres:postgres@db.postgres.app.com/postgres\" con = ibis . postgres . connect ( url = uri ) con . list_tables () ['country'] ibis . options {'bigquery': {'partition_col': 'PARTITIONTIME'}, 'clickhouse': {'temp_db': '__ibis_tmp'}, 'default_backend': None, 'graphviz_repr': True, 'impala': {'temp_db': '__ibis_tmp', 'temp_hdfs_path': '/tmp/ibis'}, 'interactive': False, 'sql': {'default_limit': 10000}, 'verbose': False, 'verbose_log': None} ibis . options . interactive = False sql = lambda proj : print ( ibis . postgres . compile ( proj ))","title":"Initialisation"},{"location":"_nb/%3DPy/02_SQL/04_ibis/#table-operations","text":"t = con . table ( \"accounts\" ) sql ( t . head ( 10 )) SELECT t0.id, t0.names, t0.amount FROM accounts AS t0 LIMIT %(param_1)s t . head () . execute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount 0 59 Tim -330 1 43 Yvonne 2731 2 58 Hannah 888 3 65 Wendy 1670 4 82 Zelda -1114 t . namesdistinct () ref_0 PostgreSQLTable[table] name: accounts schema: id : int32 names : string amount : int64 names = Column[string*] 'names' from table ref_0 t . mutate ( new = t . names ) . execute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount new 0 59 Tim -330 Tim 1 43 Yvonne 2731 Yvonne 2 58 Hannah 888 Hannah 3 65 Wendy 1670 Wendy 4 82 Zelda -1114 Zelda 5 -108 Ray 2221 Ray 6 122 Victor 1068 Victor 7 96 Charlie 373 Charlie 8 2 Ray 45 Ray 9 -85 Ingrid 1684 Ingrid 10 104 Yvonne 454 Yvonne 11 28 Quinn 559 Quinn 12 -84 Hannah 583 Hannah 13 -104 Tim 0 Tim 14 -72 Ray 1836 Ray 15 87 Jerry -16 Jerry 16 71 George 50 George 17 -61 Patricia 675 Patricia 18 94 Yvonne 1581 Yvonne 19 94 Laura 669 Laura 20 -39 Quinn 1938 Quinn 21 -9 Frank 117 Frank 22 -24 Ursula 479 Ursula 23 110 Xavier 24 Xavier 24 72 Laura 3620 Laura 25 85 Kevin 274 Kevin 26 25 Edith 125 Edith 27 28 Quinn 606 Quinn 28 -56 Charlie 1565 Charlie 29 -120 Dan 1560 Dan ... ... ... ... ... 9970 -99 Sarah 988 Sarah 9971 -116 Jerry 734 Jerry 9972 50 Quinn 1824 Quinn 9973 25 Edith 126 Edith 9974 -56 Charlie 1754 Charlie 9975 -77 Patricia 73 Patricia 9976 -36 Ursula 43 Ursula 9977 -14 Sarah -1 Sarah 9978 14 Laura 4770 Laura 9979 -66 George 32 George 9980 60 Ursula 967 Ursula 9981 -66 George 38 George 9982 -91 Patricia 188 Patricia 9983 -112 Alice -333 Alice 9984 96 Charlie 823 Charlie 9985 17 Victor 269 Victor 9986 16 Victor 232 Victor 9987 83 Michael 3144 Michael 9988 82 Ursula 3251 Ursula 9989 54 Norbert 63 Norbert 9990 -55 Zelda 893 Zelda 9991 35 Patricia -23 Patricia 9992 66 Ray 3333 Ray 9993 42 George -481 George 9994 111 Xavier 3164 Xavier 9995 -41 Oliver 26082 Oliver 9996 96 Wendy -6534 Wendy 9997 96 Charlie 1014 Charlie 9998 22 George 2196 George 9999 -2 Xavier 112 Xavier 10000 rows \u00d7 4 columns t . head () . execute () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id names amount 0 -57 Xavier 251 1 -113 Michael 1082 2 45 Michael 1642 3 -61 Patricia 583 4 -89 Frank 445","title":"Table operations"},{"location":"_nb/%3DPy/02_SQL/04_ibis/#type","text":"Ibis uses its own type aliases that map onto database types. See, for example, the correspondence between Ibis type names and Impala type names: Ibis type Impala Type ~~~~~~~~~ ~~~~~~~~~~~ int8 TINYINT int16 SMALLINT int32 INT int64 BIGINT float FLOAT double DOUBLE boolean BOOLEAN string STRING timestamp TIMESTAMP decimal(p, s) DECIMAL(p,s) interval(u) INTERVAL(u)","title":"Type"},{"location":"_nb/%3DPy/02_SQL/05_postgreSQL%20kernel/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); -- connection: postgresql://postgres:postgres@db.postgres.app.com Failed to connect to a database at postgresql://postgres:postgres@db.postgres.app.com -- connection: postgresql://johnny:password@192.168.32.3:5432/grafana Create tables \u00b6 DROP TABLE IF EXISTS t_weather_observations , t_cities ; -- data from https://simplemaps.com/data/world-cities -- fields are limited in free version -- https://simplemaps.com/data/world-cities#fields CREATE TABLE t_cities ( id SERIAL PRIMARY KEY , name VARCHAR ( 120 ) NOT NULL , name_ascii VARCHAR ( 120 ), location POINT , country VARCHAR ( 120 ), iso2 VARCHAR ( 2 ), iso3 VARCHAR ( 3 ), admin_name VARCHAR ( 120 ), capital VARCHAR ( 7 ), population VARCHAR ( 120 ) ); CREATE TABLE t_weather_observations ( id SERIAL PRIMARY KEY , city VARCHAR ( 80 ), temp_lo INT , -- low temperature temp_hi INT , -- high temperature prcp REAL , -- precipitation date DATE -- FOREIGN KEY (city) REFERENCES t_cities(name) -- or city varchar(80) REFERENCES cities(name), ); DESCRIBE TABLE using information_schema SELECT COLUMN_NAME , DATA_TYPE FROM information_schema . COLUMNS WHERE TABLE_NAME = 't_cities' ; 10 row(s) returned. column_name data_type id integer name character varying name_ascii character varying location point country character varying iso2 character varying iso3 character varying admin_name character varying capital character varying population character varying Populate a table with a row \u00b6 Simple insertion \u00b6 INSERT INTO t_cities ( name , location , capital ) VALUES ( 'San Francisco' , '-194.2, 53.0' , NULL ); SELECT * FROM t_cities LIMIT 10 ; 2 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 2 San Francisco (-194.2,53) 3 San Francisco (-194.2,53) Delete all the entries in table with optionall RETURNING (returns deletions) DELETE FROM t_cities RETURNING * ; 2 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 2 San Francisco (-194.2,53) 3 San Francisco (-194.2,53) SELECT * FROM t_cities ; 0 row(s) returned. Populate a table using FROM \u00b6 Copy the csv data from simplemaps.com with the COPY ... FROM command http://www.postgresqltutorial.com/import-csv-file-into-posgresql-table/ First, remove unused fields with cut in a terminal cut -d, -f11 --complement worldcities.csv > worldcities_modified.csv COPY t_cities ( name , name_ascii , location , country , iso2 , iso3 , admin_name , capital , population ) FROM '/tmp/worldcities_modified.csv' NULL 'NULL' CSV HEADER DELIMITER ';' ; SELECT count ( * ) FROM t_cities ; 1 row(s) returned. count 12958 SELECT * FROM t_cities LIMIT 10 ; 10 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 4 Prizren Prizren (42.2139,20.7397) Kosovo XK XKS Prizren admin 5 Zubin Potok Zubin Potok (42.9144,20.6897) Kosovo XK XKS Zubin Potok admin 6 Kamenic\u00eb Kamenice (42.5781,21.5803) Kosovo XK XKS Kamenic\u00eb admin 7 Viti Viti (42.3214,21.3583) Kosovo XK XKS Viti admin 8 Sht\u00ebrpc\u00eb Shterpce (42.2394,21.0272) Kosovo XK XKS Sht\u00ebrpc\u00eb admin 9 Shtime Shtime (42.4331,21.0397) Kosovo XK XKS Shtime admin 10 Vushtrri Vushtrri (42.8231,20.9675) Kosovo XK XKS Vushtrri admin 11 Dragash Dragash (42.0265,20.6533) Kosovo XK XKS Dragash admin 12 Podujev\u00eb Podujeve (42.9111,21.1899) Kosovo XK XKS Podujev\u00eb admin 13 Fush\u00eb Kosov\u00eb Fushe Kosove (42.6639,21.0961) Kosovo XK XKS Fush\u00eb Kosov\u00eb admin SELECT * FROM t_cities WHERE iso3 = 'USA' LIMIT 10 ; 10 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 8098 Renton Renton (47.4758,-122.1905) United States US USA Washington 101379 8099 Chehalis Chehalis (46.6637,-122.9647) United States US USA Washington 7533 8100 Mercer Island Mercer Island (47.5625,-122.2265) United States US USA Washington 25261 8101 Lynnwood Lynnwood (47.8285,-122.3034) United States US USA Washington 38273 8102 Centralia Centralia (46.7226,-122.9695) United States US USA Washington 41643 8103 Mountlake Terrace Mountlake Terrace (47.792,-122.3076) United States US USA Washington 21337 8104 Seattle Seattle (47.6211,-122.3244) United States US USA Washington 3.64376e+06 8105 Liberty Lake Liberty Lake (47.6687,-117.1032) United States US USA Washington 9893 8106 Airway Heights Airway Heights (47.6459,-117.5792) United States US USA Washington 8166 8107 Brier Brier (47.7922,-122.2734) United States US USA Washington 6884 SELECT DISTINCT capital FROM t_cities LIMIT 10 ; 4 row(s) returned. capital primary minor admin SELECT DISTINCT name , admin_name , capital FROM t_cities WHERE iso3 = 'USA' AND capital <> NULL ORDER BY capital DESC ; 0 row(s) returned. DO $$ DECLARE counter INTEGER := 1 ; first_name VARCHAR ( 50 ) := 'John' ; last_name VARCHAR ( 50 ) := 'Doe' ; payment NUMERIC ( 11 , 2 ) := 20.5 ; BEGIN RAISE NOTICE '% % % has been paid % USD' , counter , first_name , last_name , payment ; END $$ ; NOTICE: 1 John Doe has been paid 20.50 USD INSERT INTO t_weather_observations ( city , temp_lo , temp_hi , prcp , date ) VALUES ( 'San Francisco' , 43 , 57 , 0.0 , '1994-11-29' ); INSERT INTO t_weather_observations ( date , city , temp_hi , temp_lo ) VALUES ( '1994-11-29' , 'Hayward' , 54 , 37 ); insert or update on table \"t_weather_observations\" violates foreign key constraint \"t_weather_observations_city_fkey\" DETAIL: Key (city)=(Hayward) is not present in table \"t_cities\".","title":"05 postgreSQL kernel"},{"location":"_nb/%3DPy/02_SQL/05_postgreSQL%20kernel/#create-tables","text":"DROP TABLE IF EXISTS t_weather_observations , t_cities ; -- data from https://simplemaps.com/data/world-cities -- fields are limited in free version -- https://simplemaps.com/data/world-cities#fields CREATE TABLE t_cities ( id SERIAL PRIMARY KEY , name VARCHAR ( 120 ) NOT NULL , name_ascii VARCHAR ( 120 ), location POINT , country VARCHAR ( 120 ), iso2 VARCHAR ( 2 ), iso3 VARCHAR ( 3 ), admin_name VARCHAR ( 120 ), capital VARCHAR ( 7 ), population VARCHAR ( 120 ) ); CREATE TABLE t_weather_observations ( id SERIAL PRIMARY KEY , city VARCHAR ( 80 ), temp_lo INT , -- low temperature temp_hi INT , -- high temperature prcp REAL , -- precipitation date DATE -- FOREIGN KEY (city) REFERENCES t_cities(name) -- or city varchar(80) REFERENCES cities(name), ); DESCRIBE TABLE using information_schema SELECT COLUMN_NAME , DATA_TYPE FROM information_schema . COLUMNS WHERE TABLE_NAME = 't_cities' ; 10 row(s) returned. column_name data_type id integer name character varying name_ascii character varying location point country character varying iso2 character varying iso3 character varying admin_name character varying capital character varying population character varying","title":"Create tables"},{"location":"_nb/%3DPy/02_SQL/05_postgreSQL%20kernel/#populate-a-table-with-a-row","text":"","title":"Populate a table with a row"},{"location":"_nb/%3DPy/02_SQL/05_postgreSQL%20kernel/#simple-insertion","text":"INSERT INTO t_cities ( name , location , capital ) VALUES ( 'San Francisco' , '-194.2, 53.0' , NULL ); SELECT * FROM t_cities LIMIT 10 ; 2 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 2 San Francisco (-194.2,53) 3 San Francisco (-194.2,53) Delete all the entries in table with optionall RETURNING (returns deletions) DELETE FROM t_cities RETURNING * ; 2 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 2 San Francisco (-194.2,53) 3 San Francisco (-194.2,53) SELECT * FROM t_cities ; 0 row(s) returned.","title":"Simple insertion"},{"location":"_nb/%3DPy/02_SQL/05_postgreSQL%20kernel/#populate-a-table-using-from","text":"Copy the csv data from simplemaps.com with the COPY ... FROM command http://www.postgresqltutorial.com/import-csv-file-into-posgresql-table/ First, remove unused fields with cut in a terminal cut -d, -f11 --complement worldcities.csv > worldcities_modified.csv COPY t_cities ( name , name_ascii , location , country , iso2 , iso3 , admin_name , capital , population ) FROM '/tmp/worldcities_modified.csv' NULL 'NULL' CSV HEADER DELIMITER ';' ; SELECT count ( * ) FROM t_cities ; 1 row(s) returned. count 12958 SELECT * FROM t_cities LIMIT 10 ; 10 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 4 Prizren Prizren (42.2139,20.7397) Kosovo XK XKS Prizren admin 5 Zubin Potok Zubin Potok (42.9144,20.6897) Kosovo XK XKS Zubin Potok admin 6 Kamenic\u00eb Kamenice (42.5781,21.5803) Kosovo XK XKS Kamenic\u00eb admin 7 Viti Viti (42.3214,21.3583) Kosovo XK XKS Viti admin 8 Sht\u00ebrpc\u00eb Shterpce (42.2394,21.0272) Kosovo XK XKS Sht\u00ebrpc\u00eb admin 9 Shtime Shtime (42.4331,21.0397) Kosovo XK XKS Shtime admin 10 Vushtrri Vushtrri (42.8231,20.9675) Kosovo XK XKS Vushtrri admin 11 Dragash Dragash (42.0265,20.6533) Kosovo XK XKS Dragash admin 12 Podujev\u00eb Podujeve (42.9111,21.1899) Kosovo XK XKS Podujev\u00eb admin 13 Fush\u00eb Kosov\u00eb Fushe Kosove (42.6639,21.0961) Kosovo XK XKS Fush\u00eb Kosov\u00eb admin SELECT * FROM t_cities WHERE iso3 = 'USA' LIMIT 10 ; 10 row(s) returned. id name name_ascii location country iso2 iso3 admin_name capital population 8098 Renton Renton (47.4758,-122.1905) United States US USA Washington 101379 8099 Chehalis Chehalis (46.6637,-122.9647) United States US USA Washington 7533 8100 Mercer Island Mercer Island (47.5625,-122.2265) United States US USA Washington 25261 8101 Lynnwood Lynnwood (47.8285,-122.3034) United States US USA Washington 38273 8102 Centralia Centralia (46.7226,-122.9695) United States US USA Washington 41643 8103 Mountlake Terrace Mountlake Terrace (47.792,-122.3076) United States US USA Washington 21337 8104 Seattle Seattle (47.6211,-122.3244) United States US USA Washington 3.64376e+06 8105 Liberty Lake Liberty Lake (47.6687,-117.1032) United States US USA Washington 9893 8106 Airway Heights Airway Heights (47.6459,-117.5792) United States US USA Washington 8166 8107 Brier Brier (47.7922,-122.2734) United States US USA Washington 6884 SELECT DISTINCT capital FROM t_cities LIMIT 10 ; 4 row(s) returned. capital primary minor admin SELECT DISTINCT name , admin_name , capital FROM t_cities WHERE iso3 = 'USA' AND capital <> NULL ORDER BY capital DESC ; 0 row(s) returned. DO $$ DECLARE counter INTEGER := 1 ; first_name VARCHAR ( 50 ) := 'John' ; last_name VARCHAR ( 50 ) := 'Doe' ; payment NUMERIC ( 11 , 2 ) := 20.5 ; BEGIN RAISE NOTICE '% % % has been paid % USD' , counter , first_name , last_name , payment ; END $$ ; NOTICE: 1 John Doe has been paid 20.50 USD INSERT INTO t_weather_observations ( city , temp_lo , temp_hi , prcp , date ) VALUES ( 'San Francisco' , 43 , 57 , 0.0 , '1994-11-29' ); INSERT INTO t_weather_observations ( date , city , temp_hi , temp_lo ) VALUES ( '1994-11-29' , 'Hayward' , 54 , 37 ); insert or update on table \"t_weather_observations\" violates foreign key constraint \"t_weather_observations_city_fkey\" DETAIL: Key (city)=(Hayward) is not present in table \"t_cities\".","title":"Populate a table using FROM"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); OLTP - Online transaction processing OLEP - OLAP - Online analytical processing OLAP cube Cube multidimensionnel OLAP vs OLTP OLAP operations Apache Kylin OLAP \u00b6 Star/snowflake schemas \u00b6 The star schema separates business process data into facts, which hold the measurable, quantitative data about a business, and dimensions which are descriptive attributes related to fact data: - Fact tables record measurements or metrics for a specific event, generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept - Dimension tables usually have a relatively small number of records compared to fact tables, but each record may have a very large number of attributes to describe the fact data. \"Snowflaking\" is a method of normalizing the dimension tables in a star schema. Operations \u00b6 slice \u00b6 Dice \u00b6 Drill Down/Up \u00b6 Roll-up \u00b6 Pivot \u00b6","title":"OLEP   OLAP"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#olap","text":"","title":"OLAP"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#starsnowflake-schemas","text":"The star schema separates business process data into facts, which hold the measurable, quantitative data about a business, and dimensions which are descriptive attributes related to fact data: - Fact tables record measurements or metrics for a specific event, generally consist of numeric values, and foreign keys to dimensional data where descriptive information is kept - Dimension tables usually have a relatively small number of records compared to fact tables, but each record may have a very large number of attributes to describe the fact data. \"Snowflaking\" is a method of normalizing the dimension tables in a star schema.","title":"Star/snowflake schemas"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#operations","text":"","title":"Operations"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#slice","text":"","title":"slice"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#dice","text":"","title":"Dice"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#drill-downup","text":"","title":"Drill Down/Up"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#roll-up","text":"","title":"Roll-up"},{"location":"_nb/%3DPy/02_SQL/OLEP%20-%20OLAP/#pivot","text":"","title":"Pivot"},{"location":"_nb/%3DPy/02_SQL/Untitled/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); -- connection: postgresql://johnny:password@192.168.32.3:5432/grafana DROP TABLE IF EXISTS g_3p ; CREATE TABLE g_3p ( id SERIAL PRIMARY KEY , name VARCHAR ( 120 ) NOT NULL , ts TIMESTAMP , capital INTEGER , population INTEGER ); INSERT INTO g_3p ( name , ts , capital , population ) VALUES ( 'johnny' , '2019-11-08 11:10:25-07' , 2000 , 3000 ), ( 'thunder' , '2019-11-08 10:10:25-07' , 2500 , 3900 ) ; SELECT * from g_3p ; 2 row(s) returned. id name ts capital population 1 johnny 2019-11-08 11:10:25 2000 3000 2 thunder 2019-11-08 10:10:25 2500 3900 SELECT ts AS \"time\" , population , capital FROM g_3p ORDER BY ts 4 row(s) returned. time population capital 2019-11-08 10:10:25 3900 2500 2019-11-08 11:10:25 3000 2000 2019-11-08 14:10:25 390 2570 2019-11-08 15:10:25 300 2400 INSERT INTO g_3p ( name , ts , capital , population ) VALUES ( 'johnny' , '2019-11-08 15:10:25-07' , 2400 , 300 ), ( 'thunder' , '2019-11-08 14:10:25-07' , 2570 , 390 ) ;","title":"Untitled"},{"location":"_nb/%3DPy/03_MongoDB/MongoDB/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext watermark % watermark - dvmp pymongo , pyarrow 2019-11-02 CPython 3.7.3 IPython 7.8.0 pymongo 3.8.0 pyarrow not installed compiler : GCC 7.3.0 system : Linux release : 5.0.0-31-generic machine : x86_64 processor : x86_64 CPU cores : 8 interpreter: 64bit from pymongo import MongoClient mongo_local = dict ( user = 'root' , password = 'secret' , port = 27017 , host = '172.22.0.3' , database = \"starwars\" , collection = \"people\" ) mongo_atlas = dict ( user = 'm220student' , password = 'm220password' , port = 27017 , host = 'baobab-crx55.mongodb.net' ) import datetime post = { \"author\" : \"Mike\" , \"text\" : \"My first blog post!\" , \"tags\" : [ \"mongodb\" , \"python\" , \"pymongo\" ], \"date\" : datetime . datetime . utcnow ()} uri = f\"mongodb://{user}:{password}@{host}\" conn = mongo_local uri = f \"mongodb:// { conn [ 'user' ] } : { conn [ 'password' ] } @ { conn [ 'host' ] } : { conn . get ( 'port' , 27017 ) } \" print ( uri ) database = \"starwars\" collection = \"people\" client = MongoClient ( uri ) mongodb://root:secret@172.22.0.3:27017 \"mongodb://root:secret@172.22.0.3:27017/starwars\" . rsplit ( \"/\" , maxsplit = 1 )[ 0 ] 'mongodb://root:secret@172.22.0.3:27017' m = MongoClient ( \"mongodb://mongo:password@mongo\" ) . list_database_names () m . list_database_names () --------------------------------------------------------------------------- OperationFailure Traceback (most recent call last) <ipython-input-71-a100b7e4b26f> in <module> ----> 1 m . list_database_names ( ) /opt/conda/lib/python3.7/site-packages/pymongo/mongo_client.py in list_database_names (self, session) 1697 \"\"\" 1698 return [doc[\"name\"] -> 1699 for doc in self.list_databases(session, nameOnly=True)] 1700 1701 def database_names ( self , session = None ) : /opt/conda/lib/python3.7/site-packages/pymongo/mongo_client.py in list_databases (self, session, **kwargs) 1678 cmd . update ( kwargs ) 1679 admin = self . _database_default_options ( \"admin\" ) -> 1680 res = admin . command ( cmd , session = session ) 1681 # listDatabases doesn't return a cursor (yet). Fake one. 1682 cursor = { /opt/conda/lib/python3.7/site-packages/pymongo/database.py in command (self, command, value, check, allowable_errors, read_preference, codec_options, session, **kwargs) 653 or ReadPreference.PRIMARY) 654 with self.__client._socket_for_reads( --> 655 read_preference) as (sock_info, slave_ok): 656 return self._command(sock_info, command, slave_ok, value, 657 check , allowable_errors , read_preference , /opt/conda/lib/python3.7/contextlib.py in __enter__ (self) 110 del self . args , self . kwds , self . func 111 try : --> 112 return next ( self . gen ) 113 except StopIteration : 114 raise RuntimeError ( \"generator didn't yield\" ) from None /opt/conda/lib/python3.7/site-packages/pymongo/mongo_client.py in _socket_for_reads (self, read_preference) 1135 server = topology . select_server ( read_preference ) 1136 -> 1137 with self . _get_socket ( server ) as sock_info : 1138 slave_ok = (single and not sock_info.is_mongos) or ( 1139 read_preference != ReadPreference.PRIMARY) /opt/conda/lib/python3.7/contextlib.py in __enter__ (self) 110 del self . args , self . kwds , self . func 111 try : --> 112 return next ( self . gen ) 113 except StopIteration : 114 raise RuntimeError ( \"generator didn't yield\" ) from None /opt/conda/lib/python3.7/site-packages/pymongo/mongo_client.py in _get_socket (self, server) 1092 def _get_socket ( self , server ) : 1093 try : -> 1094 with server . get_socket ( self . __all_credentials ) as sock_info : 1095 yield sock_info 1096 except NetworkTimeout : /opt/conda/lib/python3.7/contextlib.py in __enter__ (self) 110 del self . args , self . kwds , self . func 111 try : --> 112 return next ( self . gen ) 113 except StopIteration : 114 raise RuntimeError ( \"generator didn't yield\" ) from None /opt/conda/lib/python3.7/site-packages/pymongo/pool.py in get_socket (self, all_credentials, checkout) 1009 sock_info = self . _get_socket_no_auth ( ) 1010 try : -> 1011 sock_info . check_auth ( all_credentials ) 1012 yield sock_info 1013 except : /opt/conda/lib/python3.7/site-packages/pymongo/pool.py in check_auth (self, all_credentials) 680 681 for credentials in cached - authset : --> 682 auth . authenticate ( credentials , self ) 683 self . authset . add ( credentials ) 684 /opt/conda/lib/python3.7/site-packages/pymongo/auth.py in authenticate (credentials, sock_info) 563 mechanism = credentials . mechanism 564 auth_func = _AUTH_MAP . get ( mechanism ) --> 565 auth_func ( credentials , sock_info ) 566 567 /opt/conda/lib/python3.7/site-packages/pymongo/auth.py in _authenticate_default (credentials, sock_info) 538 return _authenticate_scram ( credentials , sock_info , 'SCRAM-SHA-256' ) 539 else : --> 540 return _authenticate_scram ( credentials , sock_info , 'SCRAM-SHA-1' ) 541 elif sock_info . max_wire_version >= 3 : 542 return _authenticate_scram ( credentials , sock_info , 'SCRAM-SHA-1' ) /opt/conda/lib/python3.7/site-packages/pymongo/auth.py in _authenticate_scram (credentials, sock_info, mechanism) 262 ( 'payload' , Binary ( b\"n,,\" + first_bare ) ) , 263 ('autoAuthorize', 1)]) --> 264 res = sock_info . command ( source , cmd ) 265 266 server_first = res [ 'payload' ] /opt/conda/lib/python3.7/site-packages/pymongo/pool.py in command (self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields) 582 use_op_msg = self . op_msg_enabled , 583 unacknowledged = unacknowledged , --> 584 user_fields=user_fields) 585 except OperationFailure : 586 raise /opt/conda/lib/python3.7/site-packages/pymongo/network.py in command (sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields) 156 helpers._check_command_response( 157 response_doc , None , allowable_errors , --> 158 parse_write_concern_error=parse_write_concern_error) 159 except Exception as exc : 160 if publish : /opt/conda/lib/python3.7/site-packages/pymongo/helpers.py in _check_command_response (response, msg, allowable_errors, parse_write_concern_error) 153 154 msg = msg or \"%s\" --> 155 raise OperationFailure ( msg % errmsg , code , response ) 156 157 OperationFailure : Authentication failed. \"mongodb://root:secret@172.22.0.3:27017/starwars\" . split <function str.split(sep=None, maxsplit=-1)> aze = MongoClient ( \"mongodb://root:secret@172.22.0.3:27017\" ) aze . list_database_names () ['admin', 'config', 'local', 'people-bson', 'saildrone', 'sc', 'starwars', 'test'] aze.get_database('starwars').get_collection('people').find_one() % store uri Stored 'uri' (str) client . list_database_names () ['admin', 'config', 'local', 'people-bson', 'saildrone', 'sc', 'starwars', 'test'] res = client . get_database ( 'starwars' ) . get_collection ( 'people' ) res . find_one () {'_id': ObjectId('5d31e79f5decab6c5ac11358'), 'name': 'Luke Skywalker', 'height': 172, 'mass': 77, 'hair_color': 'blond', 'skin_color': 'fair', 'eye_color': 'blue', 'birth_year': '19BBY', 'gender': 'male', 'homeworld': {'name': 'Tatooine', 'rotation_period': 23, 'orbital_period': 304, 'diameter': 10465, 'climate': 'arid', 'gravity': '1 standard', 'terrain': 'desert', 'surface_water': 1, 'population': 200000, 'residents': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/2/', 'https://swapi.co/api/people/4/', 'https://swapi.co/api/people/6/', 'https://swapi.co/api/people/7/', 'https://swapi.co/api/people/8/', 'https://swapi.co/api/people/9/', 'https://swapi.co/api/people/11/', 'https://swapi.co/api/people/43/', 'https://swapi.co/api/people/62/'], 'films': ['https://swapi.co/api/films/5/', 'https://swapi.co/api/films/4/', 'https://swapi.co/api/films/6/', 'https://swapi.co/api/films/3/', 'https://swapi.co/api/films/1/'], 'created': '2014-12-09T13:50:49.641000Z', 'edited': '2014-12-21T20:48:04.175778Z', 'url': 'https://swapi.co/api/planets/1/'}, 'films': [{'title': 'The Empire Strikes Back', 'episode_id': 5, 'opening_crawl': 'It is a dark time for the\\r\\nRebellion. Although the Death\\r\\nStar has been destroyed,\\r\\nImperial troops have driven the\\r\\nRebel forces from their hidden\\r\\nbase and pursued them across\\r\\nthe galaxy.\\r\\n\\r\\nEvading the dreaded Imperial\\r\\nStarfleet, a group of freedom\\r\\nfighters led by Luke Skywalker\\r\\nhas established a new secret\\r\\nbase on the remote ice world\\r\\nof Hoth.\\r\\n\\r\\nThe evil lord Darth Vader,\\r\\nobsessed with finding young\\r\\nSkywalker, has dispatched\\r\\nthousands of remote probes into\\r\\nthe far reaches of space....', 'director': 'Irvin Kershner', 'producer': 'Gary Kurtz, Rick McCallum', 'release_date': '1980-05-17', 'characters': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/2/', 'https://swapi.co/api/people/3/', 'https://swapi.co/api/people/4/', 'https://swapi.co/api/people/5/', 'https://swapi.co/api/people/10/', 'https://swapi.co/api/people/13/', 'https://swapi.co/api/people/14/', 'https://swapi.co/api/people/18/', 'https://swapi.co/api/people/20/', 'https://swapi.co/api/people/21/', 'https://swapi.co/api/people/22/', 'https://swapi.co/api/people/23/', 'https://swapi.co/api/people/24/', 'https://swapi.co/api/people/25/', 'https://swapi.co/api/people/26/'], 'planets': ['https://swapi.co/api/planets/4/', 'https://swapi.co/api/planets/5/', 'https://swapi.co/api/planets/6/', 'https://swapi.co/api/planets/27/'], 'starships': ['https://swapi.co/api/starships/15/', 'https://swapi.co/api/starships/10/', 'https://swapi.co/api/starships/11/', 'https://swapi.co/api/starships/12/', 'https://swapi.co/api/starships/21/', 'https://swapi.co/api/starships/22/', 'https://swapi.co/api/starships/23/', 'https://swapi.co/api/starships/3/', 'https://swapi.co/api/starships/17/'], 'vehicles': ['https://swapi.co/api/vehicles/8/', 'https://swapi.co/api/vehicles/14/', 'https://swapi.co/api/vehicles/16/', 'https://swapi.co/api/vehicles/18/', 'https://swapi.co/api/vehicles/19/', 'https://swapi.co/api/vehicles/20/'], 'species': ['https://swapi.co/api/species/6/', 'https://swapi.co/api/species/7/', 'https://swapi.co/api/species/3/', 'https://swapi.co/api/species/2/', 'https://swapi.co/api/species/1/'], 'created': '2014-12-12T11:26:24.656000Z', 'edited': '2017-04-19T10:57:29.544256Z', 'url': 'https://swapi.co/api/films/2/'}, {'title': 'Revenge of the Sith', 'episode_id': 3, 'opening_crawl': 'War! The Republic is crumbling\\r\\nunder attacks by the ruthless\\r\\nSith Lord, Count Dooku.\\r\\nThere are heroes on both sides.\\r\\nEvil is everywhere.\\r\\n\\r\\nIn a stunning move, the\\r\\nfiendish droid leader, General\\r\\nGrievous, has swept into the\\r\\nRepublic capital and kidnapped\\r\\nChancellor Palpatine, leader of\\r\\nthe Galactic Senate.\\r\\n\\r\\nAs the Separatist Droid Army\\r\\nattempts to flee the besieged\\r\\ncapital with their valuable\\r\\nhostage, two Jedi Knights lead a\\r\\ndesperate mission to rescue the\\r\\ncaptive Chancellor....', 'director': 'George Lucas', 'producer': 'Rick McCallum', 'release_date': '2005-05-19', 'characters': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/2/', 'https://swapi.co/api/people/3/', 'https://swapi.co/api/people/4/', 'https://swapi.co/api/people/5/', 'https://swapi.co/api/people/6/', 'https://swapi.co/api/people/7/', 'https://swapi.co/api/people/10/', 'https://swapi.co/api/people/11/', 'https://swapi.co/api/people/12/', 'https://swapi.co/api/people/13/', 'https://swapi.co/api/people/20/', 'https://swapi.co/api/people/21/', 'https://swapi.co/api/people/33/', 'https://swapi.co/api/people/46/', 'https://swapi.co/api/people/51/', 'https://swapi.co/api/people/52/', 'https://swapi.co/api/people/53/', 'https://swapi.co/api/people/54/', 'https://swapi.co/api/people/55/', 'https://swapi.co/api/people/56/', 'https://swapi.co/api/people/58/', 'https://swapi.co/api/people/63/', 'https://swapi.co/api/people/64/', 'https://swapi.co/api/people/67/', 'https://swapi.co/api/people/68/', 'https://swapi.co/api/people/75/', 'https://swapi.co/api/people/78/', 'https://swapi.co/api/people/79/', 'https://swapi.co/api/people/80/', 'https://swapi.co/api/people/81/', 'https://swapi.co/api/people/82/', 'https://swapi.co/api/people/83/', 'https://swapi.co/api/people/35/'], 'planets': ['https://swapi.co/api/planets/2/', 'https://swapi.co/api/planets/5/', 'https://swapi.co/api/planets/8/', 'https://swapi.co/api/planets/9/', 'https://swapi.co/api/planets/12/', 'https://swapi.co/api/planets/13/', 'https://swapi.co/api/planets/14/', 'https://swapi.co/api/planets/15/', 'https://swapi.co/api/planets/16/', 'https://swapi.co/api/planets/17/', 'https://swapi.co/api/planets/18/', 'https://swapi.co/api/planets/19/', 'https://swapi.co/api/planets/1/'], 'starships': ['https://swapi.co/api/starships/48/', 'https://swapi.co/api/starships/59/', 'https://swapi.co/api/starships/61/', 'https://swapi.co/api/starships/32/', 'https://swapi.co/api/starships/63/', 'https://swapi.co/api/starships/64/', 'https://swapi.co/api/starships/65/', 'https://swapi.co/api/starships/66/', 'https://swapi.co/api/starships/74/', 'https://swapi.co/api/starships/75/', 'https://swapi.co/api/starships/2/', 'https://swapi.co/api/starships/68/'], 'vehicles': ['https://swapi.co/api/vehicles/33/', 'https://swapi.co/api/vehicles/50/', 'https://swapi.co/api/vehicles/53/', 'https://swapi.co/api/vehicles/56/', 'https://swapi.co/api/vehicles/60/', 'https://swapi.co/api/vehicles/62/', 'https://swapi.co/api/vehicles/67/', 'https://swapi.co/api/vehicles/69/', 'https://swapi.co/api/vehicles/70/', 'https://swapi.co/api/vehicles/71/', 'https://swapi.co/api/vehicles/72/', 'https://swapi.co/api/vehicles/73/', 'https://swapi.co/api/vehicles/76/'], 'species': ['https://swapi.co/api/species/19/', 'https://swapi.co/api/species/33/', 'https://swapi.co/api/species/2/', 'https://swapi.co/api/species/3/', 'https://swapi.co/api/species/36/', 'https://swapi.co/api/species/37/', 'https://swapi.co/api/species/6/', 'https://swapi.co/api/species/1/', 'https://swapi.co/api/species/34/', 'https://swapi.co/api/species/15/', 'https://swapi.co/api/species/35/', 'https://swapi.co/api/species/20/', 'https://swapi.co/api/species/23/', 'https://swapi.co/api/species/24/', 'https://swapi.co/api/species/25/', 'https://swapi.co/api/species/26/', 'https://swapi.co/api/species/27/', 'https://swapi.co/api/species/28/', 'https://swapi.co/api/species/29/', 'https://swapi.co/api/species/30/'], 'created': '2014-12-20T18:49:38.403000Z', 'edited': '2015-04-11T09:45:44.862122Z', 'url': 'https://swapi.co/api/films/6/'}, {'title': 'Return of the Jedi', 'episode_id': 6, 'opening_crawl': 'Luke Skywalker has returned to\\r\\nhis home planet of Tatooine in\\r\\nan attempt to rescue his\\r\\nfriend Han Solo from the\\r\\nclutches of the vile gangster\\r\\nJabba the Hutt.\\r\\n\\r\\nLittle does Luke know that the\\r\\nGALACTIC EMPIRE has secretly\\r\\nbegun construction on a new\\r\\narmored space station even\\r\\nmore powerful than the first\\r\\ndreaded Death Star.\\r\\n\\r\\nWhen completed, this ultimate\\r\\nweapon will spell certain doom\\r\\nfor the small band of rebels\\r\\nstruggling to restore freedom\\r\\nto the galaxy...', 'director': 'Richard Marquand', 'producer': 'Howard G. Kazanjian, George Lucas, Rick McCallum', 'release_date': '1983-05-25', 'characters': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/2/', 'https://swapi.co/api/people/3/', 'https://swapi.co/api/people/4/', 'https://swapi.co/api/people/5/', 'https://swapi.co/api/people/10/', 'https://swapi.co/api/people/13/', 'https://swapi.co/api/people/14/', 'https://swapi.co/api/people/16/', 'https://swapi.co/api/people/18/', 'https://swapi.co/api/people/20/', 'https://swapi.co/api/people/21/', 'https://swapi.co/api/people/22/', 'https://swapi.co/api/people/25/', 'https://swapi.co/api/people/27/', 'https://swapi.co/api/people/28/', 'https://swapi.co/api/people/29/', 'https://swapi.co/api/people/30/', 'https://swapi.co/api/people/31/', 'https://swapi.co/api/people/45/'], 'planets': ['https://swapi.co/api/planets/5/', 'https://swapi.co/api/planets/7/', 'https://swapi.co/api/planets/8/', 'https://swapi.co/api/planets/9/', 'https://swapi.co/api/planets/1/'], 'starships': ['https://swapi.co/api/starships/15/', 'https://swapi.co/api/starships/10/', 'https://swapi.co/api/starships/11/', 'https://swapi.co/api/starships/12/', 'https://swapi.co/api/starships/22/', 'https://swapi.co/api/starships/23/', 'https://swapi.co/api/starships/27/', 'https://swapi.co/api/starships/28/', 'https://swapi.co/api/starships/29/', 'https://swapi.co/api/starships/3/', 'https://swapi.co/api/starships/17/', 'https://swapi.co/api/starships/2/'], 'vehicles': ['https://swapi.co/api/vehicles/8/', 'https://swapi.co/api/vehicles/16/', 'https://swapi.co/api/vehicles/18/', 'https://swapi.co/api/vehicles/19/', 'https://swapi.co/api/vehicles/24/', 'https://swapi.co/api/vehicles/25/', 'https://swapi.co/api/vehicles/26/', 'https://swapi.co/api/vehicles/30/'], 'species': ['https://swapi.co/api/species/1/', 'https://swapi.co/api/species/2/', 'https://swapi.co/api/species/3/', 'https://swapi.co/api/species/5/', 'https://swapi.co/api/species/6/', 'https://swapi.co/api/species/8/', 'https://swapi.co/api/species/9/', 'https://swapi.co/api/species/10/', 'https://swapi.co/api/species/15/'], 'created': '2014-12-18T10:39:33.255000Z', 'edited': '2015-04-11T09:46:05.220365Z', 'url': 'https://swapi.co/api/films/3/'}, {'title': 'A New Hope', 'episode_id': 4, 'opening_crawl': \"It is a period of civil war.\\r\\nRebel spaceships, striking\\r\\nfrom a hidden base, have won\\r\\ntheir first victory against\\r\\nthe evil Galactic Empire.\\r\\n\\r\\nDuring the battle, Rebel\\r\\nspies managed to steal secret\\r\\nplans to the Empire's\\r\\nultimate weapon, the DEATH\\r\\nSTAR, an armored space\\r\\nstation with enough power\\r\\nto destroy an entire planet.\\r\\n\\r\\nPursued by the Empire's\\r\\nsinister agents, Princess\\r\\nLeia races home aboard her\\r\\nstarship, custodian of the\\r\\nstolen plans that can save her\\r\\npeople and restore\\r\\nfreedom to the galaxy....\", 'director': 'George Lucas', 'producer': 'Gary Kurtz, Rick McCallum', 'release_date': '1977-05-25', 'characters': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/2/', 'https://swapi.co/api/people/3/', 'https://swapi.co/api/people/4/', 'https://swapi.co/api/people/5/', 'https://swapi.co/api/people/6/', 'https://swapi.co/api/people/7/', 'https://swapi.co/api/people/8/', 'https://swapi.co/api/people/9/', 'https://swapi.co/api/people/10/', 'https://swapi.co/api/people/12/', 'https://swapi.co/api/people/13/', 'https://swapi.co/api/people/14/', 'https://swapi.co/api/people/15/', 'https://swapi.co/api/people/16/', 'https://swapi.co/api/people/18/', 'https://swapi.co/api/people/19/', 'https://swapi.co/api/people/81/'], 'planets': ['https://swapi.co/api/planets/2/', 'https://swapi.co/api/planets/3/', 'https://swapi.co/api/planets/1/'], 'starships': ['https://swapi.co/api/starships/2/', 'https://swapi.co/api/starships/3/', 'https://swapi.co/api/starships/5/', 'https://swapi.co/api/starships/9/', 'https://swapi.co/api/starships/10/', 'https://swapi.co/api/starships/11/', 'https://swapi.co/api/starships/12/', 'https://swapi.co/api/starships/13/'], 'vehicles': ['https://swapi.co/api/vehicles/4/', 'https://swapi.co/api/vehicles/6/', 'https://swapi.co/api/vehicles/7/', 'https://swapi.co/api/vehicles/8/'], 'species': ['https://swapi.co/api/species/5/', 'https://swapi.co/api/species/3/', 'https://swapi.co/api/species/2/', 'https://swapi.co/api/species/1/', 'https://swapi.co/api/species/4/'], 'created': '2014-12-10T14:23:31.880000Z', 'edited': '2015-04-11T09:46:52.774897Z', 'url': 'https://swapi.co/api/films/1/'}, {'title': 'The Force Awakens', 'episode_id': 7, 'opening_crawl': \"Luke Skywalker has vanished.\\r\\nIn his absence, the sinister\\r\\nFIRST ORDER has risen from\\r\\nthe ashes of the Empire\\r\\nand will not rest until\\r\\nSkywalker, the last Jedi,\\r\\nhas been destroyed.\\r\\n \\r\\nWith the support of the\\r\\nREPUBLIC, General Leia Organa\\r\\nleads a brave RESISTANCE.\\r\\nShe is desperate to find her\\r\\nbrother Luke and gain his\\r\\nhelp in restoring peace and\\r\\njustice to the galaxy.\\r\\n \\r\\nLeia has sent her most daring\\r\\npilot on a secret mission\\r\\nto Jakku, where an old ally\\r\\nhas discovered a clue to\\r\\nLuke's whereabouts....\", 'director': 'J. J. Abrams', 'producer': 'Kathleen Kennedy, J. J. Abrams, Bryan Burk', 'release_date': '2015-12-11', 'characters': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/3/', 'https://swapi.co/api/people/5/', 'https://swapi.co/api/people/13/', 'https://swapi.co/api/people/14/', 'https://swapi.co/api/people/27/', 'https://swapi.co/api/people/84/', 'https://swapi.co/api/people/85/', 'https://swapi.co/api/people/86/', 'https://swapi.co/api/people/87/', 'https://swapi.co/api/people/88/'], 'planets': ['https://swapi.co/api/planets/61/'], 'starships': ['https://swapi.co/api/starships/77/', 'https://swapi.co/api/starships/10/'], 'vehicles': [], 'species': ['https://swapi.co/api/species/3/', 'https://swapi.co/api/species/2/', 'https://swapi.co/api/species/1/'], 'created': '2015-04-17T06:51:30.504780Z', 'edited': '2015-12-17T14:31:47.617768Z', 'url': 'https://swapi.co/api/films/7/'}], 'species': [{'name': 'Human', 'classification': 'mammal', 'designation': 'sentient', 'average_height': 180, 'skin_colors': 'caucasian, black, asian, hispanic', 'hair_colors': 'blonde, brown, black, red', 'eye_colors': 'brown, blue, green, hazel, grey, amber', 'average_lifespan': 120, 'homeworld': 'https://swapi.co/api/planets/9/', 'language': 'Galactic Basic', 'people': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/4/', 'https://swapi.co/api/people/5/', 'https://swapi.co/api/people/6/', 'https://swapi.co/api/people/7/', 'https://swapi.co/api/people/9/', 'https://swapi.co/api/people/10/', 'https://swapi.co/api/people/11/', 'https://swapi.co/api/people/12/', 'https://swapi.co/api/people/14/', 'https://swapi.co/api/people/18/', 'https://swapi.co/api/people/19/', 'https://swapi.co/api/people/21/', 'https://swapi.co/api/people/22/', 'https://swapi.co/api/people/25/', 'https://swapi.co/api/people/26/', 'https://swapi.co/api/people/28/', 'https://swapi.co/api/people/29/', 'https://swapi.co/api/people/32/', 'https://swapi.co/api/people/34/', 'https://swapi.co/api/people/43/', 'https://swapi.co/api/people/51/', 'https://swapi.co/api/people/60/', 'https://swapi.co/api/people/61/', 'https://swapi.co/api/people/62/', 'https://swapi.co/api/people/66/', 'https://swapi.co/api/people/67/', 'https://swapi.co/api/people/68/', 'https://swapi.co/api/people/69/', 'https://swapi.co/api/people/74/', 'https://swapi.co/api/people/81/', 'https://swapi.co/api/people/84/', 'https://swapi.co/api/people/85/', 'https://swapi.co/api/people/86/', 'https://swapi.co/api/people/35/'], 'films': ['https://swapi.co/api/films/2/', 'https://swapi.co/api/films/7/', 'https://swapi.co/api/films/5/', 'https://swapi.co/api/films/4/', 'https://swapi.co/api/films/6/', 'https://swapi.co/api/films/3/', 'https://swapi.co/api/films/1/'], 'created': '2014-12-10T13:52:11.567000Z', 'edited': '2015-04-17T06:59:55.850671Z', 'url': 'https://swapi.co/api/species/1/'}], 'vehicles': [{'name': 'Snowspeeder', 'model': 't-47 airspeeder', 'manufacturer': 'Incom corporation', 'cost_in_credits': 'unknown', 'length': 4.5, 'max_atmosphering_speed': 650, 'crew': 2, 'passengers': 0, 'cargo_capacity': 10, 'consumables': 'none', 'vehicle_class': 'airspeeder', 'pilots': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/18/'], 'films': ['https://swapi.co/api/films/2/'], 'created': '2014-12-15T12:22:12Z', 'edited': '2014-12-22T18:21:15.623033Z', 'url': 'https://swapi.co/api/vehicles/14/'}, {'name': 'Imperial Speeder Bike', 'model': '74-Z speeder bike', 'manufacturer': 'Aratech Repulsor Company', 'cost_in_credits': 8000, 'length': 3, 'max_atmosphering_speed': 360, 'crew': 1, 'passengers': 1, 'cargo_capacity': 4, 'consumables': '1 day', 'vehicle_class': 'speeder', 'pilots': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/5/'], 'films': ['https://swapi.co/api/films/3/'], 'created': '2014-12-18T11:20:04.625000Z', 'edited': '2014-12-22T18:21:15.920537Z', 'url': 'https://swapi.co/api/vehicles/30/'}], 'starships': [{'name': 'X-wing', 'model': 'T-65 X-wing', 'manufacturer': 'Incom Corporation', 'cost_in_credits': 149999, 'length': 12.5, 'max_atmosphering_speed': 1050, 'crew': 1, 'passengers': 0, 'cargo_capacity': 110, 'consumables': '1 week', 'hyperdrive_rating': 1.0, 'MGLT': 100, 'starship_class': 'Starfighter', 'pilots': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/9/', 'https://swapi.co/api/people/18/', 'https://swapi.co/api/people/19/'], 'films': ['https://swapi.co/api/films/2/', 'https://swapi.co/api/films/3/', 'https://swapi.co/api/films/1/'], 'created': '2014-12-12T11:19:05.340000Z', 'edited': '2014-12-22T17:35:44.491233Z', 'url': 'https://swapi.co/api/starships/12/'}, {'name': 'Imperial shuttle', 'model': 'Lambda-class T-4a shuttle', 'manufacturer': 'Sienar Fleet Systems', 'cost_in_credits': 240000, 'length': 20, 'max_atmosphering_speed': 850, 'crew': 6, 'passengers': 20, 'cargo_capacity': 80000, 'consumables': '2 months', 'hyperdrive_rating': 1.0, 'MGLT': 50, 'starship_class': 'Armed government transport', 'pilots': ['https://swapi.co/api/people/1/', 'https://swapi.co/api/people/13/', 'https://swapi.co/api/people/14/'], 'films': ['https://swapi.co/api/films/2/', 'https://swapi.co/api/films/3/'], 'created': '2014-12-15T13:04:47.235000Z', 'edited': '2014-12-22T17:35:44.795405Z', 'url': 'https://swapi.co/api/starships/22/'}], 'created': '2014-12-09T13:50:51.644000Z', 'edited': '2014-12-20T21:17:56.891000Z', 'url': {'name': 'Luke Skywalker', 'height': 172, 'mass': 77, 'hair_color': 'blond', 'skin_color': 'fair', 'eye_color': 'blue', 'birth_year': '19BBY', 'gender': 'male', 'homeworld': 'https://swapi.co/api/planets/1/', 'films': ['https://swapi.co/api/films/2/', 'https://swapi.co/api/films/6/', 'https://swapi.co/api/films/3/', 'https://swapi.co/api/films/1/', 'https://swapi.co/api/films/7/'], 'species': ['https://swapi.co/api/species/1/'], 'vehicles': ['https://swapi.co/api/vehicles/14/', 'https://swapi.co/api/vehicles/30/'], 'starships': ['https://swapi.co/api/starships/12/', 'https://swapi.co/api/starships/22/'], 'created': '2014-12-09T13:50:51.644000Z', 'edited': '2014-12-20T21:17:56.891000Z', 'url': 'https://swapi.co/api/people/1/'}} db = client . saildrone antarctica = db . create_collection ( 'antarctica' ) pymongo . common . int validator = \"\"\"{ 'validator': { $jsonSchema: { bsonType: \"object\", required: [ \"name\", \"year\", \"major\", \"address\" ], properties: { name: { bsonType: \"string\", description: \"must be a string and is required\" }, year: { bsonType: \"int\", minimum: 2017, maximum: 3017, description: \"must be an integer in [ 2017, 3017 ] and is required\" }, major: { enum: [ \"Math\", \"English\", \"Computer Science\", \"History\", null ], description: \"can only be one of the enum values and is required\" }, gpa: { bsonType: [ \"double\" ], description: \"must be a double if the field exists\" }, address: { bsonType: \"object\", required: [ \"city\" ], properties: { street: { bsonType: \"string\", description: \"must be a string if the field exists\" }, city: { bsonType: \"string\", \"description\": \"must be a string and is required\" } } } } } } \"\"\" import json json . loads ( '{\"validator\": 1}' ) {'validator': 1} client . list_database_names () ['admin', 'config', 'local', 'saildrone', 'test'] db . list_collection_names () ['antarctica'] antarctica = db . antartica cursor = antarctica . aggregate ([{ \"$match\" : {} }]) from datetime import ( datetime , timedelta ) from random import ( choice , randint , random , uniform , lognormvariate ) def feed_db ( n ): json_body = ( { \"measurement\" : \"starfleet_01\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"speed\" : lognormvariate ( 10 , 3 ), \"consumption\" : uniform ( 0 , 300 ), \"pressure_a\" : 3 + random (), \"status_b\" : choice (( True , False )) } } for i in range ( n ) ) antarctica . insert_many ( json_body ) % timeit feed_db ( 100_000 ) 3.56 s \u00b1 26.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) antarctica . count_documents ({}) 2993003 antarctica . insert_one ( { \"item\" : \"canvas\" , \"qty\" : \"$myMul(100)\" , \"tags\" : [ \"cotton\" ], \"size\" : { \"h\" : 28 , \"w\" : 35.5 , \"uom\" : \"cm\" }}) <pymongo.results.InsertOneResult at 0x7f1d8c753c08> antarctica . aggregate ([{ \"$match\" : {}},{ \"$project\" : { \"qty\" : 1 } }]) . next () {'_id': ObjectId('5d5ef7a5e261536f4714057d'), 'qty': 100} cursor = antarctica . find ({}) list ( cursor ) [{'_id': ObjectId('5d5ef7a5e261536f4714057d'), 'item': 'canvas', 'qty': 100, 'tags': ['cotton'], 'size': {'h': 28, 'w': 35.5, 'uom': 'cm'}}, {'_id': ObjectId('5d5ef86be261536f4714057e'), 'item': 'canvas', 'qty': 'myMul(100)', 'tags': ['cotton'], 'size': {'h': 28, 'w': 35.5, 'uom': 'cm'}}, {'_id': ObjectId('5d5ef897e261536f4714057f'), 'item': 'canvas', 'qty': '$myMul(100)', 'tags': ['cotton'], 'size': {'h': 28, 'w': 35.5, 'uom': 'cm'}}] Replica set: A cluster of MongoDB servers that implements replication and automated failover. MongoDB\u2019s recommended replication strategy. See Replication. Sharded cluster","title":"MongoDB"},{"location":"_nb/%3DPy/04_influxDB/01%20-%20Intro%20InfluxDB/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); InfluxDB is a time series database designed to handle timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics. Key features \u00b6 CLI/HTTP write and query API. Expressive SQL-like query language Schemas don't have to be defined up front and schema preferences may change over time. Tags allow series to be indexed for fast and efficient queries. Retention policies efficiently auto-expire stale data. Plugins support for other data ingestion protocols such as Graphite, collectd, and OpenTSDB. Continuous queries automatically compute aggregate data to make frequent queries more efficient. InfluxDB isn\u2019t fully CRUD The open source edition of InfluxDB runs on a single node, high availability is only available in the InfluxDB Enterprise Edition. Data structure \u00b6 Time series key concepts : - time - a timestamp - is similar to a SQL primary key, - tags , zero to many key-values, contain any metadata about the value, tags are indexed - at least one key-value field set ( `field key identifies the measured element while field value are the measured value itself, e.g. \u201cvalue=0.64\u201d, or \u201ctemperature=21.2\u201d). **fields are not indexed** It\u2019s important to note that fields are not indexed. Queries that use field values as filters must scan all values that match the other conditions in the query. As a result, those queries are not performant relative to queries on tags. In general, fields should not contain commonly-queried metadata. Further concepts: - a measurement acts as a container for tags , fields , and the time column. Assimilable to a SQL table, where the primary index is always time . tags and fields are effectively columns in the table. - a series is the collection of data that share the same retention policy, measurement, and tag set. - a point represents a single data record that has four components: a measurement, tag set, field set, and a timestamp. A point is uniquely identified by its series and timestamp (similar to a row in a SQL database table) Element Optional/Required Description Type (See data types for more information.) Measurement Required The measurement name. InfluxDB accepts one measurement per point. String Tag set Optional All tag key-value pairs for the point. Tag keys and tag values are both strings. Field set Required. Points must have at least one field. All field key-value pairs for the point. Field keys are strings. Field values can be floats, integers, strings, or Booleans. Timestamp Optional. InfluxDB uses the server\u2019s local nanosecond timestamp in UTC if the timestamp is not included with the point. The timestamp for the data point. InfluxDB accepts one timestamp per point. Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API . Data type \u00b6 Datatype Element(s) Description Float Field values IEEE-754 64-bit floating-point numbers. This is the default numerical type. Examples: 1 , 1.0 , 1.e+78 , 1.E+78 . Integer Field values Signed 64-bit integers (-9223372036854775808 to 9223372036854775807). Specify an integer with a trailing i on the number. Example: 1i . String Measurements, tag keys, tag values, field keys, field values Length limit 64KB. Boolean Field values Stores TRUE or FALSE values. TRUE write syntax: [t, T, true, True, TRUE] . FALSE write syntax: [f, F, false, False, FALSE] Timestamp Timestamps Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API . The minimum valid timestamp is -9223372036854775806 or 1677-09-21T00:12:43.145224194Z . The maximum valid timestamp is 9223372036854775806 or 2262-04-11T23:47:16.854775806Z . Python module documentation Example \u00b6 *census*: time butterflies honeybees location scientist 2015-08-18T00:00:00Z 12 23 1 langstroth 2015-08-18T00:00:00Z 1 30 1 perpetua 2015-08-18T00:06:00Z 11 28 1 langstroth 015-08-18T00:06:00Z 3 28 1 perpetua 2015-08-18T05:54:00Z 2 11 2 langstroth 2015-08-18T06:00:00Z 1 10 2 langstroth 2015-08-18T06:06:00Z 8 23 2 perpetua 2015-08-18T06:12:00Z 7 22 2 perpetua **8 field sets** butterflies = 12 honeybees = 23 butterflies = 1 honeybees = 30 butterflies = 11 honeybees = 28 butterflies = 3 honeybees = 28 butterflies = 2 honeybees = 11 butterflies = 1 honeybees = 10 butterflies = 8 honeybees = 23 butterflies = 7 honeybees = 22 **4 tag sets** (different combinations of all the tag key-value pairs) location = 1, scientist = langstroth location = 2, scientist = langstroth location = 1, scientist = perpetua location = 2, scientist = perpetua Arbitrary series number Retention policy Measurement Tag set series 1 autogen census location = 1 , scientist = langstroth series 2 autogen census location = 2 , scientist = langstroth series 3 autogen census location = 1 , scientist = perpetua series 4 autogen census location = 2 , scientist = perpetua from datetime import ( datetime , timedelta ) from random import ( choice , randint , random , uniform , lognormvariate ) from influxdb import ( InfluxDBClient , DataFrameClient ) INFLUXDB_USER = 'telegraf' INFLUXDB_USER_PASSWORD = 'secretpassword' host = 'db.influxdb.app.com' port = 8086 \"\"\"Instantiate a connection to the InfluxDB.\"\"\" user = 'admin' password = 'supersecretpassword' dbname = 'example' dbuser = 'telegraf' dbuser_password = 'secretpassword' client = InfluxDBClient ( host , port , user , password , dbname ) DB init \u00b6 For creating the DB client.create_database(dbname) Define a specific retention policy, drop after 30d, with replica factor of 3 and applied by default to new elements client . create_retention_policy ( 'custom_policy' , '30d' , 3 , default = True ) # For dropping the policy: client.drop_retention_policy('custom_policy', dbname) client . switch_user ( dbuser , dbuser_password ) def feed_db ( n ): json_body = [ { \"measurement\" : \"starfleet_01\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"speed\" : lognormvariate ( 10 , 3 ), \"consumption\" : uniform ( 0 , 300 ), \"pressure_a\" : 3 + random (), \"status_b\" : choice (( True , False )) } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) def feed_db_array ( n ): json_body = [ { \"measurement\" : \"starfleet_02\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"curve_b[0]\" : 10 + random (), \"curve_b[1]\" : choice (( 15 + random (), 4 + random ())), \"curve_b[2]\" : 20 + random (), } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) % timeit feed_db ( 100_000 ) 13.2 s \u00b1 202 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) feed_db_array(1_000_000) Series can be deleted client.delete_series(database=\"example\", measurement=\"starfleet_01\") client . query ( query = 'select count(*) from starfleet_01' ) ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count_consumption': 1866653, 'count_pressure_a': 1866653, 'count_speed': 1866653, 'count_status_b': 1866653}]}) from IPython.core.magic import ( register_line_cell_magic ) @register_line_cell_magic def influxql ( line , cell = None ): \"Magic that works both as %lc magic and as %% lcmagic\" if cell is None : sqlstr = line else : sqlstr = \";\" . join (( op . strip ( \";\" ) for op in cell . strip ( \" \\n \" ) . split ( \" \\n \" ))) + \";\" return client . query ( query = sqlstr ) % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:05Z', 'speed': 774750.9698706511}, {'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}]}) result = % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; result ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}, {'time': '2019-08-19T20:52:25Z', 'speed': 9.829304084029904}]}) % influxql SELECT COUNT ( DISTINCT ( pressure_a )) FROM starfleet_01 ; ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count': 1000000}]}) % influxql SELECT COUNT ( pressure_a ) FROM starfleet_01 GROUP BY time ( 28 d ), region LIMIT 1 ; ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118099}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118282}], '('starfleet_01', {'region': 'Earth'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117988}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117959}]}) % influxql SELECT MEAN ( pressure_a ) FROM starfleet_01 GROUP BY region ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.499534039730786}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.4999780335201867}], '('starfleet_01', {'region': 'Earth'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5005520567253616}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5013387093463155}]}) You can explain query % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE region = 'Andoria' LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 750'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 2160'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 2160'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 5992395'}]}) % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE pressure_a > 3.5 LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 3000'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 8640'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 8640'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 23943525'}]}) res = % influxql SELECT * FROM \"starfleet_01\" WHERE pressure_a > 3.5 LIMIT 50 points = res . get_points ( tags = { \"region\" : \"Andoria\" }) from pandas import DataFrame df = DataFrame ( points ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region spacecraft speed status_b time 0 Archer 80.868742 3.764769 Andoria NX-1707 4.540769e+06 False 2019-07-28T01:35:50Z 1 Archer 64.351610 3.784118 Andoria NX-1707 8.549125e+05 False 2019-07-28T01:36:00Z 2 Archer 60.765559 3.907481 Andoria NX-1702 1.679463e+05 False 2019-07-28T01:36:04Z 3 Archer 252.870551 3.800938 Andoria NX-1706 1.444727e+04 True 2019-07-28T01:36:06Z 4 Kruge 192.871755 3.930994 Andoria NX-1702 4.376490e+03 False 2019-07-28T01:36:08Z df . groupby ( 'spacecraft' ) . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region speed status_b time spacecraft NX-1701 Kirk 226.626952 3.518973 Andoria 6.987498e+05 False 2019-07-28T01:38:44Z NX-1702 Kruge 217.135955 3.952388 Andoria 6.735246e+05 True 2019-07-28T01:37:38Z NX-1704 Kruge 278.172741 3.976138 Andoria 2.770339e+01 True 2019-07-28T01:37:54Z NX-1705 Kirk 197.423732 3.964069 Andoria 4.300635e+04 True 2019-07-28T01:37:42Z NX-1706 Kruge 252.870551 3.800938 Andoria 1.444727e+04 True 2019-07-28T01:37:02Z NX-1707 Kruge 289.899273 3.913948 Andoria 4.540769e+06 True 2019-07-28T01:38:46Z NX-1709 Kruge 272.085362 3.734222 Andoria 1.653500e+05 True 2019-07-28T01:37:30Z NX-1710 Kruge 297.309907 3.875039 Andoria 5.887814e+04 True 2019-07-28T01:38:20Z % influxql SHOW QUERIES ; ResultSet({'('results', None)': [{'qid': 2821, 'query': 'SHOW QUERIES', 'database': 'example', 'duration': '171\u00b5s', 'status': 'running'}]}) % influxql SHOW TAG KEYS ; ResultSet({'('starfleet_01', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}], '('starfleet_02', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}]})","title":"01   Intro InfluxDB"},{"location":"_nb/%3DPy/04_influxDB/01%20-%20Intro%20InfluxDB/#key-features","text":"CLI/HTTP write and query API. Expressive SQL-like query language Schemas don't have to be defined up front and schema preferences may change over time. Tags allow series to be indexed for fast and efficient queries. Retention policies efficiently auto-expire stale data. Plugins support for other data ingestion protocols such as Graphite, collectd, and OpenTSDB. Continuous queries automatically compute aggregate data to make frequent queries more efficient. InfluxDB isn\u2019t fully CRUD The open source edition of InfluxDB runs on a single node, high availability is only available in the InfluxDB Enterprise Edition.","title":"Key features"},{"location":"_nb/%3DPy/04_influxDB/01%20-%20Intro%20InfluxDB/#data-structure","text":"Time series key concepts : - time - a timestamp - is similar to a SQL primary key, - tags , zero to many key-values, contain any metadata about the value, tags are indexed - at least one key-value field set ( `field key identifies the measured element while field value are the measured value itself, e.g. \u201cvalue=0.64\u201d, or \u201ctemperature=21.2\u201d). **fields are not indexed** It\u2019s important to note that fields are not indexed. Queries that use field values as filters must scan all values that match the other conditions in the query. As a result, those queries are not performant relative to queries on tags. In general, fields should not contain commonly-queried metadata. Further concepts: - a measurement acts as a container for tags , fields , and the time column. Assimilable to a SQL table, where the primary index is always time . tags and fields are effectively columns in the table. - a series is the collection of data that share the same retention policy, measurement, and tag set. - a point represents a single data record that has four components: a measurement, tag set, field set, and a timestamp. A point is uniquely identified by its series and timestamp (similar to a row in a SQL database table) Element Optional/Required Description Type (See data types for more information.) Measurement Required The measurement name. InfluxDB accepts one measurement per point. String Tag set Optional All tag key-value pairs for the point. Tag keys and tag values are both strings. Field set Required. Points must have at least one field. All field key-value pairs for the point. Field keys are strings. Field values can be floats, integers, strings, or Booleans. Timestamp Optional. InfluxDB uses the server\u2019s local nanosecond timestamp in UTC if the timestamp is not included with the point. The timestamp for the data point. InfluxDB accepts one timestamp per point. Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API .","title":"Data structure"},{"location":"_nb/%3DPy/04_influxDB/01%20-%20Intro%20InfluxDB/#data-type","text":"Datatype Element(s) Description Float Field values IEEE-754 64-bit floating-point numbers. This is the default numerical type. Examples: 1 , 1.0 , 1.e+78 , 1.E+78 . Integer Field values Signed 64-bit integers (-9223372036854775808 to 9223372036854775807). Specify an integer with a trailing i on the number. Example: 1i . String Measurements, tag keys, tag values, field keys, field values Length limit 64KB. Boolean Field values Stores TRUE or FALSE values. TRUE write syntax: [t, T, true, True, TRUE] . FALSE write syntax: [f, F, false, False, FALSE] Timestamp Timestamps Unix nanosecond timestamp. Specify alternative precisions with the InfluxDB API . The minimum valid timestamp is -9223372036854775806 or 1677-09-21T00:12:43.145224194Z . The maximum valid timestamp is 9223372036854775806 or 2262-04-11T23:47:16.854775806Z . Python module documentation","title":"Data type"},{"location":"_nb/%3DPy/04_influxDB/01%20-%20Intro%20InfluxDB/#example","text":"*census*: time butterflies honeybees location scientist 2015-08-18T00:00:00Z 12 23 1 langstroth 2015-08-18T00:00:00Z 1 30 1 perpetua 2015-08-18T00:06:00Z 11 28 1 langstroth 015-08-18T00:06:00Z 3 28 1 perpetua 2015-08-18T05:54:00Z 2 11 2 langstroth 2015-08-18T06:00:00Z 1 10 2 langstroth 2015-08-18T06:06:00Z 8 23 2 perpetua 2015-08-18T06:12:00Z 7 22 2 perpetua **8 field sets** butterflies = 12 honeybees = 23 butterflies = 1 honeybees = 30 butterflies = 11 honeybees = 28 butterflies = 3 honeybees = 28 butterflies = 2 honeybees = 11 butterflies = 1 honeybees = 10 butterflies = 8 honeybees = 23 butterflies = 7 honeybees = 22 **4 tag sets** (different combinations of all the tag key-value pairs) location = 1, scientist = langstroth location = 2, scientist = langstroth location = 1, scientist = perpetua location = 2, scientist = perpetua Arbitrary series number Retention policy Measurement Tag set series 1 autogen census location = 1 , scientist = langstroth series 2 autogen census location = 2 , scientist = langstroth series 3 autogen census location = 1 , scientist = perpetua series 4 autogen census location = 2 , scientist = perpetua from datetime import ( datetime , timedelta ) from random import ( choice , randint , random , uniform , lognormvariate ) from influxdb import ( InfluxDBClient , DataFrameClient ) INFLUXDB_USER = 'telegraf' INFLUXDB_USER_PASSWORD = 'secretpassword' host = 'db.influxdb.app.com' port = 8086 \"\"\"Instantiate a connection to the InfluxDB.\"\"\" user = 'admin' password = 'supersecretpassword' dbname = 'example' dbuser = 'telegraf' dbuser_password = 'secretpassword' client = InfluxDBClient ( host , port , user , password , dbname )","title":"Example"},{"location":"_nb/%3DPy/04_influxDB/01%20-%20Intro%20InfluxDB/#db-init","text":"For creating the DB client.create_database(dbname) Define a specific retention policy, drop after 30d, with replica factor of 3 and applied by default to new elements client . create_retention_policy ( 'custom_policy' , '30d' , 3 , default = True ) # For dropping the policy: client.drop_retention_policy('custom_policy', dbname) client . switch_user ( dbuser , dbuser_password ) def feed_db ( n ): json_body = [ { \"measurement\" : \"starfleet_01\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"speed\" : lognormvariate ( 10 , 3 ), \"consumption\" : uniform ( 0 , 300 ), \"pressure_a\" : 3 + random (), \"status_b\" : choice (( True , False )) } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) def feed_db_array ( n ): json_body = [ { \"measurement\" : \"starfleet_02\" , \"tags\" : { \"cmdt\" : choice (( \"Archer\" , \"Kirk\" , \"Kruge\" )), \"region\" : choice (( \"Andoria\" , \"Deep Space Nine\" , \"Earth\" , \"Genesis\" )), \"spacecraft\" : f \"NX-17 { randint ( 1 , 10 ) : 02d } \" }, \"time\" : ( datetime . now () - timedelta ( seconds = 2 * ( n - i ))) . strftime ( \"%Y-%m- %d T%H:%M:%S\" ), \"fields\" : { \"curve_b[0]\" : 10 + random (), \"curve_b[1]\" : choice (( 15 + random (), 4 + random ())), \"curve_b[2]\" : 20 + random (), } } for i in range ( n ) ] client . write_points ( json_body , batch_size = 100_000 ) % timeit feed_db ( 100_000 ) 13.2 s \u00b1 202 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) feed_db_array(1_000_000) Series can be deleted client.delete_series(database=\"example\", measurement=\"starfleet_01\") client . query ( query = 'select count(*) from starfleet_01' ) ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count_consumption': 1866653, 'count_pressure_a': 1866653, 'count_speed': 1866653, 'count_status_b': 1866653}]}) from IPython.core.magic import ( register_line_cell_magic ) @register_line_cell_magic def influxql ( line , cell = None ): \"Magic that works both as %lc magic and as %% lcmagic\" if cell is None : sqlstr = line else : sqlstr = \";\" . join (( op . strip ( \";\" ) for op in cell . strip ( \" \\n \" ) . split ( \" \\n \" ))) + \";\" return client . query ( query = sqlstr ) % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:05Z', 'speed': 774750.9698706511}, {'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}]}) result = % influxql SELECT speed FROM starfleet_01 WHERE time > now () - 1 d LIMIT 10 ; result ResultSet({'('starfleet_01', None)': [{'time': '2019-08-19T20:52:07Z', 'speed': 487121.20016297186}, {'time': '2019-08-19T20:52:09Z', 'speed': 18753.305850476325}, {'time': '2019-08-19T20:52:11Z', 'speed': 17151.44033960194}, {'time': '2019-08-19T20:52:13Z', 'speed': 34535.68231005546}, {'time': '2019-08-19T20:52:15Z', 'speed': 17274.816825933573}, {'time': '2019-08-19T20:52:17Z', 'speed': 31681.839234807274}, {'time': '2019-08-19T20:52:19Z', 'speed': 9354.756781762246}, {'time': '2019-08-19T20:52:21Z', 'speed': 6355.474730264167}, {'time': '2019-08-19T20:52:23Z', 'speed': 40375.16829113543}, {'time': '2019-08-19T20:52:25Z', 'speed': 9.829304084029904}]}) % influxql SELECT COUNT ( DISTINCT ( pressure_a )) FROM starfleet_01 ; ResultSet({'('starfleet_01', None)': [{'time': '1970-01-01T00:00:00Z', 'count': 1000000}]}) % influxql SELECT COUNT ( pressure_a ) FROM starfleet_01 GROUP BY time ( 28 d ), region LIMIT 1 ; ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118099}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '2019-07-11T00:00:00Z', 'count': 118282}], '('starfleet_01', {'region': 'Earth'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117988}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '2019-07-11T00:00:00Z', 'count': 117959}]}) % influxql SELECT MEAN ( pressure_a ) FROM starfleet_01 GROUP BY region ResultSet({'('starfleet_01', {'region': 'Andoria'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.499534039730786}], '('starfleet_01', {'region': 'Deep Space Nine'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.4999780335201867}], '('starfleet_01', {'region': 'Earth'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5005520567253616}], '('starfleet_01', {'region': 'Genesis'})': [{'time': '1970-01-01T00:00:00Z', 'mean': 3.5013387093463155}]}) You can explain query % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE region = 'Andoria' LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 750'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 2160'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 2160'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 5992395'}]}) % influxql EXPLAIN SELECT * FROM starfleet_02 WHERE pressure_a > 3.5 LIMIT 1 ResultSet({'('results', None)': [{'QUERY PLAN': 'EXPRESSION: <nil>'}, {'QUERY PLAN': 'AUXILIARY FIELDS: cmdt::tag, \"curve_b[0]\"::float, \"curve_b[1]\"::float, \"curve_b[2]\"::float, region::tag, spacecraft::tag'}, {'QUERY PLAN': 'NUMBER OF SHARDS: 25'}, {'QUERY PLAN': 'NUMBER OF SERIES: 3000'}, {'QUERY PLAN': 'CACHED VALUES: 0'}, {'QUERY PLAN': 'NUMBER OF FILES: 8640'}, {'QUERY PLAN': 'NUMBER OF BLOCKS: 8640'}, {'QUERY PLAN': 'SIZE OF BLOCKS: 23943525'}]}) res = % influxql SELECT * FROM \"starfleet_01\" WHERE pressure_a > 3.5 LIMIT 50 points = res . get_points ( tags = { \"region\" : \"Andoria\" }) from pandas import DataFrame df = DataFrame ( points ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region spacecraft speed status_b time 0 Archer 80.868742 3.764769 Andoria NX-1707 4.540769e+06 False 2019-07-28T01:35:50Z 1 Archer 64.351610 3.784118 Andoria NX-1707 8.549125e+05 False 2019-07-28T01:36:00Z 2 Archer 60.765559 3.907481 Andoria NX-1702 1.679463e+05 False 2019-07-28T01:36:04Z 3 Archer 252.870551 3.800938 Andoria NX-1706 1.444727e+04 True 2019-07-28T01:36:06Z 4 Kruge 192.871755 3.930994 Andoria NX-1702 4.376490e+03 False 2019-07-28T01:36:08Z df . groupby ( 'spacecraft' ) . max () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cmdt consumption pressure_a region speed status_b time spacecraft NX-1701 Kirk 226.626952 3.518973 Andoria 6.987498e+05 False 2019-07-28T01:38:44Z NX-1702 Kruge 217.135955 3.952388 Andoria 6.735246e+05 True 2019-07-28T01:37:38Z NX-1704 Kruge 278.172741 3.976138 Andoria 2.770339e+01 True 2019-07-28T01:37:54Z NX-1705 Kirk 197.423732 3.964069 Andoria 4.300635e+04 True 2019-07-28T01:37:42Z NX-1706 Kruge 252.870551 3.800938 Andoria 1.444727e+04 True 2019-07-28T01:37:02Z NX-1707 Kruge 289.899273 3.913948 Andoria 4.540769e+06 True 2019-07-28T01:38:46Z NX-1709 Kruge 272.085362 3.734222 Andoria 1.653500e+05 True 2019-07-28T01:37:30Z NX-1710 Kruge 297.309907 3.875039 Andoria 5.887814e+04 True 2019-07-28T01:38:20Z % influxql SHOW QUERIES ; ResultSet({'('results', None)': [{'qid': 2821, 'query': 'SHOW QUERIES', 'database': 'example', 'duration': '171\u00b5s', 'status': 'running'}]}) % influxql SHOW TAG KEYS ; ResultSet({'('starfleet_01', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}], '('starfleet_02', None)': [{'tagKey': 'cmdt'}, {'tagKey': 'region'}, {'tagKey': 'spacecraft'}]})","title":"DB init"},{"location":"_nb/%3DPy/05_pandas_xarray/00_fetch_data/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import numpy as np import pandas as pd from pandas import DataFrame pd . options . display . max_rows = 10 Create a HDF Store for serializing the data store = pd . HDFStore ( 'store.h5' ) ADAC - df0 mit Abgasnorm Euro 6d-Temp, Euro 6d \u00b6 ADAC - df0 mit Abgasnorm Euro 6d-Temp, Euro 6d url = \"https://www.adac.de/rund-ums-fahrzeug/auto-kaufen-verkaufen/neuwagenkauf/euro-6d-temp-modelle\" res = pd . read_html ( url , header = 0 ) Concatenate all tables and reindex df0 = pd . concat (( elt for elt in res ), ignore_index = True ) df0 . rename ({ 'Markt- einf\u00fchrung' : 'Markteinfuehrung' , 'Hubraum in ccm' : 'Hubraum' , 'Leistung in KW' : 'Leistung' }, axis = 1 , inplace = True ) Convert Markteinfuehrung col to datetime For date manipulation, see for ex.https://www.python-kurs.eu/python3_time_and_date.php import locale locale . setlocale ( locale . LC_ALL , 'de_DE.UTF-8' ) pat = r \"^(\\w {3} ).*(\\d {2} )$\" repl = lambda m : f \" { m . group ( 1 ) } { m . group ( 2 ) } \" df0 . Markteinfuehrung = df0 . Markteinfuehrung \\ . str . replace ( '^v' , 'Nov' ) \\ . str . replace ( '(Mrz|Mar)' , 'M\u00e4r' ) \\ . str . replace ( pat , repl ) df0 . Markteinfuehrung = pd . to_datetime ( df0 . Markteinfuehrung , format = '%b %y' , errors = 'coerce' ) df0 = df0 . astype ({ 'Hersteller' : 'category' , 'Modell' : str , 'Motorart' : 'category' , 'Abgasnorm' : 'category' }) df0 . dtypes Hersteller category Modell object Motorart category Hubraum int64 Leistung int64 Abgasnorm category Markteinfuehrung datetime64[ns] dtype: object df0 . sample ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Hersteller Modell Motorart Hubraum Leistung Abgasnorm Markteinfuehrung 2677 VW Caddy Maxi Kombi 2.0 TDI SCR Diesel 1968 75 Euro 6d-TEMP-EVAP 2018-10-01 690 Dacia Dokker Blue dCi 75 Diesel 1461 55 Euro 6d-TEMP-EVAP 2018-09-01 1226 Lamborghini Hurac\u00e1n LP580-2 Otto 5204 426 Euro 6d-TEMP 2018-09-01 df0 . memory_usage () Index 128 Hersteller 4506 Modell 22992 Motorart 3082 Hubraum 22992 Leistung 22992 Abgasnorm 3250 Markteinfuehrung 22992 dtype: int64 Save to the store store . put ( 'ADAC' , df0 , format = 'table' ) print ( store . info ()) <class 'pandas.io.pytables.HDFStore'> File path: store.h5 /ADAC frame_table (typ->appendable,nrows->2874,ncols->7,indexers->[index],dc->[]) /ADAC/meta/values_block_0/meta series_table (typ->appendable,nrows->44,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_1/meta series_table (typ->appendable,nrows->6,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_2/meta series_table (typ->appendable,nrows->7,ncols->1,indexers->[index],dc->[values]) store . get ( '/ADAC' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Hersteller Modell Motorart Hubraum Leistung Abgasnorm Markteinfuehrung 0 Abarth 595 Otto 1368 107 Euro 6d-TEMP-EVAP 2018-09-01 1 Abarth 595 Pista Otto 1368 118 Euro 6d-TEMP-EVAP 2018-09-01 2 Abarth 595 Turismo Otto 1368 121 Euro 6d-TEMP-EVAP 2018-09-01 3 Abarth 595 Competizione Otto 1368 132 Euro 6d-TEMP-EVAP 2018-09-01 4 Abarth 595C Otto 1368 107 Euro 6d-TEMP-EVAP 2018-09-01 ... ... ... ... ... ... ... ... 2869 VW Touran 2.0 TDI SCR Diesel 1968 140 Euro 6d-TEMP 2019-01-01 2870 VW up! 1.0 Otto 999 44 Euro 6d-TEMP 2018-08-01 2871 VW up! 1.0 Otto 999 55 Euro 6d-TEMP 2018-08-01 2872 VW up! 1.0 TSI Otto 999 66 Euro 6d-TEMP 2018-09-01 2873 VW up! GTI Otto 999 85 Euro 6d-TEMP 2018-01-01 2874 rows \u00d7 7 columns Umweltdaten N\u00fcrnberg \u00b6 Messstation Jakobsplatz - Stadt N\u00fcrnberg from datetime import datetime today = datetime.today() from string import Template url = Template((\"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/\" \"archiv/csv-export/SUN/ {where}/ {where}/ /\" \"individuell/ {first}/ {first}/ /export.csv\")) make_url = lambda where, what, year: url.substitute( where=where, what=what, first=f\"01.01.{year}\", until=f'31.12.{year}') def get_air_data_in_nuremberg ( where : str , what : str , year : int ): opts = { 'skiprows' : range ( 0 , 10 ), 'encoding' : 'ISO-8859-1' , 'sep' : ';' , 'parse_dates' : [ 0 ], 'index_col' : 0 , 'na_values' : [ '-' ] } url = ( f \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/\" f \"archiv/csv-export/SUN/ { where } / { what } /\" f \"individuell/01.01. { year } /31.12. { year } /export.csv\" ) return pd . read_csv ( url , ** opts ) df = pd . concat ([ get_air_data_in_nuremberg ( where = \"nuernberg-jakobsplatz\" , what = \"stickstoffdioxid\" , year = y ) for y in range ( 2005 , 2020 )]) http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / messstation - jakobsplatz / feinstaub - pm10 / csv - export / SUN / nuernberg - jakobsplatz / staubpartikel - pm10 / 7 - Tages - Ansicht / export . csv Nu2HD5 = { 'nuernberg-jakobsplatz' : 'JAKOBSPALTZ' , 'nuernberg-flugfeld' : 'FLUGFELD' , 'stickstoffmonoxid' : 'NO' , 'stickstoffdioxid' : 'NO2' , 'ozon' : 'O3' , 'staubpartikel-pm10' : 'PM10' , 'staub-pm-25' : 'PM25' } from itertools import product for what , where in product ([ k for k in Nu2HD5 . keys () if 'nuernberg' not in k ], [ k for k in Nu2HD5 . keys () if 'nuernberg' in k ]): try : df = pd . concat ([ get_air_data_in_nuremberg ( where = where , what = what , year = y ) for y in range ( 2005 , 2020 )]) tableName = f \"AIR/ { Nu2HD5 [ where ] } / { Nu2HD5 [ what ] } \" store . put ( tableName , df , format = 'table' ) except Exception as e : print ( e ) print ( store . info ()) <class 'pandas.io.pytables.HDFStore'> File path: store.h5 /ADAC frame_table (typ->appendable,nrows->2874,ncols->7,indexers->[index],dc->[]) /ADAC/meta/values_block_0/meta series_table (typ->appendable,nrows->44,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_1/meta series_table (typ->appendable,nrows->6,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_2/meta series_table (typ->appendable,nrows->7,ncols->1,indexers->[index],dc->[values]) /AIR/FLUGFELD/NO frame_table (typ->appendable,nrows->127817,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/NO2 frame_table (typ->appendable,nrows->127779,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/O3 frame_table (typ->appendable,nrows->129541,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/PM10 frame_table (typ->appendable,nrows->130374,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/PM25 frame_table (typ->appendable,nrows->103667,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/NO frame_table (typ->appendable,nrows->123168,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/NO2 frame_table (typ->appendable,nrows->123353,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/O3 frame_table (typ->appendable,nrows->125972,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/PM10 frame_table (typ->appendable,nrows->126952,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/PM25 frame_table (typ->appendable,nrows->87343,ncols->1,indexers->[index],dc->[]) df1 = store . get ( '/AIR/JAKOBSPALTZ/O3' ) df1 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) Datum/Zeit 2005-05-24 21:00:00 59.0 2005-01-06 00:00:00 57.0 2005-01-06 01:00:00 49.0 2005-01-06 02:00:00 51.0 2005-01-06 03:00:00 34.0 df1 . groupby ( df1 . index . weekday ) . boxplot ( layout = ( 1 , 7 ), figsize = ( 16 , 6 )); with pandas \u00b6 dfk = pd . read_csv ( make_url ( where = \"nuernberg-jakobsplatz\" , what = \"stickstoffdioxid\" , year = 2019 ), ** opts ) The data will be retrieved year for year, see no \" Da bei den Abfragen m\u00f6glicherweise gro\u00dfe Datenmengen entstehen, wird empfohlen maximal 12 Monate als Bezugszeitraum anzugeben und bei Bedarf mehrere Abfragen durchzuf\u00fchren. \" with pyarrow \u00b6 from pyarrow import csv from pyarrow.csv import ( ReadOptions , ParseOptions , ConvertOptions ) read_opts = ReadOptions ( skip_rows = 13 , column_names = [ \"Date\" , \"NO2\" ]) parse_opts = ParseOptions ( delimiter = ';' ) conv_opts = ConvertOptions ( column_types = { 'NO2' : pa . int32 ()}, null_values = [ \"-\" ]) # Date are not an # 'Date': pa.timestamp('s'), from io import ( BytesIO , StringIO ) bio = BytesIO ( r . content . decode ( encoding = 'ISO-8859-1' ) . encode ()) table = csv . read_csv ( bio , read_options = read_opts , parse_options = parse_opts , convert_options = conv_opts ) table . schema Date: string NO2: int32 dfk . boxplot () <matplotlib.axes._subplots.AxesSubplot at 0x7f6fe5bd2710> import pyarrow as pa from pyarrow import csv from requests import Session year = 1 u1 = url . substitute ( what = \"stickstoffdioxid\" , first = today . strftime ( ' %d .%m.' ) + str ( today . year - year - 1 ), until = today . strftime ( ' %d .%m.' ) + str ( today . year - year )) s = Session () r = s . get ( u1 ) table . schema Date: string NO2: int32 table . to_pandas () Date object NO2 float64 dtype: object df1 = pd . concat (( pd . read_csv ( url . substitute ( what = \"stickstoffdioxid\" , first = today . strftime ( ' %d .%m.' ) + str ( today . year - year - 1 ), until = today . strftime ( ' %d .%m.' ) + str ( today . year - year )), ** opts ) for year in range ( 15 ))) df1 . info () <class 'pandas.core.frame.DataFrame'> DatetimeIndex: 112269 entries, 2019-01-01 01:00:00 to 2005-11-27 23:00:00 Data columns (total 1 columns): Wert (\u00b5g/m\u00b3) 111358 non-null float64 dtypes: float64(1) memory usage: 1.7 MB df1 [ 'Wert (\u00b5g/m\u00b3)' ] . metadata = {} df1 [ 'Wert (\u00b5g/m\u00b3)' ] . iloc [: 30 ] Datum/Zeit 2019-01-01 01:00:00 30.0 2019-01-01 02:00:00 29.0 2019-01-01 03:00:00 24.0 2019-01-01 04:00:00 22.0 2019-01-01 05:00:00 20.0 ... 2019-02-01 02:00:00 8.0 2019-02-01 03:00:00 6.0 2019-02-01 04:00:00 7.0 2019-02-01 05:00:00 6.0 2019-02-01 06:00:00 8.0 Name: Wert (\u00b5g/m\u00b3), Length: 30, dtype: float64 week_df = df1 . groupby ( df1 . index . weekday ) . mean () df1 . groupby ( df1 . index . weekday ) . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Wert (\u00b5g/m\u00b3) count mean std min 25% 50% 75% max Datum/Zeit 0 15849.0 30.795003 17.231654 1.0 18.0 27.0 40.0 133.0 1 15949.0 31.919744 17.604728 0.0 19.0 28.0 41.0 129.0 2 15943.0 32.267578 17.488338 0.0 19.0 29.0 42.0 148.0 3 15954.0 32.525385 17.798968 1.0 19.0 29.0 42.0 135.0 4 15959.0 31.770725 17.249226 -1.0 19.0 28.0 41.0 135.0 5 15816.0 28.693349 15.996195 0.0 17.0 25.0 37.0 129.0 6 15888.0 25.967082 16.686306 0.0 14.0 22.0 34.0 128.0 \" Although fine particle OC concentrations did not correlate with day of the week, EC concentrations showed a significant weekly pattern, with the highest concentration during the middle of the workweek and the lowest concentration on Sundays. \" https://www.ncbi.nlm.nih.gov/pubmed/15303295 df1 . groupby ( df1 . index . weekday ) . boxplot ( layout = ( 1 , 7 ), figsize = ( 16 , 6 )); gp = df1 . groupby ( df1 . index . weekday ) % timeit df1 [ df1 . index . weekday == 2 ] 7.61 ms \u00b1 75.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) % timeit df1 . groupby ( df1 . index . weekday ) . get_group ( 2 ) 10.2 ms \u00b1 73.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) df_wend = df1 [ df1 . index . weekday == 2 ] df_wend . groupby ( df_wend . index . hour ) . mean () . plot () <matplotlib.axes._subplots.AxesSubplot at 0x7f6ff53afc88> import seaborn as sns df_wend_h = df_wend . groupby ( df_wend . index . hour ) df_wend_h . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Wert (\u00b5g/m\u00b3) count mean std min 25% 50% 75% max Datum/Zeit 0 667.0 35.112444 19.408852 0.0 20.0 30.0 46.00 117.0 1 665.0 32.306767 18.458983 0.0 18.0 28.0 44.00 105.0 2 665.0 29.712782 17.153460 0.0 16.0 25.0 40.00 94.0 3 666.0 28.234234 15.617778 0.0 16.0 25.0 37.00 92.0 4 667.0 28.008996 14.758310 1.0 16.0 25.0 37.00 86.0 ... ... ... ... ... ... ... ... ... 19 667.0 35.962519 18.899770 6.0 21.0 32.0 46.00 144.0 20 668.0 38.938623 19.969410 6.0 25.0 36.0 48.25 148.0 21 666.0 39.978979 20.332119 4.0 25.0 36.0 50.00 125.0 22 665.0 40.756391 20.704699 6.0 26.0 37.0 52.00 124.0 23 665.0 39.533835 20.385202 5.0 24.0 36.0 51.00 118.0 24 rows \u00d7 8 columns sns . pointplot ( x = \"day\" , y = \"tip\" , data = tips , ci = 68 ) df_wend_h . boxplot ( subplots = False , figsize = ( 16 , 6 ), column = list ( df_wend_h . groups . keys ())) <matplotlib.axes._subplots.AxesSubplot at 0x7f6fe55677b8> df_wend_h . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Wert (\u00b5g/m\u00b3) count mean std min 25% 50% 75% max Datum/Zeit 0 667.0 35.112444 19.408852 0.0 20.0 30.0 46.00 117.0 1 665.0 32.306767 18.458983 0.0 18.0 28.0 44.00 105.0 2 665.0 29.712782 17.153460 0.0 16.0 25.0 40.00 94.0 3 666.0 28.234234 15.617778 0.0 16.0 25.0 37.00 92.0 4 667.0 28.008996 14.758310 1.0 16.0 25.0 37.00 86.0 ... ... ... ... ... ... ... ... ... 19 667.0 35.962519 18.899770 6.0 21.0 32.0 46.00 144.0 20 668.0 38.938623 19.969410 6.0 25.0 36.0 48.25 148.0 21 666.0 39.978979 20.332119 4.0 25.0 36.0 50.00 125.0 22 665.0 40.756391 20.704699 6.0 26.0 37.0 52.00 124.0 23 665.0 39.533835 20.385202 5.0 24.0 36.0 51.00 118.0 24 rows \u00d7 8 columns df_wend_h . boxplot ( by = 'Datum/Zeit' , figsize = ( 16 , 10 )); --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-204-ee7cf848b6f4> in <module> ----> 1 df_wend_h . boxplot ( by = 'Datum/Zeit' , figsize = ( 16 , 10 ) ) ; /opt/conda/lib/python3.7/site-packages/pandas/plotting/_core.py in boxplot_frame_groupby (grouped, subplots, column, fontsize, rot, grid, ax, figsize, layout, sharex, sharey, **kwds) 498 sharex = sharex , 499 sharey = sharey , --> 500 ** kwds 501 ) 502 /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in boxplot_frame_groupby (grouped, subplots, column, fontsize, rot, grid, ax, figsize, layout, sharex, sharey, **kwds) 390 for ( key , group ) , ax in zip ( grouped , axes ) : 391 d = group.boxplot( --> 392 ax = ax , column = column , fontsize = fontsize , rot = rot , grid = grid , ** kwds 393 ) 394 ax . set_title ( pprint_thing ( key ) ) /opt/conda/lib/python3.7/site-packages/pandas/plotting/_core.py in boxplot_frame (self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds) 418 layout = layout , 419 return_type = return_type , --> 420 ** kwds 421 ) 422 /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in boxplot_frame (self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds) 353 layout = layout , 354 return_type = return_type , --> 355 ** kwds 356 ) 357 plt . draw_if_interactive ( ) /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in boxplot (data, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds) 300 ax = ax , 301 layout = layout , --> 302 return_type = return_type , 303 ) 304 else : /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in _grouped_plot_by_column (plotf, data, columns, by, numeric_only, grid, figsize, ax, layout, return_type, **kwargs) 205 gp_col = grouped [ col ] 206 keys , values = zip ( * gp_col ) --> 207 re_plotf = plotf ( keys , values , ax , ** kwargs ) 208 ax . set_title ( col ) 209 ax . set_xlabel ( pprint_thing ( by ) ) /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in plot_group (keys, values, ax) 262 keys = [ pprint_thing ( x ) for x in keys ] 263 values = [ np . asarray ( remove_na_arraylike ( v ) ) for v in values ] --> 264 bp = ax . boxplot ( values , ** kwds ) 265 if fontsize is not None : 266 ax . tick_params ( axis = \"both\" , labelsize = fontsize ) /opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper (*args, **kwargs) 305 f\"for the old name will be dropped %(removal)s.\") 306 kwargs [ new ] = kwargs . pop ( old ) --> 307 return func ( * args , ** kwargs ) 308 309 # wrapper() must keep the same documented signature as func(): if we /opt/conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner (ax, data, *args, **kwargs) 1599 def inner ( ax , * args , data = None , ** kwargs ) : 1600 if data is None : -> 1601 return func ( ax , * map ( sanitize_sequence , args ) , ** kwargs ) 1602 1603 bound = new_sig . bind ( ax , * args , ** kwargs ) /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in boxplot (self, x, notch, sym, vert, whis, positions, widths, patch_artist, bootstrap, usermedians, conf_intervals, meanline, showmeans, showcaps, showbox, showfliers, boxprops, labels, flierprops, medianprops, meanprops, capprops, whiskerprops, manage_ticks, autorange, zorder) 3770 meanline = meanline , showfliers = showfliers , 3771 capprops = capprops , whiskerprops = whiskerprops , -> 3772 manage_ticks=manage_ticks, zorder=zorder) 3773 return artists 3774 /opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper (*args, **kwargs) 305 f\"for the old name will be dropped %(removal)s.\") 306 kwargs [ new ] = kwargs . pop ( old ) --> 307 return func ( * args , ** kwargs ) 308 309 # wrapper() must keep the same documented signature as func(): if we /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in bxp (self, bxpstats, positions, widths, vert, patch_artist, shownotches, showmeans, showcaps, showbox, showfliers, boxprops, whiskerprops, flierprops, medianprops, capprops, meanprops, meanline, manage_ticks, zorder) 4094 4095 # draw the medians -> 4096 medians . extend ( doplot ( med_x , med_y , ** final_medianprops ) ) 4097 4098 # maybe draw the means /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in doplot (*args, **kwargs) 3996 if vert : 3997 def doplot ( * args , ** kwargs ) : -> 3998 return self . plot ( * args , ** kwargs ) 3999 4000 def dopatch ( xs , ys , ** kwargs ) : /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in plot (self, scalex, scaley, data, *args, **kwargs) 1667 for line in lines : 1668 self . add_line ( line ) -> 1669 self . autoscale_view ( scalex = scalex , scaley = scaley ) 1670 return lines 1671 /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in autoscale_view (self, tight, scalex, scaley) 2493 handle_single_axis( 2494 scalex , self . _autoscaleXon , self . _shared_x_axes , 'intervalx' , -> 2495 'minposx', self.xaxis, self._xmargin, x_stickies, self.set_xbound) 2496 handle_single_axis( 2497 scaley , self . _autoscaleYon , self . _shared_y_axes , 'intervaly' , /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in handle_single_axis (scale, autoscaleon, shared_axes, interval, minpos, axis, margin, stickies, set_bound) 2460 x0 , x1 = getattr ( bb , interval ) 2461 locator = axis . get_major_locator ( ) -> 2462 x0 , x1 = locator . nonsingular ( x0 , x1 ) 2463 2464 # Add the margin in figure space and then transform back, to handle /opt/conda/lib/python3.7/site-packages/matplotlib/ticker.py in nonsingular (self, v0, v1) 1523 def nonsingular ( self , v0 , v1 ) : 1524 \"\"\"Expand a range as needed to avoid singularities.\"\"\" -> 1525 return mtransforms . nonsingular ( v0 , v1 , expander = .05 ) 1526 1527 def view_limits ( self , vmin , vmax ) : /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in nonsingular (vmin, vmax, expander, tiny, increasing) 2825 swapped = True 2826 -> 2827 maxabsvalue = max ( abs ( vmin ) , abs ( vmax ) ) 2828 if maxabsvalue < ( 1e6 / tiny ) * np . finfo ( float ) . tiny : 2829 vmin = - expander KeyboardInterrupt : Error in callback <function flush_figures at 0x7f6ff8de0d90> (for post_execute): --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py in flush_figures () 115 # ignore the tracking, just draw and close all figures 116 try : --> 117 return show ( True ) 118 except Exception as e : 119 # safely show traceback if in IPython, else raise /opt/conda/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py in show (close, block) 37 display( 38 figure_manager . canvas . figure , ---> 39 metadata = _fetch_figure_metadata ( figure_manager . canvas . figure ) 40 ) 41 finally : /opt/conda/lib/python3.7/site-packages/IPython/core/display.py in display (include, exclude, metadata, transient, display_id, *objs, **kwargs) 311 publish_display_data ( data = obj , metadata = metadata , ** kwargs ) 312 else : --> 313 format_dict , md_dict = format ( obj , include = include , exclude = exclude ) 314 if not format_dict : 315 # nothing to display (e.g. _ipython_display_ took over) /opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in format (self, obj, include, exclude) 178 md = None 179 try : --> 180 data = formatter ( obj ) 181 except : 182 # FIXME: log the exception </opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-9> in __call__ (self, obj) /opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in catch_format_error (method, self, *args, **kwargs) 222 \"\"\"show traceback on failed format call\"\"\" 223 try : --> 224 r = method ( self , * args , ** kwargs ) 225 except NotImplementedError : 226 # don't warn on NotImplementedErrors /opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in __call__ (self, obj) 339 pass 340 else : --> 341 return printer ( obj ) 342 # Finally look for special method names 343 method = get_real_method ( obj , self . print_method ) /opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda> (fig) 242 243 if 'png' in formats : --> 244 png_formatter . for_type ( Figure , lambda fig : print_figure ( fig , 'png' , ** kwargs ) ) 245 if 'retina' in formats or 'png2x' in formats : 246 png_formatter . for_type ( Figure , lambda fig : retina_figure ( fig , ** kwargs ) ) /opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure (fig, fmt, bbox_inches, **kwargs) 126 127 bytes_io = BytesIO ( ) --> 128 fig . canvas . print_figure ( bytes_io , ** kw ) 129 data = bytes_io . getvalue ( ) 130 if fmt == 'svg' : /opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure (self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs) 2058 bbox_artists = kwargs . pop ( \"bbox_extra_artists\" , None ) 2059 bbox_inches = self.figure.get_tightbbox(renderer, -> 2060 bbox_extra_artists=bbox_artists) 2061 pad = kwargs . pop ( \"pad_inches\" , None ) 2062 if pad is None : /opt/conda/lib/python3.7/site-packages/matplotlib/figure.py in get_tightbbox (self, renderer, bbox_extra_artists) 2386 return self . bbox_inches 2387 -> 2388 _bbox = Bbox . union ( bb ) 2389 2390 bbox_inches = TransformedBbox(_bbox, /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in union (bboxes) 702 x0 = np . min ( [ bbox . xmin for bbox in bboxes ] ) 703 x1 = np . max ( [ bbox . xmax for bbox in bboxes ] ) --> 704 y0 = np . min ( [ bbox . ymin for bbox in bboxes ] ) 705 y1 = np . max ( [ bbox . ymax for bbox in bboxes ] ) 706 return Bbox ( [ [ x0 , y0 ] , [ x1 , y1 ] ] ) /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in <listcomp> (.0) 702 x0 = np . min ( [ bbox . xmin for bbox in bboxes ] ) 703 x1 = np . max ( [ bbox . xmax for bbox in bboxes ] ) --> 704 y0 = np . min ( [ bbox . ymin for bbox in bboxes ] ) 705 y1 = np . max ( [ bbox . ymax for bbox in bboxes ] ) 706 return Bbox ( [ [ x0 , y0 ] , [ x1 , y1 ] ] ) /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in ymin (self) 353 def ymin ( self ) : 354 \"\"\"The bottom edge of the bounding box.\"\"\" --> 355 return np . min ( self . get_points ( ) [ : , 1 ] ) 356 357 @ property <__array_function__ internals> in amin (*args, **kwargs) /opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in amin (a, axis, out, keepdims, initial, where) 2744 \"\"\" 2745 return _wrapreduction(a, np.minimum, 'min', axis, None, out, -> 2746 keepdims=keepdims, initial=initial, where=where) 2747 2748 /opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapreduction (obj, ufunc, method, axis, dtype, out, **kwargs) 88 return reduction ( axis = axis , out = out , ** passkwargs ) 89 ---> 90 return ufunc . reduce ( obj , axis , dtype , out , ** passkwargs ) 91 92 KeyboardInterrupt : df1 . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) count 111358.000000 mean 30.567341 std 17.305318 min -1.000000 25% 18.000000 50% 27.000000 75% 40.000000 max 148.000000 np . arange ( - 1 , 150 , 20 ) . reshape ( 1 , 8 ) . shape (1, 8) df1 . iloc [:, 0 ] . to_numpy () array([30., 29., 24., ..., 38., 33., 39.]) cut = pd . qcut ( df1 . iloc [:, 0 ] . to_numpy (), q = 10 ) df1 . groupby ( cut ) . mean () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) (-1.001, 12.0] 9.379547 (12.0, 16.0] 14.562010 (16.0, 19.0] 17.998225 (19.0, 23.0] 21.479578 (23.0, 27.0] 25.477695 (27.0, 31.0] 29.440277 (31.0, 37.0] 34.399984 (37.0, 44.0] 40.804197 (44.0, 54.0] 49.080055 (54.0, 148.0] 67.818124 week_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) Datum/Zeit 0 30.795003 1 31.919744 2 32.267578 3 32.525385 4 31.770725 5 28.693349 6 25.967082 df1 . plot () <matplotlib.axes._subplots.AxesSubplot at 0x7feedcd0e780> http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / stickstoffmonoxid / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / lufttemperatur - aussen / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / stickstoffdioxid / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / staub - pm - 25 / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / staubpartikel - pm10 / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / staubpartikel - pm10 / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv urls = { 'stickstoffmonoxid' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/stickstoffmonoxid/csv-export/SUN/nuernberg-jakobsplatz/stickstoffmonoxid/7-Tages-Ansicht/export.csv\" , 'stickstoffdioxid' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/stickstoffdioxid/csv-export/SUN/nuernberg-jakobsplatz/stickstoffdioxid/7-Tages-Ansicht/export.csv\" , 'ozon' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/ozon/csv-export/SUN/nuernberg-jakobsplatz/ozon/7-Tages-Ansicht/export.csv\" , 'pm10' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/feinstaub-pm10/csv-export/SUN/nuernberg-jakobsplatz/staubpartikel-pm10/7-Tages-Ansicht/export.csv\" , 'pm25' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/feinstaub-pm25/csv-export/SUN/nuernberg-jakobsplatz/staub-pm-25/7-Tages-Ansicht/export.csv\" } opts = { 'skiprows' : range ( 10 ), 'encoding' : 'ISO-8859-1' , 'sep' : \";\" , 'index_col' : 0 } res = { comp : pd . read_csv ( url , ** opts ) for comp , url in urls . items ()} http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / stickstoffdioxid / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv [ df . shape for df in res . values ()] [(187, 1), (187, 1), (187, 1), (187, 1), (187, 1)] res [ 'ozon' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) Datum/Zeit 20.11.2019 01:00 23 20.11.2019 02:00 31 20.11.2019 03:00 32 20.11.2019 04:00 25 20.11.2019 05:00 27 ... ... 27.11.2019 15:00 12 27.11.2019 16:00 8 27.11.2019 17:00 6 27.11.2019 18:00 3 27.11.2019 19:00 - 187 rows \u00d7 1 columns import stumpy val = dfk . dropna () . to_numpy () . ravel () st = stumpy . stump ( val , val . size // 330 ) import matplotlib.pyplot as plt fig , axs = plt . subplots ( 2 , sharex = True , figsize = ( 16 , 9 ), gridspec_kw = { 'hspace' : 0 }) plt . suptitle ( 'Discord (Anomaly/Novelty) Discovery' , fontsize = '30' ) axs [ 0 ] . plot ( val ) axs [ 1 ] . set_xlabel ( 'Time' , fontsize = '20' ) axs [ 1 ] . set_ylabel ( 'Matrix Profile' , fontsize = '20' ) #axs[1].axvline(x=3864, linestyle=\"dashed\") axs [ 1 ] . plot ( st [:, 0 ]) [<matplotlib.lines.Line2D at 0x7f03c4501860>]","title":"00 fetch data"},{"location":"_nb/%3DPy/05_pandas_xarray/00_fetch_data/#adac-df0-mit-abgasnorm-euro-6d-temp-euro-6d","text":"ADAC - df0 mit Abgasnorm Euro 6d-Temp, Euro 6d url = \"https://www.adac.de/rund-ums-fahrzeug/auto-kaufen-verkaufen/neuwagenkauf/euro-6d-temp-modelle\" res = pd . read_html ( url , header = 0 ) Concatenate all tables and reindex df0 = pd . concat (( elt for elt in res ), ignore_index = True ) df0 . rename ({ 'Markt- einf\u00fchrung' : 'Markteinfuehrung' , 'Hubraum in ccm' : 'Hubraum' , 'Leistung in KW' : 'Leistung' }, axis = 1 , inplace = True ) Convert Markteinfuehrung col to datetime For date manipulation, see for ex.https://www.python-kurs.eu/python3_time_and_date.php import locale locale . setlocale ( locale . LC_ALL , 'de_DE.UTF-8' ) pat = r \"^(\\w {3} ).*(\\d {2} )$\" repl = lambda m : f \" { m . group ( 1 ) } { m . group ( 2 ) } \" df0 . Markteinfuehrung = df0 . Markteinfuehrung \\ . str . replace ( '^v' , 'Nov' ) \\ . str . replace ( '(Mrz|Mar)' , 'M\u00e4r' ) \\ . str . replace ( pat , repl ) df0 . Markteinfuehrung = pd . to_datetime ( df0 . Markteinfuehrung , format = '%b %y' , errors = 'coerce' ) df0 = df0 . astype ({ 'Hersteller' : 'category' , 'Modell' : str , 'Motorart' : 'category' , 'Abgasnorm' : 'category' }) df0 . dtypes Hersteller category Modell object Motorart category Hubraum int64 Leistung int64 Abgasnorm category Markteinfuehrung datetime64[ns] dtype: object df0 . sample ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Hersteller Modell Motorart Hubraum Leistung Abgasnorm Markteinfuehrung 2677 VW Caddy Maxi Kombi 2.0 TDI SCR Diesel 1968 75 Euro 6d-TEMP-EVAP 2018-10-01 690 Dacia Dokker Blue dCi 75 Diesel 1461 55 Euro 6d-TEMP-EVAP 2018-09-01 1226 Lamborghini Hurac\u00e1n LP580-2 Otto 5204 426 Euro 6d-TEMP 2018-09-01 df0 . memory_usage () Index 128 Hersteller 4506 Modell 22992 Motorart 3082 Hubraum 22992 Leistung 22992 Abgasnorm 3250 Markteinfuehrung 22992 dtype: int64 Save to the store store . put ( 'ADAC' , df0 , format = 'table' ) print ( store . info ()) <class 'pandas.io.pytables.HDFStore'> File path: store.h5 /ADAC frame_table (typ->appendable,nrows->2874,ncols->7,indexers->[index],dc->[]) /ADAC/meta/values_block_0/meta series_table (typ->appendable,nrows->44,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_1/meta series_table (typ->appendable,nrows->6,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_2/meta series_table (typ->appendable,nrows->7,ncols->1,indexers->[index],dc->[values]) store . get ( '/ADAC' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Hersteller Modell Motorart Hubraum Leistung Abgasnorm Markteinfuehrung 0 Abarth 595 Otto 1368 107 Euro 6d-TEMP-EVAP 2018-09-01 1 Abarth 595 Pista Otto 1368 118 Euro 6d-TEMP-EVAP 2018-09-01 2 Abarth 595 Turismo Otto 1368 121 Euro 6d-TEMP-EVAP 2018-09-01 3 Abarth 595 Competizione Otto 1368 132 Euro 6d-TEMP-EVAP 2018-09-01 4 Abarth 595C Otto 1368 107 Euro 6d-TEMP-EVAP 2018-09-01 ... ... ... ... ... ... ... ... 2869 VW Touran 2.0 TDI SCR Diesel 1968 140 Euro 6d-TEMP 2019-01-01 2870 VW up! 1.0 Otto 999 44 Euro 6d-TEMP 2018-08-01 2871 VW up! 1.0 Otto 999 55 Euro 6d-TEMP 2018-08-01 2872 VW up! 1.0 TSI Otto 999 66 Euro 6d-TEMP 2018-09-01 2873 VW up! GTI Otto 999 85 Euro 6d-TEMP 2018-01-01 2874 rows \u00d7 7 columns","title":"ADAC - df0 mit Abgasnorm Euro 6d-Temp, Euro 6d"},{"location":"_nb/%3DPy/05_pandas_xarray/00_fetch_data/#umweltdaten-nurnberg","text":"Messstation Jakobsplatz - Stadt N\u00fcrnberg from datetime import datetime today = datetime.today() from string import Template url = Template((\"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/\" \"archiv/csv-export/SUN/ {where}/ {where}/ /\" \"individuell/ {first}/ {first}/ /export.csv\")) make_url = lambda where, what, year: url.substitute( where=where, what=what, first=f\"01.01.{year}\", until=f'31.12.{year}') def get_air_data_in_nuremberg ( where : str , what : str , year : int ): opts = { 'skiprows' : range ( 0 , 10 ), 'encoding' : 'ISO-8859-1' , 'sep' : ';' , 'parse_dates' : [ 0 ], 'index_col' : 0 , 'na_values' : [ '-' ] } url = ( f \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/\" f \"archiv/csv-export/SUN/ { where } / { what } /\" f \"individuell/01.01. { year } /31.12. { year } /export.csv\" ) return pd . read_csv ( url , ** opts ) df = pd . concat ([ get_air_data_in_nuremberg ( where = \"nuernberg-jakobsplatz\" , what = \"stickstoffdioxid\" , year = y ) for y in range ( 2005 , 2020 )]) http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / messstation - jakobsplatz / feinstaub - pm10 / csv - export / SUN / nuernberg - jakobsplatz / staubpartikel - pm10 / 7 - Tages - Ansicht / export . csv Nu2HD5 = { 'nuernberg-jakobsplatz' : 'JAKOBSPALTZ' , 'nuernberg-flugfeld' : 'FLUGFELD' , 'stickstoffmonoxid' : 'NO' , 'stickstoffdioxid' : 'NO2' , 'ozon' : 'O3' , 'staubpartikel-pm10' : 'PM10' , 'staub-pm-25' : 'PM25' } from itertools import product for what , where in product ([ k for k in Nu2HD5 . keys () if 'nuernberg' not in k ], [ k for k in Nu2HD5 . keys () if 'nuernberg' in k ]): try : df = pd . concat ([ get_air_data_in_nuremberg ( where = where , what = what , year = y ) for y in range ( 2005 , 2020 )]) tableName = f \"AIR/ { Nu2HD5 [ where ] } / { Nu2HD5 [ what ] } \" store . put ( tableName , df , format = 'table' ) except Exception as e : print ( e ) print ( store . info ()) <class 'pandas.io.pytables.HDFStore'> File path: store.h5 /ADAC frame_table (typ->appendable,nrows->2874,ncols->7,indexers->[index],dc->[]) /ADAC/meta/values_block_0/meta series_table (typ->appendable,nrows->44,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_1/meta series_table (typ->appendable,nrows->6,ncols->1,indexers->[index],dc->[values]) /ADAC/meta/values_block_2/meta series_table (typ->appendable,nrows->7,ncols->1,indexers->[index],dc->[values]) /AIR/FLUGFELD/NO frame_table (typ->appendable,nrows->127817,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/NO2 frame_table (typ->appendable,nrows->127779,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/O3 frame_table (typ->appendable,nrows->129541,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/PM10 frame_table (typ->appendable,nrows->130374,ncols->1,indexers->[index],dc->[]) /AIR/FLUGFELD/PM25 frame_table (typ->appendable,nrows->103667,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/NO frame_table (typ->appendable,nrows->123168,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/NO2 frame_table (typ->appendable,nrows->123353,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/O3 frame_table (typ->appendable,nrows->125972,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/PM10 frame_table (typ->appendable,nrows->126952,ncols->1,indexers->[index],dc->[]) /AIR/JAKOBSPALTZ/PM25 frame_table (typ->appendable,nrows->87343,ncols->1,indexers->[index],dc->[]) df1 = store . get ( '/AIR/JAKOBSPALTZ/O3' ) df1 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) Datum/Zeit 2005-05-24 21:00:00 59.0 2005-01-06 00:00:00 57.0 2005-01-06 01:00:00 49.0 2005-01-06 02:00:00 51.0 2005-01-06 03:00:00 34.0 df1 . groupby ( df1 . index . weekday ) . boxplot ( layout = ( 1 , 7 ), figsize = ( 16 , 6 ));","title":"Umweltdaten N\u00fcrnberg"},{"location":"_nb/%3DPy/05_pandas_xarray/00_fetch_data/#with-pandas","text":"dfk = pd . read_csv ( make_url ( where = \"nuernberg-jakobsplatz\" , what = \"stickstoffdioxid\" , year = 2019 ), ** opts ) The data will be retrieved year for year, see no \" Da bei den Abfragen m\u00f6glicherweise gro\u00dfe Datenmengen entstehen, wird empfohlen maximal 12 Monate als Bezugszeitraum anzugeben und bei Bedarf mehrere Abfragen durchzuf\u00fchren. \"","title":"with pandas"},{"location":"_nb/%3DPy/05_pandas_xarray/00_fetch_data/#with-pyarrow","text":"from pyarrow import csv from pyarrow.csv import ( ReadOptions , ParseOptions , ConvertOptions ) read_opts = ReadOptions ( skip_rows = 13 , column_names = [ \"Date\" , \"NO2\" ]) parse_opts = ParseOptions ( delimiter = ';' ) conv_opts = ConvertOptions ( column_types = { 'NO2' : pa . int32 ()}, null_values = [ \"-\" ]) # Date are not an # 'Date': pa.timestamp('s'), from io import ( BytesIO , StringIO ) bio = BytesIO ( r . content . decode ( encoding = 'ISO-8859-1' ) . encode ()) table = csv . read_csv ( bio , read_options = read_opts , parse_options = parse_opts , convert_options = conv_opts ) table . schema Date: string NO2: int32 dfk . boxplot () <matplotlib.axes._subplots.AxesSubplot at 0x7f6fe5bd2710> import pyarrow as pa from pyarrow import csv from requests import Session year = 1 u1 = url . substitute ( what = \"stickstoffdioxid\" , first = today . strftime ( ' %d .%m.' ) + str ( today . year - year - 1 ), until = today . strftime ( ' %d .%m.' ) + str ( today . year - year )) s = Session () r = s . get ( u1 ) table . schema Date: string NO2: int32 table . to_pandas () Date object NO2 float64 dtype: object df1 = pd . concat (( pd . read_csv ( url . substitute ( what = \"stickstoffdioxid\" , first = today . strftime ( ' %d .%m.' ) + str ( today . year - year - 1 ), until = today . strftime ( ' %d .%m.' ) + str ( today . year - year )), ** opts ) for year in range ( 15 ))) df1 . info () <class 'pandas.core.frame.DataFrame'> DatetimeIndex: 112269 entries, 2019-01-01 01:00:00 to 2005-11-27 23:00:00 Data columns (total 1 columns): Wert (\u00b5g/m\u00b3) 111358 non-null float64 dtypes: float64(1) memory usage: 1.7 MB df1 [ 'Wert (\u00b5g/m\u00b3)' ] . metadata = {} df1 [ 'Wert (\u00b5g/m\u00b3)' ] . iloc [: 30 ] Datum/Zeit 2019-01-01 01:00:00 30.0 2019-01-01 02:00:00 29.0 2019-01-01 03:00:00 24.0 2019-01-01 04:00:00 22.0 2019-01-01 05:00:00 20.0 ... 2019-02-01 02:00:00 8.0 2019-02-01 03:00:00 6.0 2019-02-01 04:00:00 7.0 2019-02-01 05:00:00 6.0 2019-02-01 06:00:00 8.0 Name: Wert (\u00b5g/m\u00b3), Length: 30, dtype: float64 week_df = df1 . groupby ( df1 . index . weekday ) . mean () df1 . groupby ( df1 . index . weekday ) . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Wert (\u00b5g/m\u00b3) count mean std min 25% 50% 75% max Datum/Zeit 0 15849.0 30.795003 17.231654 1.0 18.0 27.0 40.0 133.0 1 15949.0 31.919744 17.604728 0.0 19.0 28.0 41.0 129.0 2 15943.0 32.267578 17.488338 0.0 19.0 29.0 42.0 148.0 3 15954.0 32.525385 17.798968 1.0 19.0 29.0 42.0 135.0 4 15959.0 31.770725 17.249226 -1.0 19.0 28.0 41.0 135.0 5 15816.0 28.693349 15.996195 0.0 17.0 25.0 37.0 129.0 6 15888.0 25.967082 16.686306 0.0 14.0 22.0 34.0 128.0 \" Although fine particle OC concentrations did not correlate with day of the week, EC concentrations showed a significant weekly pattern, with the highest concentration during the middle of the workweek and the lowest concentration on Sundays. \" https://www.ncbi.nlm.nih.gov/pubmed/15303295 df1 . groupby ( df1 . index . weekday ) . boxplot ( layout = ( 1 , 7 ), figsize = ( 16 , 6 )); gp = df1 . groupby ( df1 . index . weekday ) % timeit df1 [ df1 . index . weekday == 2 ] 7.61 ms \u00b1 75.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) % timeit df1 . groupby ( df1 . index . weekday ) . get_group ( 2 ) 10.2 ms \u00b1 73.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) df_wend = df1 [ df1 . index . weekday == 2 ] df_wend . groupby ( df_wend . index . hour ) . mean () . plot () <matplotlib.axes._subplots.AxesSubplot at 0x7f6ff53afc88> import seaborn as sns df_wend_h = df_wend . groupby ( df_wend . index . hour ) df_wend_h . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Wert (\u00b5g/m\u00b3) count mean std min 25% 50% 75% max Datum/Zeit 0 667.0 35.112444 19.408852 0.0 20.0 30.0 46.00 117.0 1 665.0 32.306767 18.458983 0.0 18.0 28.0 44.00 105.0 2 665.0 29.712782 17.153460 0.0 16.0 25.0 40.00 94.0 3 666.0 28.234234 15.617778 0.0 16.0 25.0 37.00 92.0 4 667.0 28.008996 14.758310 1.0 16.0 25.0 37.00 86.0 ... ... ... ... ... ... ... ... ... 19 667.0 35.962519 18.899770 6.0 21.0 32.0 46.00 144.0 20 668.0 38.938623 19.969410 6.0 25.0 36.0 48.25 148.0 21 666.0 39.978979 20.332119 4.0 25.0 36.0 50.00 125.0 22 665.0 40.756391 20.704699 6.0 26.0 37.0 52.00 124.0 23 665.0 39.533835 20.385202 5.0 24.0 36.0 51.00 118.0 24 rows \u00d7 8 columns sns . pointplot ( x = \"day\" , y = \"tip\" , data = tips , ci = 68 ) df_wend_h . boxplot ( subplots = False , figsize = ( 16 , 6 ), column = list ( df_wend_h . groups . keys ())) <matplotlib.axes._subplots.AxesSubplot at 0x7f6fe55677b8> df_wend_h . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } Wert (\u00b5g/m\u00b3) count mean std min 25% 50% 75% max Datum/Zeit 0 667.0 35.112444 19.408852 0.0 20.0 30.0 46.00 117.0 1 665.0 32.306767 18.458983 0.0 18.0 28.0 44.00 105.0 2 665.0 29.712782 17.153460 0.0 16.0 25.0 40.00 94.0 3 666.0 28.234234 15.617778 0.0 16.0 25.0 37.00 92.0 4 667.0 28.008996 14.758310 1.0 16.0 25.0 37.00 86.0 ... ... ... ... ... ... ... ... ... 19 667.0 35.962519 18.899770 6.0 21.0 32.0 46.00 144.0 20 668.0 38.938623 19.969410 6.0 25.0 36.0 48.25 148.0 21 666.0 39.978979 20.332119 4.0 25.0 36.0 50.00 125.0 22 665.0 40.756391 20.704699 6.0 26.0 37.0 52.00 124.0 23 665.0 39.533835 20.385202 5.0 24.0 36.0 51.00 118.0 24 rows \u00d7 8 columns df_wend_h . boxplot ( by = 'Datum/Zeit' , figsize = ( 16 , 10 )); --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-204-ee7cf848b6f4> in <module> ----> 1 df_wend_h . boxplot ( by = 'Datum/Zeit' , figsize = ( 16 , 10 ) ) ; /opt/conda/lib/python3.7/site-packages/pandas/plotting/_core.py in boxplot_frame_groupby (grouped, subplots, column, fontsize, rot, grid, ax, figsize, layout, sharex, sharey, **kwds) 498 sharex = sharex , 499 sharey = sharey , --> 500 ** kwds 501 ) 502 /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in boxplot_frame_groupby (grouped, subplots, column, fontsize, rot, grid, ax, figsize, layout, sharex, sharey, **kwds) 390 for ( key , group ) , ax in zip ( grouped , axes ) : 391 d = group.boxplot( --> 392 ax = ax , column = column , fontsize = fontsize , rot = rot , grid = grid , ** kwds 393 ) 394 ax . set_title ( pprint_thing ( key ) ) /opt/conda/lib/python3.7/site-packages/pandas/plotting/_core.py in boxplot_frame (self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds) 418 layout = layout , 419 return_type = return_type , --> 420 ** kwds 421 ) 422 /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in boxplot_frame (self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds) 353 layout = layout , 354 return_type = return_type , --> 355 ** kwds 356 ) 357 plt . draw_if_interactive ( ) /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in boxplot (data, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds) 300 ax = ax , 301 layout = layout , --> 302 return_type = return_type , 303 ) 304 else : /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in _grouped_plot_by_column (plotf, data, columns, by, numeric_only, grid, figsize, ax, layout, return_type, **kwargs) 205 gp_col = grouped [ col ] 206 keys , values = zip ( * gp_col ) --> 207 re_plotf = plotf ( keys , values , ax , ** kwargs ) 208 ax . set_title ( col ) 209 ax . set_xlabel ( pprint_thing ( by ) ) /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py in plot_group (keys, values, ax) 262 keys = [ pprint_thing ( x ) for x in keys ] 263 values = [ np . asarray ( remove_na_arraylike ( v ) ) for v in values ] --> 264 bp = ax . boxplot ( values , ** kwds ) 265 if fontsize is not None : 266 ax . tick_params ( axis = \"both\" , labelsize = fontsize ) /opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper (*args, **kwargs) 305 f\"for the old name will be dropped %(removal)s.\") 306 kwargs [ new ] = kwargs . pop ( old ) --> 307 return func ( * args , ** kwargs ) 308 309 # wrapper() must keep the same documented signature as func(): if we /opt/conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner (ax, data, *args, **kwargs) 1599 def inner ( ax , * args , data = None , ** kwargs ) : 1600 if data is None : -> 1601 return func ( ax , * map ( sanitize_sequence , args ) , ** kwargs ) 1602 1603 bound = new_sig . bind ( ax , * args , ** kwargs ) /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in boxplot (self, x, notch, sym, vert, whis, positions, widths, patch_artist, bootstrap, usermedians, conf_intervals, meanline, showmeans, showcaps, showbox, showfliers, boxprops, labels, flierprops, medianprops, meanprops, capprops, whiskerprops, manage_ticks, autorange, zorder) 3770 meanline = meanline , showfliers = showfliers , 3771 capprops = capprops , whiskerprops = whiskerprops , -> 3772 manage_ticks=manage_ticks, zorder=zorder) 3773 return artists 3774 /opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper (*args, **kwargs) 305 f\"for the old name will be dropped %(removal)s.\") 306 kwargs [ new ] = kwargs . pop ( old ) --> 307 return func ( * args , ** kwargs ) 308 309 # wrapper() must keep the same documented signature as func(): if we /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in bxp (self, bxpstats, positions, widths, vert, patch_artist, shownotches, showmeans, showcaps, showbox, showfliers, boxprops, whiskerprops, flierprops, medianprops, capprops, meanprops, meanline, manage_ticks, zorder) 4094 4095 # draw the medians -> 4096 medians . extend ( doplot ( med_x , med_y , ** final_medianprops ) ) 4097 4098 # maybe draw the means /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in doplot (*args, **kwargs) 3996 if vert : 3997 def doplot ( * args , ** kwargs ) : -> 3998 return self . plot ( * args , ** kwargs ) 3999 4000 def dopatch ( xs , ys , ** kwargs ) : /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in plot (self, scalex, scaley, data, *args, **kwargs) 1667 for line in lines : 1668 self . add_line ( line ) -> 1669 self . autoscale_view ( scalex = scalex , scaley = scaley ) 1670 return lines 1671 /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in autoscale_view (self, tight, scalex, scaley) 2493 handle_single_axis( 2494 scalex , self . _autoscaleXon , self . _shared_x_axes , 'intervalx' , -> 2495 'minposx', self.xaxis, self._xmargin, x_stickies, self.set_xbound) 2496 handle_single_axis( 2497 scaley , self . _autoscaleYon , self . _shared_y_axes , 'intervaly' , /opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py in handle_single_axis (scale, autoscaleon, shared_axes, interval, minpos, axis, margin, stickies, set_bound) 2460 x0 , x1 = getattr ( bb , interval ) 2461 locator = axis . get_major_locator ( ) -> 2462 x0 , x1 = locator . nonsingular ( x0 , x1 ) 2463 2464 # Add the margin in figure space and then transform back, to handle /opt/conda/lib/python3.7/site-packages/matplotlib/ticker.py in nonsingular (self, v0, v1) 1523 def nonsingular ( self , v0 , v1 ) : 1524 \"\"\"Expand a range as needed to avoid singularities.\"\"\" -> 1525 return mtransforms . nonsingular ( v0 , v1 , expander = .05 ) 1526 1527 def view_limits ( self , vmin , vmax ) : /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in nonsingular (vmin, vmax, expander, tiny, increasing) 2825 swapped = True 2826 -> 2827 maxabsvalue = max ( abs ( vmin ) , abs ( vmax ) ) 2828 if maxabsvalue < ( 1e6 / tiny ) * np . finfo ( float ) . tiny : 2829 vmin = - expander KeyboardInterrupt : Error in callback <function flush_figures at 0x7f6ff8de0d90> (for post_execute): --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py in flush_figures () 115 # ignore the tracking, just draw and close all figures 116 try : --> 117 return show ( True ) 118 except Exception as e : 119 # safely show traceback if in IPython, else raise /opt/conda/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py in show (close, block) 37 display( 38 figure_manager . canvas . figure , ---> 39 metadata = _fetch_figure_metadata ( figure_manager . canvas . figure ) 40 ) 41 finally : /opt/conda/lib/python3.7/site-packages/IPython/core/display.py in display (include, exclude, metadata, transient, display_id, *objs, **kwargs) 311 publish_display_data ( data = obj , metadata = metadata , ** kwargs ) 312 else : --> 313 format_dict , md_dict = format ( obj , include = include , exclude = exclude ) 314 if not format_dict : 315 # nothing to display (e.g. _ipython_display_ took over) /opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in format (self, obj, include, exclude) 178 md = None 179 try : --> 180 data = formatter ( obj ) 181 except : 182 # FIXME: log the exception </opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-9> in __call__ (self, obj) /opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in catch_format_error (method, self, *args, **kwargs) 222 \"\"\"show traceback on failed format call\"\"\" 223 try : --> 224 r = method ( self , * args , ** kwargs ) 225 except NotImplementedError : 226 # don't warn on NotImplementedErrors /opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in __call__ (self, obj) 339 pass 340 else : --> 341 return printer ( obj ) 342 # Finally look for special method names 343 method = get_real_method ( obj , self . print_method ) /opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda> (fig) 242 243 if 'png' in formats : --> 244 png_formatter . for_type ( Figure , lambda fig : print_figure ( fig , 'png' , ** kwargs ) ) 245 if 'retina' in formats or 'png2x' in formats : 246 png_formatter . for_type ( Figure , lambda fig : retina_figure ( fig , ** kwargs ) ) /opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure (fig, fmt, bbox_inches, **kwargs) 126 127 bytes_io = BytesIO ( ) --> 128 fig . canvas . print_figure ( bytes_io , ** kw ) 129 data = bytes_io . getvalue ( ) 130 if fmt == 'svg' : /opt/conda/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure (self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs) 2058 bbox_artists = kwargs . pop ( \"bbox_extra_artists\" , None ) 2059 bbox_inches = self.figure.get_tightbbox(renderer, -> 2060 bbox_extra_artists=bbox_artists) 2061 pad = kwargs . pop ( \"pad_inches\" , None ) 2062 if pad is None : /opt/conda/lib/python3.7/site-packages/matplotlib/figure.py in get_tightbbox (self, renderer, bbox_extra_artists) 2386 return self . bbox_inches 2387 -> 2388 _bbox = Bbox . union ( bb ) 2389 2390 bbox_inches = TransformedBbox(_bbox, /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in union (bboxes) 702 x0 = np . min ( [ bbox . xmin for bbox in bboxes ] ) 703 x1 = np . max ( [ bbox . xmax for bbox in bboxes ] ) --> 704 y0 = np . min ( [ bbox . ymin for bbox in bboxes ] ) 705 y1 = np . max ( [ bbox . ymax for bbox in bboxes ] ) 706 return Bbox ( [ [ x0 , y0 ] , [ x1 , y1 ] ] ) /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in <listcomp> (.0) 702 x0 = np . min ( [ bbox . xmin for bbox in bboxes ] ) 703 x1 = np . max ( [ bbox . xmax for bbox in bboxes ] ) --> 704 y0 = np . min ( [ bbox . ymin for bbox in bboxes ] ) 705 y1 = np . max ( [ bbox . ymax for bbox in bboxes ] ) 706 return Bbox ( [ [ x0 , y0 ] , [ x1 , y1 ] ] ) /opt/conda/lib/python3.7/site-packages/matplotlib/transforms.py in ymin (self) 353 def ymin ( self ) : 354 \"\"\"The bottom edge of the bounding box.\"\"\" --> 355 return np . min ( self . get_points ( ) [ : , 1 ] ) 356 357 @ property <__array_function__ internals> in amin (*args, **kwargs) /opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in amin (a, axis, out, keepdims, initial, where) 2744 \"\"\" 2745 return _wrapreduction(a, np.minimum, 'min', axis, None, out, -> 2746 keepdims=keepdims, initial=initial, where=where) 2747 2748 /opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapreduction (obj, ufunc, method, axis, dtype, out, **kwargs) 88 return reduction ( axis = axis , out = out , ** passkwargs ) 89 ---> 90 return ufunc . reduce ( obj , axis , dtype , out , ** passkwargs ) 91 92 KeyboardInterrupt : df1 . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) count 111358.000000 mean 30.567341 std 17.305318 min -1.000000 25% 18.000000 50% 27.000000 75% 40.000000 max 148.000000 np . arange ( - 1 , 150 , 20 ) . reshape ( 1 , 8 ) . shape (1, 8) df1 . iloc [:, 0 ] . to_numpy () array([30., 29., 24., ..., 38., 33., 39.]) cut = pd . qcut ( df1 . iloc [:, 0 ] . to_numpy (), q = 10 ) df1 . groupby ( cut ) . mean () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) (-1.001, 12.0] 9.379547 (12.0, 16.0] 14.562010 (16.0, 19.0] 17.998225 (19.0, 23.0] 21.479578 (23.0, 27.0] 25.477695 (27.0, 31.0] 29.440277 (31.0, 37.0] 34.399984 (37.0, 44.0] 40.804197 (44.0, 54.0] 49.080055 (54.0, 148.0] 67.818124 week_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) Datum/Zeit 0 30.795003 1 31.919744 2 32.267578 3 32.525385 4 31.770725 5 28.693349 6 25.967082 df1 . plot () <matplotlib.axes._subplots.AxesSubplot at 0x7feedcd0e780> http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / stickstoffmonoxid / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / lufttemperatur - aussen / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / stickstoffdioxid / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / staub - pm - 25 / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / staubpartikel - pm10 / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / staubpartikel - pm10 / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv urls = { 'stickstoffmonoxid' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/stickstoffmonoxid/csv-export/SUN/nuernberg-jakobsplatz/stickstoffmonoxid/7-Tages-Ansicht/export.csv\" , 'stickstoffdioxid' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/stickstoffdioxid/csv-export/SUN/nuernberg-jakobsplatz/stickstoffdioxid/7-Tages-Ansicht/export.csv\" , 'ozon' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/ozon/csv-export/SUN/nuernberg-jakobsplatz/ozon/7-Tages-Ansicht/export.csv\" , 'pm10' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/feinstaub-pm10/csv-export/SUN/nuernberg-jakobsplatz/staubpartikel-pm10/7-Tages-Ansicht/export.csv\" , 'pm25' : \"http://umweltdaten.nuernberg.de/csv/aussenluft/stadt-nuernberg/messstation-jakobsplatz/feinstaub-pm25/csv-export/SUN/nuernberg-jakobsplatz/staub-pm-25/7-Tages-Ansicht/export.csv\" } opts = { 'skiprows' : range ( 10 ), 'encoding' : 'ISO-8859-1' , 'sep' : \";\" , 'index_col' : 0 } res = { comp : pd . read_csv ( url , ** opts ) for comp , url in urls . items ()} http : // umweltdaten . nuernberg . de / csv / aussenluft / stadt - nuernberg / archiv / csv - export / SUN / nuernberg - jakobsplatz / stickstoffdioxid / individuell / 01.06 . 2005 / 27.11 . 2019 / export . csv [ df . shape for df in res . values ()] [(187, 1), (187, 1), (187, 1), (187, 1), (187, 1)] res [ 'ozon' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Wert (\u00b5g/m\u00b3) Datum/Zeit 20.11.2019 01:00 23 20.11.2019 02:00 31 20.11.2019 03:00 32 20.11.2019 04:00 25 20.11.2019 05:00 27 ... ... 27.11.2019 15:00 12 27.11.2019 16:00 8 27.11.2019 17:00 6 27.11.2019 18:00 3 27.11.2019 19:00 - 187 rows \u00d7 1 columns import stumpy val = dfk . dropna () . to_numpy () . ravel () st = stumpy . stump ( val , val . size // 330 ) import matplotlib.pyplot as plt fig , axs = plt . subplots ( 2 , sharex = True , figsize = ( 16 , 9 ), gridspec_kw = { 'hspace' : 0 }) plt . suptitle ( 'Discord (Anomaly/Novelty) Discovery' , fontsize = '30' ) axs [ 0 ] . plot ( val ) axs [ 1 ] . set_xlabel ( 'Time' , fontsize = '20' ) axs [ 1 ] . set_ylabel ( 'Matrix Profile' , fontsize = '20' ) #axs[1].axvline(x=3864, linestyle=\"dashed\") axs [ 1 ] . plot ( st [:, 0 ]) [<matplotlib.lines.Line2D at 0x7f03c4501860>]","title":"with pyarrow"},{"location":"_nb/%3DPy/05_pandas_xarray/Interval%20and%20IntervalArray/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import xarray xarray . set_options ( display_style = \"html\" ) import numpy as np import pandas as pd from pandas import ( DataFrame , Series , Interval ) from pandas.arrays import IntervalArray from xarray import DataArray % load_ext watermark % watermark - vdmp xarray The watermark extension is already loaded. To reload it, use: %reload_ext watermark 2019-11-27 CPython 3.7.3 IPython 7.9.0 xarray 0.14.1 compiler : GCC 7.3.0 system : Linux release : 5.0.0-36-generic machine : x86_64 processor : x86_64 CPU cores : 8 interpreter: 64bit Interval \u00b6 Pandas IntervalDtype extends Numpy ndarray type . This is not an actual numpy dtype , but a duck type. An Interval is an immutable object implementing a bounded slice-like interval. Parameters: - left: scalar - right: scalar - closed: string , default to right s = Series ( np . random . randint ( 1 , 100 , size = 100 )) r = pd . cut ( s , np . arange ( 0 , 101 , 20 )) r . sample ( 3 ) 14 (20, 40] 92 (0, 20] 97 (0, 20] dtype: category Categories (5, interval[int64]): [(0, 20] < (20, 40] < (40, 60] < (60, 80] < (80, 100]] IntervalArray \u00b6 IntervalArray . from_arrays ([ 0 , 1 , 2 ], [ 1 , 2 , 3 ]) IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') IntervalArray . from_tuples ((( 0 , 1 ), ( 1 , 2 ), ( 2 , 3 ))) IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') IntervalArray . from_breaks ([ 0 , 1 , 2 , 3 ]) IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') intervals = IntervalArray . from_breaks ([ 0 , 1 , 2 , 3 ]) from sys import getsizeof getsizeof ( intervals ) 56 intervals . take ([ 1 ]) IntervalArray([(1, 2]], closed='right', dtype='interval[int64]') intervals IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') a = np . array ([( 1 , 2 ), ( 2 , 3 )], dtype = [( 'lb' , np . int32 ), ( 'ub' , np . int32 )]) da = DataArray ( range ( 2 ), coords = [( 'x' , IntervalArray ([ Interval ( * v ) for v in a ]))]) da + da Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.DataArray x : 2 0 2 array([0, 2]) Coordinates: (1) x (x) object (1, 2] (2, 3] array([Interval(1, 2, closed='right'), Interval(2, 3, closed='right')], dtype=object) Attributes: (0) da . plot . hist () (array([1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), <a list of 10 Patch objects>) i = Interval ( left = 0 , right = 5 ) i . length 5 da . to_dict () {'dims': ('x',), 'attrs': {}, 'data': [0, 1], 'coords': {'x': {'dims': ('x',), 'attrs': {}, 'data': [Interval(1, 2, closed='right'), Interval(2, 3, closed='right')]}}, 'name': None} da . to_masked_array () masked_array(data=[0, 1], mask=[False, False], fill_value=999999)","title":"Interval and IntervalArray"},{"location":"_nb/%3DPy/05_pandas_xarray/Interval%20and%20IntervalArray/#interval","text":"Pandas IntervalDtype extends Numpy ndarray type . This is not an actual numpy dtype , but a duck type. An Interval is an immutable object implementing a bounded slice-like interval. Parameters: - left: scalar - right: scalar - closed: string , default to right s = Series ( np . random . randint ( 1 , 100 , size = 100 )) r = pd . cut ( s , np . arange ( 0 , 101 , 20 )) r . sample ( 3 ) 14 (20, 40] 92 (0, 20] 97 (0, 20] dtype: category Categories (5, interval[int64]): [(0, 20] < (20, 40] < (40, 60] < (60, 80] < (80, 100]]","title":"Interval"},{"location":"_nb/%3DPy/05_pandas_xarray/Interval%20and%20IntervalArray/#intervalarray","text":"IntervalArray . from_arrays ([ 0 , 1 , 2 ], [ 1 , 2 , 3 ]) IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') IntervalArray . from_tuples ((( 0 , 1 ), ( 1 , 2 ), ( 2 , 3 ))) IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') IntervalArray . from_breaks ([ 0 , 1 , 2 , 3 ]) IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') intervals = IntervalArray . from_breaks ([ 0 , 1 , 2 , 3 ]) from sys import getsizeof getsizeof ( intervals ) 56 intervals . take ([ 1 ]) IntervalArray([(1, 2]], closed='right', dtype='interval[int64]') intervals IntervalArray([(0, 1], (1, 2], (2, 3]], closed='right', dtype='interval[int64]') a = np . array ([( 1 , 2 ), ( 2 , 3 )], dtype = [( 'lb' , np . int32 ), ( 'ub' , np . int32 )]) da = DataArray ( range ( 2 ), coords = [( 'x' , IntervalArray ([ Interval ( * v ) for v in a ]))]) da + da Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.DataArray x : 2 0 2 array([0, 2]) Coordinates: (1) x (x) object (1, 2] (2, 3] array([Interval(1, 2, closed='right'), Interval(2, 3, closed='right')], dtype=object) Attributes: (0) da . plot . hist () (array([1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), <a list of 10 Patch objects>) i = Interval ( left = 0 , right = 5 ) i . length 5 da . to_dict () {'dims': ('x',), 'attrs': {}, 'data': [0, 1], 'coords': {'x': {'dims': ('x',), 'attrs': {}, 'data': [Interval(1, 2, closed='right'), Interval(2, 3, closed='right')]}}, 'name': None} da . to_masked_array () masked_array(data=[0, 1], mask=[False, False], fill_value=999999)","title":"IntervalArray"},{"location":"data/_intro/","text":"Apache Arrow \u00b6","title":"Intro"},{"location":"data/_intro/#apache-arrow","text":"","title":"Apache Arrow"},{"location":"data/windowing/","text":"Windowing \u00b6 Machine learning and data exploring: the potential of windowing","title":"Windowing"},{"location":"data/windowing/#windowing","text":"Machine learning and data exploring: the potential of windowing","title":"Windowing"},{"location":"data/timeseries/_intro/","text":"Time series \u00b6 http://www.timeseriesclassification.com","title":"Intro"},{"location":"data/timeseries/_intro/#time-series","text":"http://www.timeseriesclassification.com","title":"Time series"},{"location":"data/timeseries/features/","text":"Time series features extraction \u00b6 https://towardsdatascience.com/time-series-feature-extraction-for-industrial-big-data-iiot-applications-5243c84aaf0e http://fastml.com/classifying-time-series-using-feature-extraction/","title":"Features selection"},{"location":"data/timeseries/features/#time-series-features-extraction","text":"https://towardsdatascience.com/time-series-feature-extraction-for-industrial-big-data-iiot-applications-5243c84aaf0e http://fastml.com/classifying-time-series-using-feature-extraction/","title":"Time series features extraction"},{"location":"data/timeseries/matrixprofile/","text":"Matrix profile \u00b6 Papers: Efficient Matrix Profile Computation Using DifferentDistance Functions Matrix Profile I: All Pairs Similarity Joins for Time Series:A Unifying View that Includes Motifs, Discords and Shapelets Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred MillionBarrierfor Time Series Motifs and Joins Stumpy \u00b6 Quote At its core, the STUMPY library efficiently computes something called a matrix profile , a vector that stores the z-normalized Euclidean distance between any subsequence within a time series and its nearest neigbor.","title":"Matrix profile"},{"location":"data/timeseries/matrixprofile/#matrix-profile","text":"Papers: Efficient Matrix Profile Computation Using DifferentDistance Functions Matrix Profile I: All Pairs Similarity Joins for Time Series:A Unifying View that Includes Motifs, Discords and Shapelets Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred MillionBarrierfor Time Series Motifs and Joins","title":"Matrix profile"},{"location":"data/timeseries/matrixprofile/#stumpy","text":"Quote At its core, the STUMPY library efficiently computes something called a matrix profile , a vector that stores the z-normalized Euclidean distance between any subsequence within a time series and its nearest neigbor.","title":"Stumpy "},{"location":"devops/ansible/","text":"Ansible \u00b6 Mac setup and configuration from Jeff Gerling github repo < Cowsay and Ansible > \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Testing \u00b6 Molecule \u00b6","title":"Ansible"},{"location":"devops/ansible/#ansible","text":"Mac setup and configuration from Jeff Gerling github repo < Cowsay and Ansible > \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Ansible"},{"location":"devops/ansible/#testing","text":"","title":"Testing"},{"location":"devops/ansible/#molecule","text":"","title":"Molecule"},{"location":"devops/codecov/","text":"Code coverage \u00b6 In computer science, test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs. Coverage criteria \u00b6 There are a number of coverage criteria, the main ones being: Function coverage has each function (or subroutine) in the program been called? Statement coverage has each statement in the program been executed? Edge coverage has every edge in the Control flow graph been executed? Branch coverage has each branch (also called DD-path) of each control structure (such as in if and case statements) been executed? For example, given an if statement, have both the true and false branches been executed? This is a subset of edge coverage. Condition coverage (or predicate coverage) has each Boolean sub-expression evaluated both to true and false? https://codecov.io/","title":"Code coverage"},{"location":"devops/codecov/#code-coverage","text":"In computer science, test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs.","title":"Code coverage"},{"location":"devops/codecov/#coverage-criteria","text":"There are a number of coverage criteria, the main ones being: Function coverage has each function (or subroutine) in the program been called? Statement coverage has each statement in the program been executed? Edge coverage has every edge in the Control flow graph been executed? Branch coverage has each branch (also called DD-path) of each control structure (such as in if and case statements) been executed? For example, given an if statement, have both the true and false branches been executed? This is a subset of edge coverage. Condition coverage (or predicate coverage) has each Boolean sub-expression evaluated both to true and false? https://codecov.io/","title":"Coverage criteria"},{"location":"doc/md/","text":"mkdocs pymdown-extensions mkdocs-material Github guide Comments: see StackOverflow 4823469","title":"Markdown"},{"location":"ml/dl/","text":"Time series \u00b6 tsfresh \u00b6 Quote tsfresh is a python package. It automatically calculates a large number of time series characteristics, the so called features. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks. 1D convolutional neural network \u00b6 Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences A Visual Introduction to Machine Learning and AI 1D ConvolutionalNeural Networks andApplications\u2013A Survey Signal Status Recognition Based on 1DCNN and Its Feature Extraction Mechanism Analysis Working with 1D convonlutional neural network in Keras \u00b6 https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/ Example \u00b6","title":"Time series"},{"location":"ml/dl/#time-series","text":"","title":"Time series"},{"location":"ml/dl/#tsfresh","text":"Quote tsfresh is a python package. It automatically calculates a large number of time series characteristics, the so called features. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks.","title":"tsfresh"},{"location":"ml/dl/#1d-convolutional-neural-network","text":"Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences A Visual Introduction to Machine Learning and AI 1D ConvolutionalNeural Networks andApplications\u2013A Survey Signal Status Recognition Based on 1DCNN and Its Feature Extraction Mechanism Analysis","title":"1D convolutional neural network"},{"location":"ml/dl/#working-with-1d-convonlutional-neural-network-in-keras","text":"https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/","title":"Working with 1D convonlutional neural network in Keras"},{"location":"ml/dl/#example","text":"","title":"Example"},{"location":"ndarray/intro/","text":"ND-array \u00b6 The N-dimensional array (ndarray) \u00b6 contiguous one-dimensional segment of computer memory NumPy ndarray internals can be read in source code typedef struct tagPyArrayObject_fields { PyObject_HEAD /* Pointer to the raw data buffer */ char * data ; /* The number of dimensions, also called 'ndim' */ int nd ; /* The size in each dimension, also called 'shape' */ npy_intp * dimensions ; /* * Number of bytes to jump to get to the * next element in each dimension */ npy_intp * strides ; /* * This object is decref'd upon * deletion of array. Except in the * case of WRITEBACKIFCOPY which has * special handling. * * For views it points to the original * array, collapsed so no chains of * views occur. * * For creation from buffer object it * points to an object that should be * decref'd on deletion * * For WRITEBACKIFCOPY flag this is an * array to-be-updated upon calling * PyArray_ResolveWritebackIfCopy */ PyObject * base ; /* Pointer to type structure */ PyArray_Descr * descr ; /* Flags describing array -- see below */ int flags ; /* For weak references */ PyObject * weakreflist ; } PyArrayObject_fields ; Papers: https://jakevdp.github.io/PythonDataScienceHandbook/02.01-understanding-data-types.html NumPy documentation https://github.com/numpy/numpy/blob/b7c27bd2a3817f59c84b004b87bba5db57d9a9b0/numpy/core/include/numpy/ndarraytypes.h#L1343 ndarray.flags Information about the memory layout of the array. ndarray.shape Tuple of array dimensions. ndarray.strides Tuple of bytes to step in each dimension when traversing an array. ndarray.ndim Number of array dimensions. ndarray.data Python buffer object pointing to the start of the array\u2019s data. ndarray.size Number of elements in the array. ndarray.itemsize Length of one array element in bytes. ndarray.nbytes Total bytes consumed by the elements of the array. ndarray.base Base object if memory is from some other object.","title":"Intro"},{"location":"ndarray/intro/#nd-array","text":"","title":"ND-array"},{"location":"ndarray/intro/#the-n-dimensional-array-ndarray","text":"contiguous one-dimensional segment of computer memory NumPy ndarray internals can be read in source code typedef struct tagPyArrayObject_fields { PyObject_HEAD /* Pointer to the raw data buffer */ char * data ; /* The number of dimensions, also called 'ndim' */ int nd ; /* The size in each dimension, also called 'shape' */ npy_intp * dimensions ; /* * Number of bytes to jump to get to the * next element in each dimension */ npy_intp * strides ; /* * This object is decref'd upon * deletion of array. Except in the * case of WRITEBACKIFCOPY which has * special handling. * * For views it points to the original * array, collapsed so no chains of * views occur. * * For creation from buffer object it * points to an object that should be * decref'd on deletion * * For WRITEBACKIFCOPY flag this is an * array to-be-updated upon calling * PyArray_ResolveWritebackIfCopy */ PyObject * base ; /* Pointer to type structure */ PyArray_Descr * descr ; /* Flags describing array -- see below */ int flags ; /* For weak references */ PyObject * weakreflist ; } PyArrayObject_fields ; Papers: https://jakevdp.github.io/PythonDataScienceHandbook/02.01-understanding-data-types.html NumPy documentation https://github.com/numpy/numpy/blob/b7c27bd2a3817f59c84b004b87bba5db57d9a9b0/numpy/core/include/numpy/ndarraytypes.h#L1343 ndarray.flags Information about the memory layout of the array. ndarray.shape Tuple of array dimensions. ndarray.strides Tuple of bytes to step in each dimension when traversing an array. ndarray.ndim Number of array dimensions. ndarray.data Python buffer object pointing to the start of the array\u2019s data. ndarray.size Number of elements in the array. ndarray.itemsize Length of one array element in bytes. ndarray.nbytes Total bytes consumed by the elements of the array. ndarray.base Base object if memory is from some other object.","title":"The N-dimensional array (ndarray)"},{"location":"shell/fzf/","text":"fzf \u00b6","title":"fzf"},{"location":"shell/fzf/#fzf","text":"","title":"fzf"},{"location":"shell/git/","text":"GIT \u00b6 Configuration \u00b6 The configuration is made out of four files : $(prefix)/etc/gitconfig $XDG_CONFIG_HOME/git/config ~/.gitconfig $GIT_DIR/config Pull request \u00b6 Write a pull request \u00b6 https://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/ https://stackoverflow.com/questions/14680711 https://stackoverflow.com/questions/29049650 Switch branches or restore working tree files git checkout master git checkout -b mybranch git remote manage the set of repositories (\"remotes\") whose branches you track. git remote -v # If upstream is not configured, add the upstream route git remote add upstream /url/original/repo git fetch upstream # reset master to upstream/master git checkout master git reset --hard upstream/master git push --force y--y--y (mybranch) / z--z--z (master, upstream/master, origin/master) # replay the patches (even they are rejected for now) on top of master git checkout mybranch git rebase master git push -u origin mybranch y'--y'--y' (mybranch, origin/mybranch) / z--z--z (master, upstream/master, origin/master) Submodules \u00b6 Commands \u00b6 status prints the SHA1 status of checked submodules. Prefixed with: - if the submodule is not initialized + in case of conflicts between current subdmodule and the index of the containing directory U in case of merge conflicts Remove a submodule \u00b6 Remove the submodule entry from .git/config git submodule deinit -f path/to/submodule Remove the submodule directory from the superproject's .git/modules directory rm -rf .git/modules/path/to/submodule Commit the changes git commit-m \"Removed submodule \" Remove the entry in .gitmodules and remove the submodule directory located at path/to/submodule git rm -f path/to/submodule Cherry pick from another repository \u00b6 First the other repository source must be added to the remote list git remote add other https://other.url/repository.git git fetch other Then use git cherry-pick","title":"git"},{"location":"shell/git/#git","text":"","title":"GIT"},{"location":"shell/git/#configuration","text":"The configuration is made out of four files : $(prefix)/etc/gitconfig $XDG_CONFIG_HOME/git/config ~/.gitconfig $GIT_DIR/config","title":"Configuration"},{"location":"shell/git/#pull-request","text":"","title":"Pull request"},{"location":"shell/git/#write-a-pull-request","text":"https://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/ https://stackoverflow.com/questions/14680711 https://stackoverflow.com/questions/29049650 Switch branches or restore working tree files git checkout master git checkout -b mybranch git remote manage the set of repositories (\"remotes\") whose branches you track. git remote -v # If upstream is not configured, add the upstream route git remote add upstream /url/original/repo git fetch upstream # reset master to upstream/master git checkout master git reset --hard upstream/master git push --force y--y--y (mybranch) / z--z--z (master, upstream/master, origin/master) # replay the patches (even they are rejected for now) on top of master git checkout mybranch git rebase master git push -u origin mybranch y'--y'--y' (mybranch, origin/mybranch) / z--z--z (master, upstream/master, origin/master)","title":"Write a pull request"},{"location":"shell/git/#submodules","text":"","title":"Submodules"},{"location":"shell/git/#commands","text":"status prints the SHA1 status of checked submodules. Prefixed with: - if the submodule is not initialized + in case of conflicts between current subdmodule and the index of the containing directory U in case of merge conflicts","title":"Commands"},{"location":"shell/git/#remove-a-submodule","text":"Remove the submodule entry from .git/config git submodule deinit -f path/to/submodule Remove the submodule directory from the superproject's .git/modules directory rm -rf .git/modules/path/to/submodule Commit the changes git commit-m \"Removed submodule \" Remove the entry in .gitmodules and remove the submodule directory located at path/to/submodule git rm -f path/to/submodule","title":"Remove a submodule"},{"location":"shell/git/#cherry-pick-from-another-repository","text":"First the other repository source must be added to the remote list git remote add other https://other.url/repository.git git fetch other Then use git cherry-pick","title":"Cherry pick from another repository"},{"location":"shell/intro/","text":"Linux \u00b6 qsdsqsdqsd fzf (fuzzy finder) Ranger A VIM-inspired filemanager for the console","title":"Intro"},{"location":"shell/intro/#linux","text":"qsdsqsdqsd fzf (fuzzy finder) Ranger A VIM-inspired filemanager for the console","title":"Linux"},{"location":"shell/sys/","text":"Helpers \u00b6 Displaying IP address on eth0 interface ifconfig eth0 | grep \"inet ad\" | cut -d ':' -f 2 | cut -d ' ' -f 1","title":"helpers"},{"location":"shell/sys/#helpers","text":"Displaying IP address on eth0 interface ifconfig eth0 | grep \"inet ad\" | cut -d ':' -f 2 | cut -d ' ' -f 1","title":"Helpers"},{"location":"shell/vim/","text":"Vim \u00b6 :h terminal or Panes \u00b6 Close a terminal pane with C-w C-c To change two vertically split windows to horizonally split C-w t C-w K Horizontally to vertically: C-w t C-w H C-w t makes the first (topleft) window current C-w H moves the current window to full-height at far left C-w K moves the current window to full-width at the very top Comment line (via plugin vim-commentary): gcc in normal mode, gc in visual mode Surroung element while editing Code edition \u00b6 Folding \u00b6 For Python, https://github.com/Vimjas/vim-python-pep8-indent Plugins \u00b6 File editing \u00b6 vim-commentary comment stuff out vim-surround provides mapping for parentheses, brackets, quotes, XML tags, and more. vim-multiple-cursors is a Sublime Text style multiple selections for Vim Browse files, buffers, search \u00b6 Ranger.vim , Ranger integration in vim ctrlp.vim ack.vim Languages \u00b6 Coc.vim has full support for Language Server Protocol completion LSP vim-markdown , Markdown Vim Mode. Javascript \u00b6 A guide to setting up Vim for JavaScript development JavaScript Documentation Standards - vim-javascript , comment stuff out - vim-polyglot , collection of language packs for Vim. Python \u00b6 https://docs.python-guide.org/dev/env/ https://www.vimfromscratch.com/articles/vim-for-python/ https://github.com/neoclide/coc-python html \u00b6 :set omnifunc=htmlcomplete#CompleteTags DB \u00b6 Modern database interface for Vim","title":"vim"},{"location":"shell/vim/#vim","text":":h terminal or","title":"Vim"},{"location":"shell/vim/#panes","text":"Close a terminal pane with C-w C-c To change two vertically split windows to horizonally split C-w t C-w K Horizontally to vertically: C-w t C-w H C-w t makes the first (topleft) window current C-w H moves the current window to full-height at far left C-w K moves the current window to full-width at the very top Comment line (via plugin vim-commentary): gcc in normal mode, gc in visual mode Surroung element while editing","title":"Panes"},{"location":"shell/vim/#code-edition","text":"","title":"Code edition"},{"location":"shell/vim/#folding","text":"For Python, https://github.com/Vimjas/vim-python-pep8-indent","title":"Folding"},{"location":"shell/vim/#plugins","text":"","title":"Plugins"},{"location":"shell/vim/#file-editing","text":"vim-commentary comment stuff out vim-surround provides mapping for parentheses, brackets, quotes, XML tags, and more. vim-multiple-cursors is a Sublime Text style multiple selections for Vim","title":"File editing"},{"location":"shell/vim/#browse-files-buffers-search","text":"Ranger.vim , Ranger integration in vim ctrlp.vim ack.vim","title":"Browse files, buffers, search"},{"location":"shell/vim/#languages","text":"Coc.vim has full support for Language Server Protocol completion LSP vim-markdown , Markdown Vim Mode.","title":"Languages"},{"location":"shell/vim/#javascript","text":"A guide to setting up Vim for JavaScript development JavaScript Documentation Standards - vim-javascript , comment stuff out - vim-polyglot , collection of language packs for Vim.","title":"Javascript"},{"location":"shell/vim/#python","text":"https://docs.python-guide.org/dev/env/ https://www.vimfromscratch.com/articles/vim-for-python/ https://github.com/neoclide/coc-python","title":"Python"},{"location":"shell/vim/#html","text":":set omnifunc=htmlcomplete#CompleteTags","title":"html"},{"location":"shell/vim/#db","text":"Modern database interface for Vim","title":"DB"},{"location":"shell/zsh/","text":"Zsh \u00b6 The Z-shell http://www.zsh.org/ Plugins \u00b6 zsh-z Jump quickly to directories that you have visited \"frecently.\" zsh-autosuggestions Fish-like autosuggestions for zsh Other links \u00b6 Adding Vi to your Zsh","title":"zsh"},{"location":"shell/zsh/#zsh","text":"The Z-shell http://www.zsh.org/","title":"Zsh"},{"location":"shell/zsh/#plugins","text":"zsh-z Jump quickly to directories that you have visited \"frecently.\" zsh-autosuggestions Fish-like autosuggestions for zsh","title":"Plugins"},{"location":"shell/zsh/#other-links","text":"Adding Vi to your Zsh","title":"Other links"},{"location":"shell/GNU/Intro/","text":"GNU \u00b6 JQ","title":"GNU"},{"location":"shell/GNU/Intro/#gnu","text":"JQ","title":"GNU"},{"location":"shell/GNU/Makefile/","text":"Makefile \u00b6 Splitting Recipe Lines https://www.gnu.org/software/make/manual/html_node/Splitting-Recipe-Lines.html Makefile tutorial https://makefiletutorial.com/ Introduction \u00e0 Makefile https://gl.developpez.com/tutoriel/outil/makefile/","title":"Makefile"},{"location":"shell/GNU/Makefile/#makefile","text":"Splitting Recipe Lines https://www.gnu.org/software/make/manual/html_node/Splitting-Recipe-Lines.html Makefile tutorial https://makefiletutorial.com/ Introduction \u00e0 Makefile https://gl.developpez.com/tutoriel/outil/makefile/","title":"Makefile"},{"location":"shell/Ubuntu/configuration/","text":"Configuration \u00b6 LVM \u00b6 https://doc.ubuntu-fr.org/lvm Partitioning: https://www.cyberciti.biz/tips/fdisk-unable-to-create-partition-greater-2tb.html Touchpad \u00b6 libinput \u00b6 See https://wayland.freedesktop.org/libinput/doc/latest/index.html libinput-gesture fusuma","title":"Configuration"},{"location":"shell/Ubuntu/configuration/#configuration","text":"","title":"Configuration"},{"location":"shell/Ubuntu/configuration/#lvm","text":"https://doc.ubuntu-fr.org/lvm Partitioning: https://www.cyberciti.biz/tips/fdisk-unable-to-create-partition-greater-2tb.html","title":"LVM"},{"location":"shell/Ubuntu/configuration/#touchpad","text":"","title":"Touchpad"},{"location":"shell/Ubuntu/configuration/#libinput","text":"See https://wayland.freedesktop.org/libinput/doc/latest/index.html libinput-gesture fusuma","title":"libinput"},{"location":"shell/Ubuntu/python/","text":"Tagen tenditur missos pectus viam vocavit fama \u00b6 Cereri est tamquam et flammas solvit et \u00b6 Ubuntu pyhton's distribution patches various modules, especialy site.py Lorem markdownum tubas; ore manesque prior lacrimae femina convertunt tamen concessa ait tecta das. Hyperborea timent et fontana , defuit illo nati secura, ero. Tamquam arduus qua : in inputet habemus ventis pendens quicquam Minervae, puro pavido his vastius. Acta omnia pectus cui balistave maxima narrare medicamine coepere? Danger Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main(void) { printf ( \"hello, world \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello, world! \\n \" ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello, world!\" ); } } S\u00e9paration python #!/usr/bin/python import tensorflow as tf python #!/usr/bin/python import tensorflow as tf < b > aze </ b > ! [ ndarray ]( https : // docs . scipy . org / doc / numpy / _images / threefundamental . png ) config = { container: \"#tree-simple\" }; parent_node = { text: { name: \"Parent node\" } }; first_child = { parent: parent_node, text: { name: \"First child\" } }; second_child = { parent: parent_node, text: { name: \"Second child\" } }; simple_chart_config = [ config, parent_node, first_child, second_child ]; Un essai st=>start: Start:>http://www.google.com[blank] e=>end:>http://www.google.com op1=>operation: My Operation op2=>operation: My Operation 2 sub1=>subroutine: My Subroutine cond=>condition: Yes or No?:>http://www.google.com io=>inputoutput: catch something... st->op1->op2->cond cond(yes)->io->e cond(no)->sub1(right)->op1 Title: Here is a title A->B: Ligne line B-->C: Dashed line C->>D: Open arrow D-->>A: Dashed open arrow Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tela illa litus quot Graia rediit dexteriore Quaesita loquaci Herbas nec volentem finierat congreditur quibus vestibus Puellae factum Lapillos minores ait admovit apertos me hunc Rapidas spectat cornua Et maior \u00b6 Meritis collumque regale terrae dictum mollitaque natura collumque aliquemque vetus speculabar totque, eam in. Binominis silvestre purpura mercibus, Achivos fata cognoscendus Alpes tremebunda litus carpit . Si fiducia palus, vires nate est actis sed protinus inque. Hic ubera et pars iube fraudare certaminis Nocte aggere sed in exanimata stamina tum Modo illa has fuerant Aures in nusquam longius putes Haec est terga sua Lycum fulmina fatetur Nixus volucres Incipit ut locus Phoeboque natam arduus nec \u00b6 Esse ululavit fuit: fuit nam haec malorum seu fertur lyram memorique Tartara exsulta? Mitia tactuque carpant, inanes aratri, videt cornua defenditur. if (memory_veronica(38, ics_media_software) * pretest(cloneTtl, snowScrapingPodcast, -5)) { username_page_pdf.browserBluetooth = abend_tunneling(cycleManagement, 1) - cpsArtificial(ldapDlc); } else { web_power.non.hoc_mouse(overwriteAutoresponderText); dnsOleWddm = extension_baseband; digital = 230673; } if (partyIpSmtp + -2 + 68 < 3 + file_mpeg) { runtime.localhost = quad; } if (command(3, dynamic_host_copyright) + panel(fileRouter)) { menuDirect.middleware_memory -= trinitronPostSystem - cLogic.tokenBox( control, 4, leopard_grep); zettabyteInstallerView -= switchGolden; minicomputerCorrection += 3 + ntfs; } Habuere quasque stamina \u00b6 Oculisque orat Euandri pariterque modo urbem animo, tenent vulnere? Daedalon orare risisse erit , posita, urbes arida ademit , tenentibus litore Cypriae tibi virentes. Clades bello sub; adpellatque praesens dammis, foedera sub montis dedere! Opis intendunt canes, lacertis timet proelia secuta thalamis sed res Nec! Totque poena oculosque Deoida tumuletur poma fidem diffugiunt petentes, similem inclinatoque Rhodon Boreas Famemque cratere peregrinis bellis, Minervae. Nec nec praetemptat salutant exibat, adest aut perosus motus vaccam me! Reddunt omnibus est convicia comminus illi nescio. Menoeten spectacula quia tum viret dat stirpis tremor atria, Athin Ledam est violentus quid?","title":"Python"},{"location":"shell/Ubuntu/python/#tagen-tenditur-missos-pectus-viam-vocavit-fama","text":"","title":"Tagen tenditur missos pectus viam vocavit fama"},{"location":"shell/Ubuntu/python/#cereri-est-tamquam-et-flammas-solvit-et","text":"Ubuntu pyhton's distribution patches various modules, especialy site.py Lorem markdownum tubas; ore manesque prior lacrimae femina convertunt tamen concessa ait tecta das. Hyperborea timent et fontana , defuit illo nati secura, ero. Tamquam arduus qua : in inputet habemus ventis pendens quicquam Minervae, puro pavido his vastius. Acta omnia pectus cui balistave maxima narrare medicamine coepere? Danger Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main(void) { printf ( \"hello, world \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello, world! \\n \" ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello, world!\" ); } } S\u00e9paration python #!/usr/bin/python import tensorflow as tf python #!/usr/bin/python import tensorflow as tf < b > aze </ b > ! [ ndarray ]( https : // docs . scipy . org / doc / numpy / _images / threefundamental . png ) config = { container: \"#tree-simple\" }; parent_node = { text: { name: \"Parent node\" } }; first_child = { parent: parent_node, text: { name: \"First child\" } }; second_child = { parent: parent_node, text: { name: \"Second child\" } }; simple_chart_config = [ config, parent_node, first_child, second_child ]; Un essai st=>start: Start:>http://www.google.com[blank] e=>end:>http://www.google.com op1=>operation: My Operation op2=>operation: My Operation 2 sub1=>subroutine: My Subroutine cond=>condition: Yes or No?:>http://www.google.com io=>inputoutput: catch something... st->op1->op2->cond cond(yes)->io->e cond(no)->sub1(right)->op1 Title: Here is a title A->B: Ligne line B-->C: Dashed line C->>D: Open arrow D-->>A: Dashed open arrow Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tela illa litus quot Graia rediit dexteriore Quaesita loquaci Herbas nec volentem finierat congreditur quibus vestibus Puellae factum Lapillos minores ait admovit apertos me hunc Rapidas spectat cornua","title":"Cereri est tamquam et flammas solvit et"},{"location":"shell/Ubuntu/python/#et-maior","text":"Meritis collumque regale terrae dictum mollitaque natura collumque aliquemque vetus speculabar totque, eam in. Binominis silvestre purpura mercibus, Achivos fata cognoscendus Alpes tremebunda litus carpit . Si fiducia palus, vires nate est actis sed protinus inque. Hic ubera et pars iube fraudare certaminis Nocte aggere sed in exanimata stamina tum Modo illa has fuerant Aures in nusquam longius putes Haec est terga sua Lycum fulmina fatetur Nixus volucres","title":"Et maior"},{"location":"shell/Ubuntu/python/#incipit-ut-locus-phoeboque-natam-arduus-nec","text":"Esse ululavit fuit: fuit nam haec malorum seu fertur lyram memorique Tartara exsulta? Mitia tactuque carpant, inanes aratri, videt cornua defenditur. if (memory_veronica(38, ics_media_software) * pretest(cloneTtl, snowScrapingPodcast, -5)) { username_page_pdf.browserBluetooth = abend_tunneling(cycleManagement, 1) - cpsArtificial(ldapDlc); } else { web_power.non.hoc_mouse(overwriteAutoresponderText); dnsOleWddm = extension_baseband; digital = 230673; } if (partyIpSmtp + -2 + 68 < 3 + file_mpeg) { runtime.localhost = quad; } if (command(3, dynamic_host_copyright) + panel(fileRouter)) { menuDirect.middleware_memory -= trinitronPostSystem - cLogic.tokenBox( control, 4, leopard_grep); zettabyteInstallerView -= switchGolden; minicomputerCorrection += 3 + ntfs; }","title":"Incipit ut locus Phoeboque natam arduus nec"},{"location":"shell/Ubuntu/python/#habuere-quasque-stamina","text":"Oculisque orat Euandri pariterque modo urbem animo, tenent vulnere? Daedalon orare risisse erit , posita, urbes arida ademit , tenentibus litore Cypriae tibi virentes. Clades bello sub; adpellatque praesens dammis, foedera sub montis dedere! Opis intendunt canes, lacertis timet proelia secuta thalamis sed res Nec! Totque poena oculosque Deoida tumuletur poma fidem diffugiunt petentes, similem inclinatoque Rhodon Boreas Famemque cratere peregrinis bellis, Minervae. Nec nec praetemptat salutant exibat, adest aut perosus motus vaccam me! Reddunt omnibus est convicia comminus illi nescio. Menoeten spectacula quia tum viret dat stirpis tremor atria, Athin Ledam est violentus quid?","title":"Habuere quasque stamina"}]}